{"docstore/data": {"4f893ef3-3cd0-4438-b9c8-654a6a968e5d": {"__data__": {"id_": "4f893ef3-3cd0-4438-b9c8-654a6a968e5d", "embedding": null, "metadata": {"file_path": "data\\2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_name": "2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_type": "text/plain", "file_size": 9919, "creation_date": "2024-07-05", "last_modified_date": "2024-07-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89e0d2a9-b761-4f3f-b468-7e32709320db", "node_type": "4", "metadata": {"file_path": "data\\2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_name": "2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_type": "text/plain", "file_size": 9919, "creation_date": "2024-07-05", "last_modified_date": "2024-07-02"}, "hash": "526f5f29af9082f39338870e8a3300f57b6ebdf716ca816614357047348f1c1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9c843394-de50-426a-94f0-fab40c016033", "node_type": "1", "metadata": {}, "hash": "6658fb531edb238c1dfe469852c2dc1288e091095d8d8d6fe8fae144f5df978a", "class_name": "RelatedNodeInfo"}}, "text": "Title:\r\nScaling and Optimizing and Cost of Machine Learning Ingestion on Unstructured Data for Subsurface Application\r\n\r\n\r\nAuthors:\r\nL.C.L. Panganiban, F. Baillard', N.M. Hernandez!\r\nTraya Energies\r\n\r\n\r\nContent:\r\n\r\nSummary:\r\nIn recent years, the energy industry has shifted their attention into extracting additional values from\r\ntheir in-house legacy datasets for shorter project turnaround and better decision making. Internal digital\r\ntransformation initiatives and access to new technology such as cloud computing, machine learning and\r\nmicroservices made it possible to shift towards a scalable ingestion platform.\r\n\r\nA modern scalable ingestion platform often includes 1) automated machine learning (ML) components\r\narticulated around pipelines to parse and go through the data and extract the needed information 2)\r\nstorage component such Database, Datawarehouse and Datalake defining what data to be stored and\r\nhow.\r\n\r\nIn this paper, we will provide a description of these different components, their modern implementation\r\ninto a cloud environment using microservices and some performance benchmark based on real world\r\ndata examples.\r\n\r\n\r\nIntroduction:\r\nIn recent years, the energy industry has shifted their attention into extracting additional values from\r\ntheir in-house legacy datasets for shorter project turnaround and better decision making. Internal digital\r\ntransformation initiatives and access to new technology such as cloud computing, machine learning and\r\nmicroservices made it possible to shift towards a scalable ingestion platform.\r\n\r\nA modern scalable ingestion platform often includes 1) automated machine learning (ML) components\r\narticulated around pipelines to parse and go through the data and extract the needed information 2)\r\nstorage component such Database, Datawarehouse and Datalake defining what data to be stored and\r\nhow.\r\n\r\nIn this paper, we will provide a description of these different components, their modern implementation\r\ninto a cloud environment using microservices and some performance benchmark based on real world\r\ndata examples.\r\n\r\n\r\nScalable ingestion platform architecture:\r\nThe vast majority of experimentation and testing of new ML application are performed on notebooks\r\non local machines leveraging on local hardware with the data and the code being stored locally-. Such\r\na setup has the advantage of being lightweight allowing a fast turnaround between two ML iterations.\r\nHowever the lack of traceability, compatibility and hardware limitation makes it challenging to scale\r\nsuch application, hence the requirement of a more scalable solution.\r\n\r\nIn comparison, a scalable ingestion platform is a type of system native able to handle current and future\r\nworkloads regardless of the amount of data ingested or users connecting to it, considering both scaling\r\nin (shrinking resources) and scaling out (expanding resources). An example of such system can be seen\r\non Figure 1. The architecture is made of of four components namely pipeline, storage, compute and\r\ninterfaces.\r\n\r\nFigure 1: Ingestion Platform Architecture\r\nPipeline or data pipeline is the main driver in extracting and transforming data into a unified format.\r\nThe typical ingestion workflow is based on an Extract-Transform-Load process with machine learning\r\nworkflows able to accommodate the variety in sources and forms present in unstructured data\r\n(Hernandez et al., 2019).\r\n\r\nThe storage component is where the data resides. Storage is often the least thought about in development\r\nand architecture strategies, but it is one of the core contributors in terms of cost to the organization. The\r\ncompute node component schedules and orchestrates the data pipelines, logic flows, and algorithms.\r\nThe interface or the dashboard component provides the monitoring and observability capabilities. This\r\npresents the events, statuses, logs, and other operational insights that can be used for decision making.\r\nThese components have different ways to be scaled depending of 1) the environment of installation: on-\r\npremise vs. cloud 2) the volume and type of data being processed 3) the early development decision: all\r\nout of the box cloud providers solution, opensource self maintained solution or hybrid.\r\n\r\n\r\nComponents optimization:\r\nThe first metrics to consider for optimization is processing time and should be independent from the\r\nvolume of data to be processed by the pipeline. This optimization is achieved through parallelization,\r\ndistributed computing and orchestration by scaling up or down the usage based on demand. Applications\r\nand data pipelines are packaged into containers and then deployed into a Kubernetes cluster in which it\r\nhandles the scheduling, distribution, and allocation across different machines. Workflow managers (\r\nWM) provide the platform to execute and orchestrate the jobs and tasks that are contained in a Pod/Pool\r\n(Figure 2). Workflow managers also implement orchestration and scheduling though it handles the tasks\r\nin the application layer where this determines if the data execution is successful\r\n\r\nFigure 2: Unstructured data pipeline with orchestration in place\r\nThe second metric to consider is the extensibility metric. A single service or component can be used\r\nbeyond what is originally intended.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 5278, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c843394-de50-426a-94f0-fab40c016033": {"__data__": {"id_": "9c843394-de50-426a-94f0-fab40c016033", "embedding": null, "metadata": {"file_path": "data\\2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_name": "2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_type": "text/plain", "file_size": 9919, "creation_date": "2024-07-05", "last_modified_date": "2024-07-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89e0d2a9-b761-4f3f-b468-7e32709320db", "node_type": "4", "metadata": {"file_path": "data\\2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_name": "2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_type": "text/plain", "file_size": 9919, "creation_date": "2024-07-05", "last_modified_date": "2024-07-02"}, "hash": "526f5f29af9082f39338870e8a3300f57b6ebdf716ca816614357047348f1c1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4f893ef3-3cd0-4438-b9c8-654a6a968e5d", "node_type": "1", "metadata": {"file_path": "data\\2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_name": "2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_type": "text/plain", "file_size": 9919, "creation_date": "2024-07-05", "last_modified_date": "2024-07-02"}, "hash": "e80b1afc6f35510f39fae624361ca5a01aee4d901fa3c9128c3dcd6fe224fb11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a0b1bc1a-a89c-4925-bf09-1accfb5cf48a", "node_type": "1", "metadata": {}, "hash": "4c094ea1a5a3e4390dc67bee3f9293cb5942210d426a595638e2c092796f8e54", "class_name": "RelatedNodeInfo"}}, "text": "Components optimization:\r\nThe first metrics to consider for optimization is processing time and should be independent from the\r\nvolume of data to be processed by the pipeline. This optimization is achieved through parallelization,\r\ndistributed computing and orchestration by scaling up or down the usage based on demand. Applications\r\nand data pipelines are packaged into containers and then deployed into a Kubernetes cluster in which it\r\nhandles the scheduling, distribution, and allocation across different machines. Workflow managers (\r\nWM) provide the platform to execute and orchestrate the jobs and tasks that are contained in a Pod/Pool\r\n(Figure 2). Workflow managers also implement orchestration and scheduling though it handles the tasks\r\nin the application layer where this determines if the data execution is successful\r\n\r\nFigure 2: Unstructured data pipeline with orchestration in place\r\nThe second metric to consider is the extensibility metric. A single service or component can be used\r\nbeyond what is originally intended. Thanks to the microservices, exposing your data via application\r\nprogramming interfaces or APIs is a good practise (Figure 3). It provides a common language across\r\nteams where implementation is unified across different data sources. An additional advantage of the use\r\nof APIs is the additional level of abstraction for the storage of the data. Data are now accessible from\r\nvarious mounting locations through a single call, allowing democratization and versioning of the data.\r\n\r\nFigure 3: Connecting applications using API links\r\nAs an example, a full-text search endpoint originally used to do searches, can also be used to create\r\naggregations models like heatmap and knowledge graphs (Baillard et Al., 2021) as seen on Figure 4.\r\n\r\nThe third metric to consider is the UI/UX responsivness and define the optimal data representation for\r\nthe user or the operator to QC the data and for the service to have still an acceptable response time. As\r\nseen on Figure 4, the heat map and the knowledge graph are built on-top of 100,000 data points\r\n(Mamador et al., 2021). Interface and visualization scaling requires removing the dependency on the\r\nvolume of data points. The amount of time visualizing 100 points should be the same as visualizing\r\n100,000 points  in which development of aggregation and interpolation workflows must be performed\r\nin the backend and frontend to overcome this volume challenge.\r\n\r\n\r\nBenchmarks and testing:\r\nTo see the performance of this architecture we can consider the following data. We have 3 buckets of\r\ndata for this assessment. It is composed of geoscience documents that ranges from final well reports,\r\ngeological report, regional studies, interpretations coming from various file formats, countries, and\r\nlanguages. The data also includes images like thin sections, core, well logs, seismic. All these data are\r\naudited and stored into an object storage.\r\n\r\nTable 1: Extraction Performance with 4-cores and 16 GB of RAM per node\r\nTable 1 shows the extraction performance. The extraction workload encompasses the data loading,\r\nprocessing, and uploading. It was tested on a variety of datasets from different regions and languages.\r\nFor a single node execution, the average number of pages per hour is around 244 pages and for a 5-node\r\ncluster, the average number of pages per hour is 1000. Since workflow manager provide an auto-retry\r\n\r\nfunction, this reduces the failure rate by a significant amount. This is due to tasks that are failing due to\r\nnetwork or infrastructure issues to be retried or re-executed. The failure rate is now mostly on the data\r\nsince we haveve removed the infrastructure constraint.\r\n\r\nAs one of the metrics that we have set earlier, we need to consider the cost and infrastructure usage.\r\nUsing the same workloads, we can assess the cost and infrastructure optimization in Table 2.\r\n\r\nTable 2: Extraction Infrastructure Utilization with 4-cores and 16GB of RAM\r\nTable 2 shows that we have a higher utilization in the cluster hence we are fully utilizing the server. In\r\nthe case of single node, only 41% of the resources is utilized vs 83% in the cluster approach. Both the\r\nreduced failure rates and the hardware utilization illustratres the gain of efficency in deploying a scalable\r\ningestion platform\r\n\r\n\r\nConclusions:\r\nIn this paper, we have seen how to scale and optimize the ML ingestion pipeline for subsurface\r\napplications. We have shown that scaling and optimizing ML ingestion pipelines can lead to\r\nimprovements in terms of time and cost. We have also demonstrated that the ingestion platform is\r\nscalable to handle data from a variety of regions and languages.", "mimetype": "text/plain", "start_char_idx": 4240, "end_char_idx": 8924, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a0b1bc1a-a89c-4925-bf09-1accfb5cf48a": {"__data__": {"id_": "a0b1bc1a-a89c-4925-bf09-1accfb5cf48a", "embedding": null, "metadata": {"file_path": "data\\2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_name": "2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_type": "text/plain", "file_size": 9919, "creation_date": "2024-07-05", "last_modified_date": "2024-07-02"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89e0d2a9-b761-4f3f-b468-7e32709320db", "node_type": "4", "metadata": {"file_path": "data\\2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_name": "2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_type": "text/plain", "file_size": 9919, "creation_date": "2024-07-05", "last_modified_date": "2024-07-02"}, "hash": "526f5f29af9082f39338870e8a3300f57b6ebdf716ca816614357047348f1c1e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9c843394-de50-426a-94f0-fab40c016033", "node_type": "1", "metadata": {"file_path": "data\\2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_name": "2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_type": "text/plain", "file_size": 9919, "creation_date": "2024-07-05", "last_modified_date": "2024-07-02"}, "hash": "227b4b13cd5697fe99adccc008b2b95c9b25618e4df8b98777ecb45651a6823d", "class_name": "RelatedNodeInfo"}}, "text": "Using the same workloads, we can assess the cost and infrastructure optimization in Table 2.\r\n\r\nTable 2: Extraction Infrastructure Utilization with 4-cores and 16GB of RAM\r\nTable 2 shows that we have a higher utilization in the cluster hence we are fully utilizing the server. In\r\nthe case of single node, only 41% of the resources is utilized vs 83% in the cluster approach. Both the\r\nreduced failure rates and the hardware utilization illustratres the gain of efficency in deploying a scalable\r\ningestion platform\r\n\r\n\r\nConclusions:\r\nIn this paper, we have seen how to scale and optimize the ML ingestion pipeline for subsurface\r\napplications. We have shown that scaling and optimizing ML ingestion pipelines can lead to\r\nimprovements in terms of time and cost. We have also demonstrated that the ingestion platform is\r\nscalable to handle data from a variety of regions and languages. Finally, weve seen the power of scaling\r\nvisualizations and exposing the data via API links in which we can get more insight and enhance the\r\nability of the team to extract knowledge.\r\n\r\n\r\nReferences:\r\nBaillard F. and Hernandez N.: A Case Study of Understanding Bonaparte Basin using Unstructured\r\nData Analysis with Machine Learning Techniques. 82nd EAGE Conference & Exhibition, 18-21\r\nOctober 2021, Amsterdam.\r\nMamador C., Hernandez N., Baillard F.: Production-scale processing of EAGEs EarthDoc data\r\nto stimulate new insights in CO2 and new energy management. 82nd EAGE Conference &\r\nExhibition worskhop on ML solutions at scale, 22 October 2021, Amsterdam.\r\nHernandez N., Lucajias P., Graciosa J.C., Mamador C., and Panganiban L. C. I., 2019: Automated\r\ninformation retrieval from unstructured documents utilizing a sequence of smart machine learning\r\nmethods within a hybrid cloud container. EAGE Workshop on Big Data and Machine Learning for\r\nE&P Efficiency 25 - 27 February.", "mimetype": "text/plain", "start_char_idx": 8039, "end_char_idx": 9908, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"4f893ef3-3cd0-4438-b9c8-654a6a968e5d": {"doc_hash": "e80b1afc6f35510f39fae624361ca5a01aee4d901fa3c9128c3dcd6fe224fb11", "ref_doc_id": "89e0d2a9-b761-4f3f-b468-7e32709320db"}, "9c843394-de50-426a-94f0-fab40c016033": {"doc_hash": "227b4b13cd5697fe99adccc008b2b95c9b25618e4df8b98777ecb45651a6823d", "ref_doc_id": "89e0d2a9-b761-4f3f-b468-7e32709320db"}, "a0b1bc1a-a89c-4925-bf09-1accfb5cf48a": {"doc_hash": "f2618efd213c98114f30cae2c68919671390738bd8557aa0c23229f692229e43", "ref_doc_id": "89e0d2a9-b761-4f3f-b468-7e32709320db"}}, "docstore/ref_doc_info": {"89e0d2a9-b761-4f3f-b468-7e32709320db": {"node_ids": ["4f893ef3-3cd0-4438-b9c8-654a6a968e5d", "9c843394-de50-426a-94f0-fab40c016033", "a0b1bc1a-a89c-4925-bf09-1accfb5cf48a"], "metadata": {"file_path": "data\\2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_name": "2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.txt", "file_type": "text/plain", "file_size": 9919, "creation_date": "2024-07-05", "last_modified_date": "2024-07-02"}}}}