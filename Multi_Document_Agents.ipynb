{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67c8afd7",
   "metadata": {
    "id": "67c8afd7"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/run-llama/llama_index/blob/main/docs/docs/examples/agent/multi_document_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43497beb-817d-4366-9156-f4d7f0d44942",
   "metadata": {
    "id": "43497beb-817d-4366-9156-f4d7f0d44942"
   },
   "source": [
    "# Multi-Document Agents\n",
    "\n",
    "In this guide, you learn towards setting up an agent that can effectively answer different types of questions over a larger set of documents.\n",
    "\n",
    "These questions include the following\n",
    "\n",
    "- QA over a specific doc\n",
    "- QA comparing different docs\n",
    "- Summaries over a specific doc\n",
    "- Comparing summaries between different docs\n",
    "\n",
    "We do this with the following architecture:\n",
    "\n",
    "- setup a \"document agent\" over each Document: each doc agent can do QA/summarization within its doc\n",
    "- setup a top-level agent over this set of document agents. Do tool retrieval and then do CoT over the set of tools to answer a question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be00aba-b6c5-4940-9825-81c5d2cd2f0b",
   "metadata": {
    "id": "9be00aba-b6c5-4940-9825-81c5d2cd2f0b"
   },
   "source": [
    "## Setup and Download Data\n",
    "\n",
    "In this section, we'll define imports and then download Wikipedia articles about different cities. Each article is stored separately.\n",
    "\n",
    "We load in 18 cities - this is not quite at the level of \"hundreds\" of documents but its still large enough to warrant some top-level document retrieval!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d81f93c",
   "metadata": {
    "id": "5d81f93c"
   },
   "source": [
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41e9905-77a9-44c5-88ac-c7a4d08a4612",
   "metadata": {
    "id": "e41e9905-77a9-44c5-88ac-c7a4d08a4612",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:27:31.478432600Z",
     "start_time": "2024-07-04T07:27:31.337894500Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SimpleDirectoryReader,\n",
    ")\n",
    "from llama_index.core import SummaryIndex\n",
    "from llama_index.core.schema import IndexNode\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.callbacks import CallbackManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4343cf7-eec9-4a67-b5be-c72dbe3280a7",
   "metadata": {
    "id": "e4343cf7-eec9-4a67-b5be-c72dbe3280a7",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:27:35.372271900Z",
     "start_time": "2024-07-04T07:27:35.218136Z"
    }
   },
   "outputs": [],
   "source": [
    "wiki_titles = [\n",
    "    \"A Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with Machine Learning Techniques\",\n",
    "    \"An Automated Information Retrieval Platform For Unstructured Well Data Utilizing Smart Machine Learning Algorithms Within A Hybrid Cloud Container\",\n",
    "    \"Supporting the UN 2050 Net Zero goals by reading the earth better\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf0c13b-0d77-43e8-8c1c-84258299a494",
   "metadata": {
    "id": "5bf0c13b-0d77-43e8-8c1c-84258299a494",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:27:40.250020700Z",
     "start_time": "2024-07-04T07:27:39.893784200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load all wiki documents\n",
    "city_docs = {}\n",
    "for wiki_title in wiki_titles:\n",
    "    city_docs[wiki_title] = SimpleDirectoryReader(\n",
    "        input_files=[f\"data/{wiki_title}.txt\"]\n",
    "    ).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(city_docs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smVPrrlFmWNs",
    "outputId": "c372dc49-7872-4996-da57-e995dd5e1f28",
    "ExecuteTime": {
     "end_time": "2024-07-04T07:27:41.604196700Z",
     "start_time": "2024-07-04T07:27:41.173094500Z"
    }
   },
   "id": "smVPrrlFmWNs",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with Machine Learning Techniques': [Document(id_='246df66b-6e36-4797-a348-80136f389a35', embedding=None, metadata={'file_path': 'data\\\\A Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with Machine Learning Techniques.txt', 'file_name': 'A Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with Machine Learning Techniques.txt', 'file_type': 'text/plain', 'file_size': 9304, 'creation_date': '2024-07-04', 'last_modified_date': '2024-07-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Title:\\r\\nA CASE STUDY OF UNDERSTANDING THE BONAPARTE BASIN USING UNSTRUCTURED DATA ANALYSIS WITH MACHINE LEARNING TECHNIQUES\\r\\n\\r\\nAuthors:\\r\\nA.N.N. Sazali, N.M. Hernandez, F. Baillard, K.G. Maver\\r\\n\\r\\n\\r\\nContent:\\r\\n\\r\\nSummary:\\r\\nAs part of exploration and production the oil and gas industry produce substantial amounts of data\\r\\nwithin different disciplines of which 80% are unstructured like reports, presentations, spreadsheets etc.\\r\\nThe value of technical work is reduced due to the lack of time available for analysis and critical\\r\\nthinking and the under-utilization of the data. To assist geoscientist and engineers, Machine Learning\\r\\n(ML) and Artificial Intelligence (AI) technologies are applied to process the unstructured data from\\r\\n440 wells from the Bonaparte Basin in Australia making it possible to perform more accurate analysis\\r\\nand make faster decisions.\\r\\n\\r\\nBased on the play-based exploration pyramid concept, the time spent at the Basin Focus stage can be\\r\\nreduced, and more time are available to focus on the other project stages. The explorationist will be\\r\\nable to bring more value to the study.\\r\\n\\r\\nIt will be shown that potential issues encountered during exploration of the Bonaparte Basin can be\\r\\nidentified. Based on a quick look and gathering of all information it can be concluded that most of the\\r\\nproduction in the Bonaparte Basin is from Jurassic and Triassic with observed net pay of 18-60m\\r\\nthickness, porosity of 11-29% and saturation of 11-55% Sw.\\r\\n\\r\\nA Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with\\r\\nMachine Learning Techniques\\r\\n\\r\\n\\r\\nIntroduction:\\r\\nAs part of exploration and production the oil and gas industry produce substantial amounts of data\\r\\nwithin different disciplines of which 80% are unstructured like reports, presentations, spreadsheets etc\\r\\nand it is expected to grow exponentially. As a result, geoscientists and engineers spend 50 to 80% of\\r\\ntheir time searching and assembling data and only 1 to 5% of the data is fully utilized. The value of\\r\\ntechnical work is therefore reduced due to the lack of time available for analysis and critical thinking\\r\\nand the under-utilization of the data. To assist geoscientist and engineers, Machine Learning (ML)\\r\\nand Artificial Intelligence (AI) technologies are applied to process the unstructured data making it\\r\\npossible to perform more accurate analysis and make faster decisions.\\r\\n\\r\\nIn this case study the area of interest covers Bonaparte Basin, which is located north-west of\\r\\nthe Australian continental margin (Figure 1). It joins the Money Shoal basin in the north-east and\\r\\nthe Browse Basin in the south-west. Furthermore, the Timor Trough defines the northern boundary.\\r\\nThe areal extent of the basin is approximately 270,000 sq. km. The objective of this study is to\\r\\nunderstand and obtain meaningful insights into the Bonaparte Basin based on the substantial amount\\r\\nof information available in previous studies, reports and presentations. The unstructured data of the\\r\\nBonaparte Basin have been ingested in a Knowledge Container through consecutive ML and AI\\r\\npipelines and analysed using big data analytics tools.\\r\\n\\r\\nFigure 1 Location of the Bonaparte Basin within the Australian continental margin (left) and 14\\r\\nstructural elements observed within the Bonaparte Basin (right).\\r\\n\\r\\n\\r\\nMethodology:\\r\\nAs of 2021, the Bonaparte Basin encompasses 440 wells representing 58 years of exploration history\\r\\nsummarized in over 270,000 pages of documents and in 250,000 images. It is estimated that billions of\\r\\ndollars have been invested over the years to acquire and interprete the data, making it a substantial\\r\\nsource of information for new exploration activities.\\r\\n\\r\\nThe Play Based Exploration (PBE) approach is often used as a traditional framework to refine the\\r\\ngeoscientists s understanding from a broad basin level to a narrow prospect focus (Lottaroli et al., 2016).\\r\\nAs a start such an approach often involves capturing the current state of knowledge with massive\\r\\nbackground resources to understand and analyse the key features of the basin and the major risks\\r\\nassociated to it. Such information is primarily available in unstructured data, requiring geoscientist and\\r\\nengineers to process and ingest the information before focusing on a specific play and prospect using\\r\\nstructured data. Therefore, we have modified the existing PBE pyramid to introduce an additional\\r\\n\\r\\ndimension associated with data science identifying the different types of data available at different\\r\\nstages, allowing us to better define the best suited ML/AI strategy for a given stage (Figure 2).\\r\\n\\r\\nFigure 2 Customized Play Based Exploration (PBE) pyramid with ML technology (Modified from\\r\\nLottaroli et al., 2016).\\r\\n\\r\\nFocusing on the unstructured data associated with the Basin and Play Analysis, all the data from the\\r\\nBonaparte Basin have been processed through a succession of AI/ML automated pipeline such as\\r\\nNatural Language Processing or Deep Convolutional Neural Network (Hernandez et al., 2019), (Figure\\r\\n3). The sharable structured data is then further processed through deeper level of analytics to detect\\r\\ntrends and anomalies present within the data. Machine assistance is heavily used in repetitive tasks early\\r\\nin the process during the processing of the data and up to 95% of the tasks will be performed by the\\r\\nmachine. This provides additional time for the specialist to focus on critical thinking and cognitive skills\\r\\n\\r\\nFigure 3 Unstructured Big Data pipeline\\r\\n\\r\\nIn this case study, interpretation using the Big Data workflow was used to understand the exploration\\r\\nhistory, how the basin developed, its petroleum system and the main issue of the dry wells occurrence\\r\\nto avoid repeating the mistakes of the past and improve future decision making.\\r\\n\\r\\nBy analysing the data, five potential issues are identified i.e. (i) Discrepancies in Formation Tops, (ii)\\r\\nLimited understanding of Lithology Distribution, (iii) Limited Mineral Composition Understanding,\\r\\n(iv) Fluid Distribution, and (v) Pressure/Temperature Patterns. Each potential issue is tackled by\\r\\nidentifying trends and anomalies across the basin using images, tables and plots extracted from the\\r\\nunstructured data corpus.\\r\\n\\r\\n\\r\\nResults:\\r\\nAs an example, the analysis of the (ii) Limited Understanding of Lithology Distribution, shown in\\r\\nFigure 4, is performed using heatmaps. The heatmaps show the distribution of clastic and carbonates\\r\\nacross the Bonaparte Basin and identify patterns and anomalies present in the area. The result can be\\r\\nsupported by the stratigraphic chart where the carbonate environment occurs in the younger formation\\r\\nfrom Cretaceous to Neogene period, whereas clastic environment is present in the older formations from\\r\\nTriassic to Cretaceous.\\r\\n\\r\\nFigure 4 Lithology distribution on heatmaps (left) and corresponding stratigraphic chart (right).\\r\\n\\r\\nThe analysis of the (iii) Limited Mineral Composition Understanding, shown in Figure 5, utilizes the\\r\\nthin section automatically extracted using ML classification over the full area and suggests that:\\r\\n\\r\\nQuartz overgrowth and kaolinite are quite common in Bonaparte Basin\\r\\n\\r\\nMica mineral can be observed at the north-eastern part of the basin\\r\\n\\r\\nHighly corroded, skeletal feldspar has been extensively dissolved, which forms secondary\\r\\nporosity, and can be observed in the northern part of the basin\\r\\n\\r\\nSome patchy siderites are also observed in the southern part of the basin\\r\\n\\r\\nFigure 5 Thin section images distributed on a map across the Bonaparte Basin.\\r\\n\\r\\n\\r\\nConclusion:\\r\\nA regional understanding is critical and time consuming as it involves dealing with a very large data\\r\\nvolume. Within a project time frame, based on PBE pyramid, the time spent at the Basin Focus stage\\r\\ncan be reduced, and more time are available to focus on the other project stages. The explorationist will\\r\\nbe able to bring more value to the study.\\r\\n\\r\\nML applications have proven to be able to play a crucial part in order to organize large unstructured\\r\\ndata corpuses. This allows faster and accurate decision making within the fast-moving industry.\\r\\n\\r\\nIn this study, some potential issues encountered during exploration of the Bonaparte Basin can be\\r\\nidentified. Based on a quick look and gathering of all information it can be concluded that most of the\\r\\nproduction in the Bonaparte Basin is from Jurassic and Triassic with observed net pay ~18-60m\\r\\nthickness, porosity ~11-29% and saturation ~11-55% Sw.\\r\\n\\r\\n\\r\\nReferences:\\r\\nHernandez N., Lucafias P., Graciosa J.C., Mamador C., and Panganiban L. C. I, 2019: Automated\\r\\ninformation retrieval from unstructured documents utilizing a sequence of smart machine learning\\r\\nmethods within a hybrid cloud container. EAGE Workshop on Big Data and Machine Learning for E&P\\r\\nEfficiency 25 - 27 February.\\r\\n\\r\\nLottaroli F., Craig J., Cozzi A., 2016: Evaluating a vintage play fairway exercise using subsequent\\r\\nexploration results: did it work? Petroleum Geoscience, Vol 24, no 2, p. 159   171.\\r\\n\\r\\nMaver K.G., Hernandez N., Lucafias P., Graciosa J.C., Mamador C., Panganiban L.C.L, Yu C., and\\r\\nMaver M.G., 2018: An automated information retrieval platform for unstructured well data smart\\r\\nmachine learning algorithms within a hybrid cloud container. EAGE/PESGB Workshop on Machine\\r\\nLearning, 29   30 November.\\r\\n\\r\\n\\r\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')], 'An Automated Information Retrieval Platform For Unstructured Well Data Utilizing Smart Machine Learning Algorithms Within A Hybrid Cloud Container': [Document(id_='d8dc010d-c50a-4893-9638-5012085feede', embedding=None, metadata={'file_path': 'data\\\\An Automated Information Retrieval Platform For Unstructured Well Data Utilizing Smart Machine Learning Algorithms Within A Hybrid Cloud Container.txt', 'file_name': 'An Automated Information Retrieval Platform For Unstructured Well Data Utilizing Smart Machine Learning Algorithms Within A Hybrid Cloud Container.txt', 'file_type': 'text/plain', 'file_size': 7713, 'creation_date': '2024-07-04', 'last_modified_date': '2024-07-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Title:\\r\\nAn Automated Information Retrieval Platform For Unstructured Well Data Utilizing Smart Machine Learning Algorithms Within A Hybrid Cloud Container\\r\\n\\r\\n\\r\\nAuthors:\\r\\nN.M. Hernandez, P.J. Lucafias', J.C. Graciosa', C. Mamador, L. Caezar', I. Panganiban, C. Yu', K.G.\\r\\nMaver*, M.G. Maver\\r\\nIraya Energies, 7 KGM geoconsulting\\r\\n\\r\\n\\r\\nContent:\\r\\n\\r\\nSummary:\\r\\nThere is a large amount of historic and valuable well information available stored either on paper and more\\r\\nrecently as digital documents and reports in the oil and gas industry especially by national data management\\r\\nsystems and oil companies, These technical documents contain valuable information from disciplines like\\r\\ngeoscience and engineering and are in general stored in a unstructured format. To extract and utilize all this well\\r\\ndata, a machine learning-enabled platform, consisting of a carefully selected sequence of algorithms, has been\\r\\ndeveloped as a hybrid cloud container that automatically reads and understands the technical documents with little\\r\\nhuman supervision. The user can upload raw data to the platform, which are stored on a private local server. The\\r\\nmachine learning algorithms are activated and implement the necessary processing and workflows. Structured\\r\\ndata is generated as output, which are pushed through to a search engine that is accessible to the user in the cloud.\\r\\nThe aim of the platform is to ease the identification of important parts of the technical documents, automatically\\r\\nextract relevant information and visualize it for the user, so they can easily do further analysis, share it with\\r\\ncolleagues or agnostically port it to other platforms as input.\\r\\n\\r\\n\\r\\nIntroduction:\\r\\nThere is a large amount of historic well information available stored either on paper and more recently\\r\\nas digital documents and reports in the oil and gas industry. These technical documents contain\\r\\nvaluable information from diverse disciplines such as geology, geophysics, petrophysics, reservoir\\r\\nengineering, drilling and other subject matters and are in general stored in a unstructured format.\\r\\n\\r\\nEspecially national data management systems and oil companies hosts these large amounts of very\\r\\nvaluable historical well data, which contain information such as reservoir metadata, images, texts, and\\r\\nprocessed information, such as lithology, geology, shows, drilling risks etc. Due to the large volume,\\r\\nvintage variety, and non-standard formats, extraction of valuable information, which can be used as\\r\\ninput for further work, is an arduous task as the manual nature of data mining is very time-consuming.\\r\\n\\r\\nTo extract and utilize all this well data, a machine learning-enabled platform has been developed as a\\r\\nhybrid cloud container that automatically reads and understands hundreds or thousands of technical\\r\\ndocuments with little human supervision. The aim of the platform is to ease the identification of\\r\\nimportant parts of the technical documents, automatically extract relevant information and visualize it\\r\\nfor the user, so they can easily do further analysis, share it with colleagues or agnostically port it to\\r\\nother platforms as input.\\r\\n\\r\\n\\r\\nMethodology:\\r\\nThe platform utilizes a hybrid data service architecture, which leverages the 2-tier strength of both\\r\\ncloud and private servers. The hybrid architecture serves to:\\r\\n\\r\\nEnhance the platforms security and data privacy by storing raw data locally\\r\\nIncrease data shareability in real-time by utilizing a cloud solution\\r\\n\\r\\nReduce data redundancy and increase data integrity among users\\r\\n\\r\\nProvide a pragmatic solution to optimize data storage costs\\r\\n\\r\\nThe user can upload raw data to the platform, which are stored on a private local server. The machine\\r\\nlearning algorithms are activated and implement the necessary processing and workflows. Structured\\r\\ndata is generated as output, which are pushed through to a search engine that is accessible to the user\\r\\nin the cloud (Figure 1).\\r\\n\\r\\nFigure 1 The hybrid architecture of platform (ElasticDocs) utilizing the 2-tier strength of local and\\r\\ncloud sever applications for data security, integrity and shareability. Carefully selected machine\\r\\nlearning sequence for automated text and image analysis include: optical character recognition, deep\\r\\nconvolutional neural network and image clustering.\\r\\n\\r\\n\\r\\nMachine Learning:\\r\\nThe platform capitalizes on the machine learning algorithms that automatically process the\\r\\nunstructured data into a condensed format in which only pre-selected information are stored. The\\r\\nmachine learning algorithms employs a unique sequence of separate steps, which are set-up to mimic\\r\\nthe human experience of processing unstructured documents.\\r\\n\\r\\n\\r\\nWorkflow:\\r\\nA Norwegian dataset consisting of 400 well reports (58,000 pages) and an Australian well database\\r\\nconsisting of 6,000 pages have been used as training data for generating structured data.\\r\\n\\r\\nFor the unstructured data the first machine learning step is the digitization and conversion of .pdf or\\r\\n.docx file formats into an editable format. This conversion uses Optical Character Recognition (OCR),\\r\\nwhere the machine identifies each character in the image.\\r\\n\\r\\nAfter the documents are digitized important information has to be identified. This metadata extraction\\r\\nand tagging utilizes Natural Language Processing (NLP) to tokenize each digitized text and identify\\r\\nterms of significant value. Named Entity Recognition (NER) is then performed to create a model to\\r\\nextract the metadata like well name, basin, permit, operator, well classification, latitude, longitude,\\r\\nspudding date, kelly bushing etc.\\r\\n\\r\\nFor the images extracted by the digitization process, a modified VGG-16 neural network is used to\\r\\nautomatically classify tables, charts, stratigraphic chart images, maps, seismic, core samples and\\r\\nscanning electron microscope images within each document (Simonyan ef al., 2014)\\r\\n\\r\\nFor the visualization of the images an at-Distributed Stochastic Neighbor Embedding (t-SNE)\\r\\nalgorithm is used to quantify the similarity of each image, which has been developed to visualize\\r\\nhigh-dimensional datasets and reveal clustering within the datasets (van der Maaten e al., 2008).\\r\\n\\r\\nThe output from the machine learning sequence is then exposed to the users through the platform to\\r\\nease the work of identifying important information and perform analysis in a more efficient way. The\\r\\nextracted information is outputted in an agnostic format, which can be efficiently loaded to other\\r\\nplatforms or used as is, ie. X, Y or Latitude/Longitude, formation tops in csv or excel format,\\r\\ndigitized maps as shapefile for loading into GIS software.\\r\\n\\r\\n\\r\\nDiscussion and conclusion:\\r\\nWells provide key information about the subsurface in oil and gas exploration and production but at a\\r\\nsubstantial cost. As this valuable information associated with a well is often stored as unstructured\\r\\ndata, it is difficult to do further analysis or apply additional artificial intelligence processing to the\\r\\nwell database to enable geoscientists to gain new insights and extract new relationships.\\r\\n\\r\\nThe carefully selected sequence of machine learning algorithms in the workflow deals with these\\r\\nlarge unstructured datasets, is housed within a hybrid cloud platform to automatically extract relevant\\r\\ninformation within technical documents and convert the unstructured data into a shareable structured\\r\\ndataset.\\r\\n\\r\\n\\r\\nReferences:\\r\\nSimonyan, K. & Zisserman, A., 2014: Very Deep Convolutional Networks for Large-scale Image\\r\\nRecognition. arXiv preprint arXiv: 1409.1556\\r\\n\\r\\nvan der Maaten, L.J.P. & Hinton G.E., 2008]: Visualizing High-Dimensional Data using t-SNE.\\r\\nJournal of Machine Learning Research, 9, 2576-2605.\\r\\non Machine Learning\\r\\n\\r\\n\", mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')], 'Supporting the UN 2050 Net Zero goals by reading the earth better': [Document(id_='bcd788b0-95bc-415e-bc0f-5ccb1c850e3a', embedding=None, metadata={'file_path': 'data\\\\Supporting the UN 2050 Net Zero goals by reading the earth better.txt', 'file_name': 'Supporting the UN 2050 Net Zero goals by reading the earth better.txt', 'file_type': 'text/plain', 'file_size': 17134, 'creation_date': '2024-07-04', 'last_modified_date': '2024-07-02'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Title:\\r\\nSupporting the UN 2050 Net Zero goals by reading the earth better\\r\\n\\r\\n\\r\\nAuthors:\\r\\nNina Marie Hernandez\", Kim Gunn Maver and Charmyne Mamador\\r\\n\\r\\n\\r\\nContent:\\r\\n\\r\\nIntroduction:\\r\\nAgainst the backdrop of the current global pandemic, the UN\\r\\n2050 net zero goals call for global greenhouse emissions to be cut\\r\\nby half by 2030 and reach net zero no later than 2050 to achieve\\r\\nthe goal of limiting global warming to 1.5 Celsius above pre-in-\\r\\ndustrial levels. The EU has pledged to become the first carbon\\r\\nneutral continent by 2050, and more than 110 other countries\\r\\nhave pledged carbon neutrality by this time. Several energy com-\\r\\npanies have laid out their medium-to-long-term plans towards\\r\\nthis objective, which includes acquisition of renewables assets,\\r\\nand developing competitive technologies for carbon capture and\\r\\nstorage (CCS) and hydrogen production. Among the companies\\r\\nthat have set zero-emissions targets are BP, Shell, Total, Repsol,\\r\\nEquinor and Petronas. This energy pivot will require significant\\r\\ncapital spending to reach the goals.\\r\\n\\r\\nTo determine the technical and economic feasibility of these\\r\\nnew energy technologies and at the same time achieve sustainable\\r\\ndevelopment goals, multivariate earth data, both existing and\\r\\nnew, are required and are key to making the right management\\r\\nand investment decisions.\\r\\n\\r\\n\\r\\nEarthDoc pointing to the future:\\r\\nTo this end, more than 39 years of conference proceedings and\\r\\npublications are already available from EAGE through Earth-\\r\\nDoc, which aggregates a wealth of subsurface information from\\r\\nresearch institutions, energy companies, service companies\\r\\nand dedicated professionals. The 70,000 scientific publications\\r\\nfocus on conventional topics within geoscience and engineering\\r\\nespecially in relation to oil and gas extraction. This subsurface\\r\\ninformation can be upcycled to provide highly valuable insights\\r\\nfor new energy technologies. The reuse of existing oil and\\r\\ngas data reduces data acquisition costs, which translate into a\\r\\nreduction in research and development costs.\\r\\n\\r\\nThrough a collaboration between Iraya Energies and EAGE, a\\r\\nnew database initiative has been launched with EarthDocs repository of 70,000 scientific publications being processed using the latest in machine learning and artificial intelligence techniques\\r\\nand initially available to institutional and corporate subscribers.\\r\\nThe whole data corpus is made instantly accessible and provides\\r\\nnew tools to search and retrieve the diversity of information that\\r\\none is looking for across any technical discipline.\\r\\n\\r\\nWith Big Data analytics applied to the entire data corpus of\\r\\n70,000 scientific publications, additional in-depth and advanced\\r\\nnavigation options are now available.\\r\\n\\r\\nTo extract information on a large scale, the ElasticDocs AI\\r\\npipeline is applied to the EarthDoc corpus. This pipeline consists\\r\\nof a set of algorithms which are used to identify blocks/segments\\r\\nwithin the document corpus. Optical Character Recognition\\r\\n(OCR) is applied to the text segments to convert them into\\r\\nprocessable text. A Deep Convolutional Neural Network (DCNN)\\r\\nalgorithm pipeline classifies the images into various generic and\\r\\ngeological image classes, including tables, seismic, map, well\\r\\nplots, stratigraphic charts, core, thin sections, image logs, and\\r\\nrose diagrams. Untagged images are generically categorized as\\r\\nfigures and remain accessible to the user.\\r\\n\\r\\nThe ingested documents are now available as structured\\r\\ninformation for analysis, which is possible in different ways:\\r\\n\\r\\n+ Metadata extraction of relevant information such as locations,\\r\\nnames etc.\\r\\nEfficient inter and intra-search of the global text corpus for\\r\\nquantitative textual analysis of document contents to create\\r\\nautomated and standardized geoscience or engineering content\\r\\nsummary.\\r\\nAutomated heatmaps to visualize density of information\\r\\nwithin a basin or country.\\r\\nImage extraction of similar image classes for efficient identification \\r\\nof analogues, duplicates and clusters.\\r\\n\\r\\nSome key functionality that has a significant impact on utilizing\\r\\nthe scientific publications is that not only are images classified\\r\\naccording to type, but it is possible to do a search on image\\r\\nembedded text making the search capabilities far more advanced\\r\\nthan just a normal search-and-find option. The search results of\\r\\nthis database are exportable as .csv files making statistical work\\r\\nand analysis instantly possible across data points.\\r\\n\\r\\n\\r\\nProprietary corporate data meets published scientific knowledge:\\r\\nRecognizing that energy companies hold valuable information in\\r\\ntheir repositories accumulated throughout many decades of oper-\\r\\nations, the data kept internally can now be easily integrated with\\r\\npublished geoscience and engineering data. The combination of\\r\\n\\r\\ndatabase results in a two-way enrichment process between public\\r\\nand private information. Scientific studies from peers in related\\r\\nfields offer additional information that either validates internal\\r\\ncompany studies, or offer alternative technical perspectives from\\r\\nindustry experts.\\r\\n\\r\\nIn the Figure 1 example above, we show how exploring\\r\\ntemperature information available in a Final Well Reports repos-\\r\\nitory that is commonly considered as proprietary corporate data,\\r\\nis combined with published EarthDoc contents. (Data source:\\r\\nNOPIMS)\\r\\n\\r\\nFigure 3 Visualization of knowledge graph illustrates\\r\\nrelated geological and engineering concepts.\\r\\n\\r\\nSeveral Earth readability tools are deployed in ED2K to\\r\\namplify EarthDoc capabilities. This includes access to digital\\r\\ntextual content, the ability to quality control OCR results and\\r\\nread in multiple languages via machine-translate. Word clouds are\\r\\nsimple context clues which provide a quick summary of content\\r\\nof the articles.\\r\\n\\r\\nThe full ED2K corpus is geotagged to more than 800\\r\\ngeological basins around the world. The geotagging is one of the\\r\\nmost complex machine learning tasks in this implementation,\\r\\nbecause the scientific articles contain both location of the\\r\\ngeological area of interest, as well as the location of confer-\\r\\nences where the publications are presented. Often, these two\\r\\nreference locations are different and introduce ambiguity for the\\r\\nsystem.\\r\\n\\r\\nAnother feature that is implemented is the knowledge graph\\r\\nvisualization. It illustrates the connectivity of  related concepts \\r\\nbased on publication references. In the example above, it shows\\r\\n operational excellence ,  optimization  and  engineering suc-\\r\\ncess  within the same network (Figure 3). Similarly,  hydraulic\\r\\nfracture stimulation ,  natural fracture interaction , and  Jurassic\\r\\ncarbonates  show connectivity. In theory, a knowledge graph can\\r\\nbe built in multiple ways by the user.\\r\\n\\r\\n\\r\\nUtilizing ED2K to reach the zero-emission goal\\r\\nThe advanced access in the new database makes possible a new\\r\\nuse of the highly valuable subsurface information and facilitates\\r\\ncross-discipline usage.\\r\\n\\r\\nMany of the new cross-function usages of the existing\\r\\nscientific publication repository are geothermal energy, hydrogen\\r\\nenergy, CCS and windmill foundation derisking.\\r\\n\\r\\nA query for  carbon capture and storage  generates an\\r\\ninformation heat map captured in Figure 4. The map shows the\\r\\ngeographical distribution of the resulting publications either\\r\\nbased on country or basin.\\r\\n\\r\\nFigure 5 maps out the major carbon capture projects around\\r\\nthe world vis-a-vis needs requirement, Europe and US show high\\r\\nactivity in CCS initiatives, which are driven by government poli-\\r\\ncy. Comparing Figure 4 and Figure 5 (left), it may be incidental,\\r\\n\\r\\nalthough not entirely surprising that where there is a significant\\r\\namount of data to aid technical and management decision maKing, \\r\\nCCS implementations are also active.\\r\\n\\r\\nThe new database contains climate change and greenhouse\\r\\ngases industry discussion materials spanning over three decades.\\r\\nAlready between 1990 and 2000, the possibility of disposing\\r\\ncarbon dioxide (CO,) is discussed in papers such as J. Leeb. W,\\r\\n(1993), and Wildenborg, F.B., et. al., (1996), in conjunction with\\r\\nthe use of CO, for improved oil recovery methods (IOR).\\r\\n\\r\\nBetween 2001 and 2010, the discussions tackled issues in\\r\\nestablishing a geological storage hub (Espie, 2000) and various\\r\\npotential site feasibility studies (Gregersen et. al, 2000), costs\\r\\n(Wildenborg et. al, 2000), use of seismic monitoring (Benson,\\r\\n2003), (Gosselet et. al., 2006), pilot and numerical simulations\\r\\n(Domitrovic et. al., 2005), (Battistelli et.al, 2005), reservoir\\r\\nperformance (Broad et.al, 2007) and improving facilities perfor-\\r\\nmance to reduce operating costs in CO, and H,S contaminated\\r\\nfields (Swatton et. al, 2009).\\r\\n\\r\\nFrom 2011 up to the present, with the advancements in\\r\\nseismic methods, reservoir modelling techniques and laboratory\\r\\nexperiments (Bolourinejad, 2013} many more complex analyses\\r\\non the subject of carbon storage were performed. Combined with\\r\\nenhanced oil recovery experiments (EOR), the amount of data\\r\\nmodelling CO, behavior underground has multiplied ten-fold.\\r\\n\\r\\nIn areas where it is not possible to implement subsurface\\r\\ncarbon capture, utilization strategies are discussed by Harsh, A.\\r\\net. al (2014) on the industrial usage of CO, including, but not\\r\\nlimited to, polymer processing and chemicals production.\\r\\n\\r\\nFor a deeper dive in the corpus, we draw an arbitrary areal\\r\\npolygon, indicated by the yellow box in Figure 5, around South\\r\\nEast Asia. Our new database reveals some of the strategies\\r\\nthat have been identified by an operator to manage greenhouse\\r\\nemissions in its operations in Malaysia (Mehta et.al.,2008).\\r\\nThis includes, among others, ending continuous gas flaring\\r\\n\\r\\nFigure 4 Indicative geolocation of global knowledge\\r\\nabout  carbon capture and storage  between 2008\\r\\nto 2020.\\r\\n\\r\\nFigure 5 Location of major carbon capture projects\\r\\naround the world (leff) and the requirement index\\r\\nbased on fossil fuel production and consumption\\r\\n(tight). Reference: Global CCS Institute.\\r\\n\\r\\nand minimizing gas venting, improving energy efficiency in\\r\\nthe design of assets and production operations, accounting for\\r\\nthe cost of emitting greenhouse gases in investment decisions,\\r\\nsupporting development of CCS infrastructures, and policy\\r\\nadvocacy.\\r\\n\\r\\nThe rich diversity of data available in the new database\\r\\nare illustrated in Figures 6 and 7. These include graphical\\r\\ninformation of PVT analyses, miscibility, flow rates, and time-\\r\\nlapse pressure profiles, which are useful for reservoir simulation\\r\\nstudies focused on the interaction of reservoir rocks with CO,\\r\\nduring injection. Also available are petrography data that makes\\r\\nit easier to interpret reservoir modelling results by being able\\r\\nto look at the structural fabric of the storage rocks down to\\r\\nmicroscopic levels.\\r\\n\\r\\n\\r\\nNew energy from old data:\\r\\nWith this data-driven strategy it will be possible to facilitate the\\r\\npivot to new energy from valuable existing data. No part of the\\r\\ndata is left unprocessed. It may be that not all relevant informa-\\r\\ntion will exist within the 70,000 scientific publications, but this\\r\\ncan be confirmed instantly saving valuable time and resources\\r\\ndoing data exploration. On the other hand, if the information is\\r\\navailable, it will be immediately accessible, trackable and put\\r\\ninto context with other relevant information and geographically.\\r\\nFor example, the geothermal gradient is a key parameter of\\r\\ninterest in relation to geothermal energy. The temperature data in\\r\\n\\r\\nthe new database has been acquired by energy companies mostly\\r\\nfor the purpose of understanding the hydrocarbon generation\\r\\nwindow, petrophysical interpretation and reservoir modelling\\r\\nanalyses. They can be relooked at for further exploratory geo-\\r\\nthermal applications. It currently excludes temperature data from\\r\\ngeothermal companies.\\r\\n\\r\\nWe are barely scratching the surface on the data and insights\\r\\navailable   multiple data stories are waiting to be reimagined,\\r\\nreconnected and retold in the context of the future of energy.\\r\\n\\r\\n\\r\\nAccelerating internal digitalization initiatives:\\r\\nAll the elements of the new database are stored and structured\\r\\nin a digital data warehouse. They will be optionally available\\r\\nas an API link to be used for additional geological analysis,\\r\\ndata analytics, or machine learning experimentations. Already\\r\\nin structured format, they can be fed into additional natural lan-\\r\\nguage processing or image segmentation processing for in-house\\r\\nexperimentation.\\r\\n\\r\\n\\r\\nOpportunity for the energy geoscientists and engineers of the future:\\r\\nWhile the energy industry has faced significant headwinds, it is\\r\\nnow moving faster than ever towards new, cleaner energy pro-\\r\\nduction. It is possible to see that multiple opportunities remain,\\r\\nas already pointed out by Raistrick (2008), and remain relevant in\\r\\n2021, for the geoscientists and engineers who are looking at the\\r\\n\\r\\nFigure 6 Experimental engineering data of CO,\\r\\nbehaviours in enhanced oil recovery operations of\\r\\nmature fields can be transferable to carbon storage\\r\\ndesign and monitoring.\\r\\n\\r\\nFigure 8 Locations of relatively  high geothermal gradient areas  based on existing\\r\\ncorpus, data acquired by oil and gas companies. Data excludes information from\\r\\ngeothermal companies.\\r\\n\\r\\nFigure 9 Compiled graphical temperature information filtered by country, basin or\\r\\nan arbitrary polygon location. Reference: Geoscience Australia, NOPIMS.\\r\\n\\r\\nfuture of energy. Strong, flexible technical skills will be needed\\r\\nto explore for suitable carbon capture facilities, assess their\\r\\nstorage, containment and injectivity capacities. Meanwhile, the\\r\\nnew energy industry will continue to gather, integrate and analyse\\r\\nempirical data, whether it is on the reservoir, sub-surface or at\\r\\nhydrocarbon or future hydrogen production facilities.\\r\\n\\r\\nWe have already seen a lot of data. It is up to us to use the\\r\\nright tools to read the earth better, and get a head start towards\\r\\nnew energy.\\r\\n\\r\\n\\r\\nReferences:\\r\\nGlobal CCS Institute (https://www.globalcesinstitute.com). CCS Facil-\\r\\nities Database (https://co2re.co/) Geoscience Australia, NOPIMS\\r\\n(http://www.ga.gov.aw/nopims).\\r\\n\\r\\nBolourinejad, P. and Herber, R. [2013]. Experimental and Modeling Study\\r\\nof Salt Precipitation during Injection of CO2 Contaminated with H2S\\r\\ninto Depleted Gas Fields in Northeast Netherlands - (SPE-164932),\\r\\n75th EAGE Conference & Exhibition incorporating SPE EUROPEC\\r\\n2013, London, UK.\\r\\n\\r\\nBattistelli, A., Giorgis, T. and Marzorati, D. [2005]. Modeling Halite Pre-\\r\\ncipitation around CO2 Injection Wells in Depleted Gas Reservoirs,\\r\\n67th EAGE Conference & Exhibition, Madrid, Spain.\\r\\n\\r\\nBroad, J., Ab Majid, M.N., Ariffin, T., Hussain, A. and Basher, A.B.\\r\\n[2007]. Deposition of  Asphaltenes  during CO2 Injection and\\r\\nImplications for EOS Description and Reservoir Performance, IPT\\r\\n2007: International Petroleum Technology Conference, Dubai, Unit-\\r\\ned Arab Emirates.\\r\\n\\r\\nDomitrovic, D., Tuschl, M. and Sunjerga, S. [2005]. CO2 Pilot Injection\\r\\nat Ivanic Oil Field   Numerical Simulation, IOR 2005 - 13th Euro-\\r\\npean Symposium on Improved Oil Recovery, Budapest, Hungary.\\r\\n\\r\\nGosselet, A.C. and Singh,  . [2006]. Elastic Full Waveform Inversion\\r\\nfor CO2 Sequestration monitoring - ID Synthetic Data Investiga-\\r\\ntions,68th EAGE Conference and Exhibition incorporating SPE\\r\\nEUROPEC 2006\\r\\n\\r\\nGregersen, U.N., Johannessen, P., Kirby, G., Chadwick, A. and Holloway,\\r\\nS. [2000]. Regional Study of the Neogene Deposits in the Southem\\r\\nViking Graben Area - a Site for Potential CO2 Storage,62nd EAGE\\r\\nConference and Exhibition - Special Session on CO2, Glasgow, UK.\\r\\n\\r\\nHarsh, A.H. and Anne, V.A. [2014]. Carbon Dioxide Capture, Utilization\\r\\nand Storage (CCUS),76th EAGE Conference and Exhibition 2014,\\r\\nAmsterdam, Netherlands.\\r\\n\\r\\nLeeb, W. [1993]. Case study of CO2 disposal in aquifers - A solution\\r\\nto reduce the greenhouse effect, 55th EAEG Meeting, Stavanger,\\r\\nNorway.\\r\\n\\r\\nMehta, A., Hj-Kip, S. and Foo, J. [2008]. Managing Greenhouse Gas\\r\\nEmissions in Upstream Operations in a Carbon-Constrained World,\\r\\nIPTC 2008: International Petroleum Technology Conference, Kuala\\r\\nLumpur, Malaysia\\r\\n\\r\\nRahim, M., Azran, A., Press, D., Lee, K-H., Phuat, C.T., Anis, L.,\\r\\nDarman, N. and Othman, M. [2012]. An Integrated Reservoir Sim-\\r\\nulation-Geomechanical Study on Feasibility of CO2 Storage in M4\\r\\nCarbonate Reservoir, Malaysia, IPTC 2012: Intemational Petroleum\\r\\nTechnology Conference, Bangkok, Thailand.\\r\\n\\r\\nRaistrick, M. [2008]. Carbon capture and storage projects to challenge\\r\\ngovernments, scientists, and engineers, First Break.\\r\\n\\r\\nWildenborg, A. G.H.and van der Meer L. [1996]. Potential\\r\\nof CO2-disposal in deep reservoirs and aquifers of the Nether-\\r\\nJands,58th EAGE Conference and Exhibition, Netherlands.\\r\\n\\r\\nWildenborg, A., Floris, FD., van Wees, J. and Hendriks, C. [2000].\\r\\nCosts of CO2 Sequestration by Underground Storage,62nd EAGE\\r\\nConference and Exhibition - Special Session on CO2, Glasgow,\\r\\nUK.\\r\\n\\r\\nSwatton, M.J.R., van Soest-Vercammen, E. and Klein Nagelvoort, R.\\r\\n\\r\\n, Breune:\\r\\n\\r\\n[2009]. Innovation and Integration in LNG Technology Solutions,\\r\\nIPTC 2009: International Petroleum Technology Conference, Doha,\\r\\nQatar.\\r\\n\\r\\n\\r\\n', mimetype='text/plain', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189aaf4-2eb7-40bc-9e83-79ce4f221b4b",
   "metadata": {
    "id": "6189aaf4-2eb7-40bc-9e83-79ce4f221b4b"
   },
   "source": [
    "Define Global LLM and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dd6e5e48-91b9-4701-a85d-d98c92323350",
   "metadata": {
    "id": "dd6e5e48-91b9-4701-a85d-d98c92323350",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:35.340045200Z",
     "start_time": "2024-07-04T08:44:32.065592700Z"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"phi3:medium\",\n",
    "    base_url=\"https://7a6b-35-201-236-203.ngrok-free.app\",\n",
    "    request_timeout=200.0\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eeef31a-fc25-4367-a5ba-945f81d04cf9",
   "metadata": {
    "id": "4eeef31a-fc25-4367-a5ba-945f81d04cf9"
   },
   "source": [
    "## Building Multi-Document Agents\n",
    "\n",
    "In this section we show you how to construct the multi-document agent. We first build a document agent for each document, and then define the top-level parent agent with an object index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976cd798-2e8d-474c-922a-51b12c5c6f36",
   "metadata": {
    "id": "976cd798-2e8d-474c-922a-51b12c5c6f36"
   },
   "source": [
    "### Build Document Agent for each Document\n",
    "\n",
    "In this section we define \"document agents\" for each document.\n",
    "\n",
    "We define both a vector index (for semantic search) and summary index (for summarization) for each document. The two query engines are then converted into tools that are passed to an OpenAI function calling agent.\n",
    "\n",
    "This document agent can dynamically choose to perform semantic search or summarization within a given document.\n",
    "\n",
    "We create a separate document agent for each city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "eacdf3a7-cfe3-4c2b-9037-b28a065ed148",
   "metadata": {
    "id": "eacdf3a7-cfe3-4c2b-9037-b28a065ed148",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:35.620214Z",
     "start_time": "2024-07-04T08:44:35.335061100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document_1\n",
      "Document_2\n",
      "Document_3\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import load_index_from_storage, StorageContext\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import os\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1500, chunk_overlap=300)\n",
    "\n",
    "# Build agents dictionary\n",
    "agents = {}\n",
    "query_engines = {}\n",
    "\n",
    "# this is for the baseline\n",
    "all_nodes = []\n",
    "\n",
    "for idx, wiki_title in enumerate(wiki_titles):\n",
    "    nodes = node_parser.get_nodes_from_documents(city_docs[wiki_title])\n",
    "    all_nodes.extend(nodes)\n",
    "\n",
    "    if not os.path.exists(f\"./data/{wiki_title}\"):\n",
    "        # build vector index\n",
    "        vector_index = VectorStoreIndex(nodes)\n",
    "        vector_index.storage_context.persist(\n",
    "            persist_dir=f\"./data/{wiki_title}\"\n",
    "        )\n",
    "    else:\n",
    "        vector_index = load_index_from_storage(\n",
    "            StorageContext.from_defaults(persist_dir=f\"./data/{wiki_title}\"),\n",
    "        )\n",
    "\n",
    "    # build summary index\n",
    "    summary_index = SummaryIndex(nodes)\n",
    "    # define query engines\n",
    "    vector_query_engine = vector_index.as_query_engine(llm=Settings.llm)\n",
    "    summary_query_engine = summary_index.as_query_engine(llm=Settings.llm)\n",
    "    \n",
    "    s = f\"Document_{idx+1}\"\n",
    "    \n",
    "    # define tools\n",
    "    query_engine_tools = [\n",
    "        QueryEngineTool(\n",
    "            query_engine=vector_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"vector_tool\",\n",
    "                description=(\n",
    "                    f\"Useful for questions related to specific aspects of {s}\"\n",
    "                    f\" {wiki_title} (e.g. the title, authors,\"\n",
    "                    \" content, references, or more).\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "        QueryEngineTool(\n",
    "            query_engine=summary_query_engine,\n",
    "            metadata=ToolMetadata(\n",
    "                name=\"summary_tool\",\n",
    "                description=(\n",
    "                    \"Useful for any requests that require a holistic summary\"\n",
    "                    f\" of EVERYTHING about {s} which is about {wiki_title}. For questions about\"\n",
    "                    \" more specific sections, please use the vector_tool.\"\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # build agent\n",
    "    function_llm = Settings.llm\n",
    "    agent = ReActAgent.from_tools(\n",
    "        query_engine_tools,\n",
    "        llm=function_llm,\n",
    "        verbose=True,\n",
    "        system_prompt=f\"\"\"\\\n",
    "You are a specialized agent designed to answer queries about {s} which is about {wiki_title}.\n",
    "You must ALWAYS use at least one of the tools provided when answering a question; do NOT rely on prior knowledge.\\\n",
    "\"\"\",\n",
    "    )\n",
    "    \n",
    "    print(s)\n",
    "    agents[s] = agent\n",
    "    query_engines[s] = vector_index.as_query_engine(\n",
    "        similarity_top_k=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899ca55b-0c02-429b-a765-8e4f806d503f",
   "metadata": {
    "id": "899ca55b-0c02-429b-a765-8e4f806d503f"
   },
   "source": [
    "### Build Retriever-Enabled OpenAI Agent\n",
    "\n",
    "We build a top-level agent that can orchestrate across the different document agents to answer any user query.\n",
    "\n",
    "This agent takes in all document agents as tools. This specific agent `RetrieverOpenAIAgent` performs tool retrieval before tool use (unlike a default agent that tries to put all tools in the prompt).\n",
    "\n",
    "Here we use a top-k retriever, but we encourage you to customize the tool retriever method!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6884ff15-bf40-4bdd-a1e3-58cbd056a12a",
   "metadata": {
    "id": "6884ff15-bf40-4bdd-a1e3-58cbd056a12a",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:35.622212600Z",
     "start_time": "2024-07-04T08:44:35.543470200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document_1\n",
      "Document_2\n",
      "Document_3\n"
     ]
    }
   ],
   "source": [
    "# define tool for each document agent\n",
    "all_tools = []\n",
    "for idx, wiki_title in enumerate(wiki_titles):\n",
    "    s = f\"Document_{idx+1}\"\n",
    "    wiki_summary = (\n",
    "        f\"This content contains {s} information about {wiki_title}. Use\"\n",
    "        f\" this tool if you want to answer any questions about {wiki_title}.\\n\"\n",
    "    )\n",
    "    print(s)\n",
    "    doc_tool = QueryEngineTool(\n",
    "        query_engine=agents[s],\n",
    "        metadata=ToolMetadata(\n",
    "            name=f\"tool_{s}\",\n",
    "            description=wiki_summary,\n",
    "        ),\n",
    "    )\n",
    "    all_tools.append(doc_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "da80793c-54b9-43e2-b34a-27412156268a",
   "metadata": {
    "id": "da80793c-54b9-43e2-b34a-27412156268a",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:35.788895600Z",
     "start_time": "2024-07-04T08:44:35.557864800Z"
    }
   },
   "outputs": [],
   "source": [
    "# define an \"object\" index and retriever over these tools\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "obj_index = ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "content = \"\"\n",
    "for idx, doc in enumerate(wiki_titles):\n",
    "    content += f\"DOCUMENT {idx+1} refers to {doc}\\n\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:35.810073700Z",
     "start_time": "2024-07-04T08:44:35.644462400Z"
    }
   },
   "id": "f65a1fb9acec10a3",
   "execution_count": 145
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fed38942-1e37-4c61-89fa-d2ef41151831",
   "metadata": {
    "id": "fed38942-1e37-4c61-89fa-d2ef41151831",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:35.844671700Z",
     "start_time": "2024-07-04T08:44:35.723614100Z"
    }
   },
   "outputs": [],
   "source": [
    "top_agent = ReActAgent.from_tools(\n",
    "    tool_retriever=obj_index.as_retriever(similarity_top_k=1),\n",
    "    system_prompt=f\"\"\" \\\n",
    "You are an agent designed to answer queries about a set of research papers.\n",
    "Please always use the tools provided to answer a question. Do not rely on prior knowledge.\n",
    "Additionally, {content}. \\n if any of those are called, use the tools depending on their document number.\n",
    "\\\n",
    "\n",
    "\"\"\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32b97c-6779-4b60-823d-6ca3be6f358a",
   "metadata": {
    "id": "aa32b97c-6779-4b60-823d-6ca3be6f358a"
   },
   "source": [
    "### Define Baseline Vector Store Index\n",
    "\n",
    "As a point of comparison, we define a \"naive\" RAG pipeline which dumps all docs into a single vector index collection.\n",
    "\n",
    "We set the top_k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "78e7b48c-687a-4585-a730-e651212de3b5",
   "metadata": {
    "id": "78e7b48c-687a-4585-a730-e651212de3b5",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:36.268427300Z",
     "start_time": "2024-07-04T08:44:36.031726800Z"
    }
   },
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex(all_nodes)\n",
    "base_query_engine = base_index.as_query_engine(similarity_top_k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedb927-a992-4f21-a0fb-4ce4361adcb3",
   "metadata": {
    "id": "8dedb927-a992-4f21-a0fb-4ce4361adcb3"
   },
   "source": [
    "## Running Example Queries\n",
    "\n",
    "Let's run some example queries, ranging from QA / summaries over a single document to QA / summarization over multiple documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8e743c62-7dd8-4ac9-85a5-f1cbc112a79c",
   "metadata": {
    "id": "8e743c62-7dd8-4ac9-85a5-f1cbc112a79c",
    "outputId": "c309dcdc-8e2f-4376-fbc7-bbb9dc03c689",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:39.269863700Z",
     "start_time": "2024-07-04T08:44:37.483281500Z"
    }
   },
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '404 Not Found' for url 'https://7a6b-35-201-236-203.ngrok-free.app/api/chat'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[148], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# should use Boston agent -> vector tool\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mtop_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mTell me the author of the Bonaparte Basin Paper\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m callback_manager \u001B[38;5;241m=\u001B[39m cast(CallbackManager, callback_manager)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m callback_manager\u001B[38;5;241m.\u001B[39mas_trace(trace_id):\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:640\u001B[0m, in \u001B[0;36mAgentRunner.chat\u001B[1;34m(self, message, chat_history, tool_choice)\u001B[0m\n\u001B[0;32m    635\u001B[0m     tool_choice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_tool_choice\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[0;32m    637\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mAGENT_STEP,\n\u001B[0;32m    638\u001B[0m     payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mMESSAGES: [message]},\n\u001B[0;32m    639\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 640\u001B[0m     chat_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    641\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    642\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchat_history\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchat_history\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    643\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    644\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatResponseMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWAIT\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    645\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    646\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chat_response, AgentChatResponse)\n\u001B[0;32m    647\u001B[0m     e\u001B[38;5;241m.\u001B[39mon_end(payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mRESPONSE: chat_response})\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:572\u001B[0m, in \u001B[0;36mAgentRunner._chat\u001B[1;34m(self, message, chat_history, tool_choice, mode)\u001B[0m\n\u001B[0;32m    569\u001B[0m dispatcher\u001B[38;5;241m.\u001B[39mevent(AgentChatWithStepStartEvent(user_msg\u001B[38;5;241m=\u001B[39mmessage))\n\u001B[0;32m    570\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     cur_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    573\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtool_choice\u001B[49m\n\u001B[0;32m    574\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    576\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cur_step_output\u001B[38;5;241m.\u001B[39mis_last:\n\u001B[0;32m    577\u001B[0m         result_output \u001B[38;5;241m=\u001B[39m cur_step_output\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:411\u001B[0m, in \u001B[0;36mAgentRunner._run_step\u001B[1;34m(self, task_id, step, input, mode, **kwargs)\u001B[0m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001B[39;00m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;66;03m# not clear when you would do that by theoretically possible\u001B[39;00m\n\u001B[0;32m    410\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m ChatResponseMode\u001B[38;5;241m.\u001B[39mWAIT:\n\u001B[1;32m--> 411\u001B[0m     cur_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent_worker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m ChatResponseMode\u001B[38;5;241m.\u001B[39mSTREAM:\n\u001B[0;32m    413\u001B[0m     cur_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_worker\u001B[38;5;241m.\u001B[39mstream_step(step, task, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m callback_manager \u001B[38;5;241m=\u001B[39m cast(CallbackManager, callback_manager)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m callback_manager\u001B[38;5;241m.\u001B[39mas_trace(trace_id):\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\react\\step.py:746\u001B[0m, in \u001B[0;36mReActAgentWorker.run_step\u001B[1;34m(self, step, task, **kwargs)\u001B[0m\n\u001B[0;32m    743\u001B[0m \u001B[38;5;129m@trace_method\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_step\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, step: TaskStep, task: Task, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m TaskStepOutput:\n\u001B[0;32m    745\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run step.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 746\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\react\\step.py:538\u001B[0m, in \u001B[0;36mReActAgentWorker._run_step\u001B[1;34m(self, step, task)\u001B[0m\n\u001B[0;32m    530\u001B[0m input_chat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_react_chat_formatter\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    531\u001B[0m     tools,\n\u001B[0;32m    532\u001B[0m     chat_history\u001B[38;5;241m=\u001B[39mtask\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39mtask\u001B[38;5;241m.\u001B[39minput)\n\u001B[0;32m    533\u001B[0m     \u001B[38;5;241m+\u001B[39m task\u001B[38;5;241m.\u001B[39mextra_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_memory\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mget_all(),\n\u001B[0;32m    534\u001B[0m     current_reasoning\u001B[38;5;241m=\u001B[39mtask\u001B[38;5;241m.\u001B[39mextra_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent_reasoning\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    535\u001B[0m )\n\u001B[0;32m    537\u001B[0m \u001B[38;5;66;03m# send prompt\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m chat_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_llm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_chat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;66;03m# given react prompt outputs, call tools or return response\u001B[39;00m\n\u001B[0;32m    540\u001B[0m reasoning_steps, is_done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_actions(\n\u001B[0;32m    541\u001B[0m     task, tools, output\u001B[38;5;241m=\u001B[39mchat_response\n\u001B[0;32m    542\u001B[0m )\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:172\u001B[0m, in \u001B[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001B[1;34m(_self, messages, **kwargs)\u001B[0m\n\u001B[0;32m    163\u001B[0m event_id \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_event_start(\n\u001B[0;32m    164\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mLLM,\n\u001B[0;32m    165\u001B[0m     payload\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    169\u001B[0m     },\n\u001B[0;32m    170\u001B[0m )\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 172\u001B[0m     f_return_val \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_self\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    174\u001B[0m     callback_manager\u001B[38;5;241m.\u001B[39mon_event_end(\n\u001B[0;32m    175\u001B[0m         CBEventType\u001B[38;5;241m.\u001B[39mLLM,\n\u001B[0;32m    176\u001B[0m         payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mEXCEPTION: e},\n\u001B[0;32m    177\u001B[0m         event_id\u001B[38;5;241m=\u001B[39mevent_id,\n\u001B[0;32m    178\u001B[0m     )\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:135\u001B[0m, in \u001B[0;36mOllama.chat\u001B[1;34m(self, messages, **kwargs)\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mClient(timeout\u001B[38;5;241m=\u001B[39mTimeout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_timeout)) \u001B[38;5;28;01mas\u001B[39;00m client:\n\u001B[0;32m    131\u001B[0m     response \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mpost(\n\u001B[0;32m    132\u001B[0m         url\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/api/chat\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    133\u001B[0m         json\u001B[38;5;241m=\u001B[39mpayload,\n\u001B[0;32m    134\u001B[0m     )\n\u001B[1;32m--> 135\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m     raw \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[0;32m    137\u001B[0m     message \u001B[38;5;241m=\u001B[39m raw[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\httpx\\_models.py:761\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    759\u001B[0m error_type \u001B[38;5;241m=\u001B[39m error_types\u001B[38;5;241m.\u001B[39mget(status_class, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid status code\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    760\u001B[0m message \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m, error_type\u001B[38;5;241m=\u001B[39merror_type)\n\u001B[1;32m--> 761\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTPStatusError(message, request\u001B[38;5;241m=\u001B[39mrequest, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPStatusError\u001B[0m: Client error '404 Not Found' for url 'https://7a6b-35-201-236-203.ngrok-free.app/api/chat'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404"
     ]
    }
   ],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = top_agent.chat(\"Tell me the author of the Bonaparte Basin Paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce2a76-5779-4acf-9337-69109dae7fd6",
   "metadata": {
    "id": "a4ce2a76-5779-4acf-9337-69109dae7fd6",
    "outputId": "1dc0b8ec-d892-407b-db4d-afe2df404489",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "start_time": "2024-07-04T08:44:39.182163300Z"
    }
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28b422-fb73-4b59-9e77-3ba3afa87795",
   "metadata": {
    "id": "af28b422-fb73-4b59-9e77-3ba3afa87795",
    "outputId": "874d131a-3db1-4922-d867-c48a0a6daf6f",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "start_time": "2024-07-04T08:44:39.188163700Z"
    }
   },
   "outputs": [],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = top_agent.chat(\"What was used as Datas in the paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The introduction of document 1 is about a case study that uses machine learning techniques to analyze unstructured data from the Bonaparte Basin, leading to improvements in analysis and decision-making time.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:39.471595300Z",
     "start_time": "2024-07-04T08:44:39.378034400Z"
    }
   },
   "id": "6e5ad8d657a9a717",
   "execution_count": 149
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "aa3d98ab-cb82-4473-ab2b-bc8a17e1b86a",
   "metadata": {
    "id": "aa3d98ab-cb82-4473-ab2b-bc8a17e1b86a",
    "outputId": "57749970-4d47-4cd0-b83a-e00068469b98",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:44:41.944593700Z",
     "start_time": "2024-07-04T08:44:40.305830600Z"
    }
   },
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '404 Not Found' for url 'https://7a6b-35-201-236-203.ngrok-free.app/api/chat'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mHTTPStatusError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[150], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# should use Boston agent -> vector tool\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mtop_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mHow may wells were used in the paper?\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m callback_manager \u001B[38;5;241m=\u001B[39m cast(CallbackManager, callback_manager)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m callback_manager\u001B[38;5;241m.\u001B[39mas_trace(trace_id):\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:640\u001B[0m, in \u001B[0;36mAgentRunner.chat\u001B[1;34m(self, message, chat_history, tool_choice)\u001B[0m\n\u001B[0;32m    635\u001B[0m     tool_choice \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault_tool_choice\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_manager\u001B[38;5;241m.\u001B[39mevent(\n\u001B[0;32m    637\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mAGENT_STEP,\n\u001B[0;32m    638\u001B[0m     payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mMESSAGES: [message]},\n\u001B[0;32m    639\u001B[0m ) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 640\u001B[0m     chat_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_chat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    641\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmessage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmessage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    642\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchat_history\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchat_history\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    643\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    644\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatResponseMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mWAIT\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    645\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    646\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chat_response, AgentChatResponse)\n\u001B[0;32m    647\u001B[0m     e\u001B[38;5;241m.\u001B[39mon_end(payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mRESPONSE: chat_response})\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:572\u001B[0m, in \u001B[0;36mAgentRunner._chat\u001B[1;34m(self, message, chat_history, tool_choice, mode)\u001B[0m\n\u001B[0;32m    569\u001B[0m dispatcher\u001B[38;5;241m.\u001B[39mevent(AgentChatWithStepStartEvent(user_msg\u001B[38;5;241m=\u001B[39mmessage))\n\u001B[0;32m    570\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;66;03m# pass step queue in as argument, assume step executor is stateless\u001B[39;00m\n\u001B[1;32m--> 572\u001B[0m     cur_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_step\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    573\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtool_choice\u001B[49m\n\u001B[0;32m    574\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    576\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m cur_step_output\u001B[38;5;241m.\u001B[39mis_last:\n\u001B[0;32m    577\u001B[0m         result_output \u001B[38;5;241m=\u001B[39m cur_step_output\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\runner\\base.py:411\u001B[0m, in \u001B[0;36mAgentRunner._run_step\u001B[1;34m(self, task_id, step, input, mode, **kwargs)\u001B[0m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;66;03m# TODO: figure out if you can dynamically swap in different step executors\u001B[39;00m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;66;03m# not clear when you would do that by theoretically possible\u001B[39;00m\n\u001B[0;32m    410\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m ChatResponseMode\u001B[38;5;241m.\u001B[39mWAIT:\n\u001B[1;32m--> 411\u001B[0m     cur_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent_worker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    412\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m ChatResponseMode\u001B[38;5;241m.\u001B[39mSTREAM:\n\u001B[0;32m    413\u001B[0m     cur_step_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39magent_worker\u001B[38;5;241m.\u001B[39mstream_step(step, task, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\callbacks\\utils.py:41\u001B[0m, in \u001B[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     39\u001B[0m callback_manager \u001B[38;5;241m=\u001B[39m cast(CallbackManager, callback_manager)\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m callback_manager\u001B[38;5;241m.\u001B[39mas_trace(trace_id):\n\u001B[1;32m---> 41\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\react\\step.py:746\u001B[0m, in \u001B[0;36mReActAgentWorker.run_step\u001B[1;34m(self, step, task, **kwargs)\u001B[0m\n\u001B[0;32m    743\u001B[0m \u001B[38;5;129m@trace_method\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_step\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    744\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, step: TaskStep, task: Task, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m TaskStepOutput:\n\u001B[0;32m    745\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Run step.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 746\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\agent\\react\\step.py:538\u001B[0m, in \u001B[0;36mReActAgentWorker._run_step\u001B[1;34m(self, step, task)\u001B[0m\n\u001B[0;32m    530\u001B[0m input_chat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_react_chat_formatter\u001B[38;5;241m.\u001B[39mformat(\n\u001B[0;32m    531\u001B[0m     tools,\n\u001B[0;32m    532\u001B[0m     chat_history\u001B[38;5;241m=\u001B[39mtask\u001B[38;5;241m.\u001B[39mmemory\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39mtask\u001B[38;5;241m.\u001B[39minput)\n\u001B[0;32m    533\u001B[0m     \u001B[38;5;241m+\u001B[39m task\u001B[38;5;241m.\u001B[39mextra_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnew_memory\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mget_all(),\n\u001B[0;32m    534\u001B[0m     current_reasoning\u001B[38;5;241m=\u001B[39mtask\u001B[38;5;241m.\u001B[39mextra_state[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcurrent_reasoning\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[0;32m    535\u001B[0m )\n\u001B[0;32m    537\u001B[0m \u001B[38;5;66;03m# send prompt\u001B[39;00m\n\u001B[1;32m--> 538\u001B[0m chat_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_llm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchat\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_chat\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    539\u001B[0m \u001B[38;5;66;03m# given react prompt outputs, call tools or return response\u001B[39;00m\n\u001B[0;32m    540\u001B[0m reasoning_steps, is_done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_actions(\n\u001B[0;32m    541\u001B[0m     task, tools, output\u001B[38;5;241m=\u001B[39mchat_response\n\u001B[0;32m    542\u001B[0m )\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:230\u001B[0m, in \u001B[0;36mDispatcher.span.<locals>.wrapper\u001B[1;34m(func, instance, args, kwargs)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspan_enter(\n\u001B[0;32m    227\u001B[0m     id_\u001B[38;5;241m=\u001B[39mid_, bound_args\u001B[38;5;241m=\u001B[39mbound_args, instance\u001B[38;5;241m=\u001B[39minstance, parent_id\u001B[38;5;241m=\u001B[39mparent_id\n\u001B[0;32m    228\u001B[0m )\n\u001B[0;32m    229\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 230\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    231\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    232\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevent(SpanDropEvent(span_id\u001B[38;5;241m=\u001B[39mid_, err_str\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(e)))\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\core\\llms\\callbacks.py:172\u001B[0m, in \u001B[0;36mllm_chat_callback.<locals>.wrap.<locals>.wrapped_llm_chat\u001B[1;34m(_self, messages, **kwargs)\u001B[0m\n\u001B[0;32m    163\u001B[0m event_id \u001B[38;5;241m=\u001B[39m callback_manager\u001B[38;5;241m.\u001B[39mon_event_start(\n\u001B[0;32m    164\u001B[0m     CBEventType\u001B[38;5;241m.\u001B[39mLLM,\n\u001B[0;32m    165\u001B[0m     payload\u001B[38;5;241m=\u001B[39m{\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    169\u001B[0m     },\n\u001B[0;32m    170\u001B[0m )\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 172\u001B[0m     f_return_val \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_self\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    173\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    174\u001B[0m     callback_manager\u001B[38;5;241m.\u001B[39mon_event_end(\n\u001B[0;32m    175\u001B[0m         CBEventType\u001B[38;5;241m.\u001B[39mLLM,\n\u001B[0;32m    176\u001B[0m         payload\u001B[38;5;241m=\u001B[39m{EventPayload\u001B[38;5;241m.\u001B[39mEXCEPTION: e},\n\u001B[0;32m    177\u001B[0m         event_id\u001B[38;5;241m=\u001B[39mevent_id,\n\u001B[0;32m    178\u001B[0m     )\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\llama_index\\llms\\ollama\\base.py:135\u001B[0m, in \u001B[0;36mOllama.chat\u001B[1;34m(self, messages, **kwargs)\u001B[0m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mClient(timeout\u001B[38;5;241m=\u001B[39mTimeout(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_timeout)) \u001B[38;5;28;01mas\u001B[39;00m client:\n\u001B[0;32m    131\u001B[0m     response \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mpost(\n\u001B[0;32m    132\u001B[0m         url\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/api/chat\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    133\u001B[0m         json\u001B[38;5;241m=\u001B[39mpayload,\n\u001B[0;32m    134\u001B[0m     )\n\u001B[1;32m--> 135\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    136\u001B[0m     raw \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mjson()\n\u001B[0;32m    137\u001B[0m     message \u001B[38;5;241m=\u001B[39m raw[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\OJTChatbotBEIC\\.venv\\Lib\\site-packages\\httpx\\_models.py:761\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    759\u001B[0m error_type \u001B[38;5;241m=\u001B[39m error_types\u001B[38;5;241m.\u001B[39mget(status_class, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid status code\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    760\u001B[0m message \u001B[38;5;241m=\u001B[39m message\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mself\u001B[39m, error_type\u001B[38;5;241m=\u001B[39merror_type)\n\u001B[1;32m--> 761\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HTTPStatusError(message, request\u001B[38;5;241m=\u001B[39mrequest, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[1;31mHTTPStatusError\u001B[0m: Client error '404 Not Found' for url 'https://7a6b-35-201-236-203.ngrok-free.app/api/chat'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404"
     ]
    }
   ],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = top_agent.chat(\"How may wells were used in the paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d476c54b-98af-4d2a-8f17-4baa37d0d360",
   "metadata": {
    "id": "d476c54b-98af-4d2a-8f17-4baa37d0d360",
    "outputId": "3734c830-17f4-4884-e677-a1b01757cb86",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:03:05.376424100Z",
     "start_time": "2024-07-04T08:03:05.351921600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paper mentions 440 wells.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c89b3581-ff70-4069-8cbb-f2791756128a",
   "metadata": {
    "id": "c89b3581-ff70-4069-8cbb-f2791756128a",
    "outputId": "dab1a1e2-ceb7-4969-cf7b-9d648285b284",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "ExecuteTime": {
     "end_time": "2024-07-04T08:08:11.645032700Z",
     "start_time": "2024-07-04T08:06:49.901519600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;3;38;5;200mThought: I need to use a tool to help me answer this question.\n",
      "Action: tool_Document_3\n",
      "Action Input: {'input': 'Document 2', 'type': 'object'}\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: The current input is \"Document 2\". I need to use a tool to help me answer the question.\n",
      "Action: summary_tool\n",
      "Action Input: {'input': 'Document 2'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: Strong, flexible technical skills will be needed to explore for suitable carbon capture facilities, assess their storage, containment and injectivity capacities. Meanwhile, the new energy industry will continue to gather, integrate and analyse empirical data, whether it is on the reservoir, sub-surface or at hydrocarbon or future hydrogen production facilities.\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: The user provided some text related to Document 2. I'll use the vector_tool to help me analyze this specific section.\n",
      "Action: vector_tool\n",
      "Action Input: {'input': 'Strong, flexible technical skills will be needed to explore for suitable carbon capture facilities, assess their storage, containment and injectivity capacities. Meanwhile, the new energy industry will continue to gather, integrate and analyse empirical data, whether it is on the reservoir, sub-surface or at hydrocarbon or future hydrogen production facilities.'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: To accelerate the transition towards cleaner energy, a fundamental understanding of geological storage hubs and potential site feasibility studies will be crucial. This requires strong, flexible technical skills to explore for suitable carbon capture facilities, assess their storage, containment, and injectivity capacities. Additionally, the ability to integrate and analyze empirical data from various sources, including reservoir, sub-surface, and hydrocarbon or future hydrogen production facilities, will be essential for informing decision-making and optimizing operations.\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: The user provided more text related to Document 2. I'll use the summary_tool to help me summarize this information.\n",
      "Action: summary_tool\n",
      "Action Input: {'input': 'To accelerate the transition towards cleaner energy, a fundamental understanding of geological storage hubs and potential site feasibility studies will be crucial.'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: Strong technical skills will be essential for exploring suitable carbon capture facilities, assessing their storage, containment, and injectivity capacities. Engineers can leverage the rich diversity of data available to accelerate the transition towards cleaner energy. Geological storage hubs and potential site feasibility studies will play a crucial role in this effort.\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: I think I have enough information to answer the question without using any more tools.\n",
      "Answer: It seems that Document 2 is discussing the importance of strong technical skills, particularly geological knowledge, for exploring suitable carbon capture facilities and assessing their capabilities. The text also emphasizes the need for engineers to leverage diverse data sources to accelerate the transition towards cleaner energy.\n",
      "\u001B[0m\u001B[1;3;34mObservation: It seems that Document 2 is discussing the importance of strong technical skills, particularly geological knowledge, for exploring suitable carbon capture facilities and assessing their capabilities. The text also emphasizes the need for engineers to leverage diverse data sources to accelerate the transition towards cleaner energy.\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: It seems that Document 2 is discussing the importance of strong technical skills, particularly geological knowledge, for exploring suitable carbon capture facilities and assessing their capabilities. The text also emphasizes the need for engineers to leverage diverse data sources to accelerate the transition towards cleaner energy.\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = top_agent.chat(\"Tel me something about document 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9d7dcec-5d1a-485a-b40f-aa9604fd92df",
   "metadata": {
    "id": "c9d7dcec-5d1a-485a-b40f-aa9604fd92df",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "outputId": "85c24938-a9c0-49c2-bc5f-426293c3a055",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:08:29.269427900Z",
     "start_time": "2024-07-04T08:08:29.156517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems that Document 2 is discussing the importance of strong technical skills, particularly geological knowledge, for exploring suitable carbon capture facilities and assessing their capabilities. The text also emphasizes the need for engineers to leverage diverse data sources to accelerate the transition towards cleaner energy.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee6ef20c-3ccc-46c3-ad87-667138d78d5d",
   "metadata": {
    "id": "ee6ef20c-3ccc-46c3-ad87-667138d78d5d",
    "outputId": "deb92b26-0ca2-41ed-8e0e-02489d00aeb2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "ExecuteTime": {
     "end_time": "2024-07-04T08:09:18.574313400Z",
     "start_time": "2024-07-04T08:09:14.789742300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: I apologize, but since we only have access to Document 1, I don't have any information about Document 2 or its authors.\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "# should use Boston agent -> vector tool\n",
    "response = top_agent.chat(\"In connection with the second question, who are the author of the paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cfe1dd4c-8bfd-43d0-99bc-ca60861dc418",
   "metadata": {
    "id": "cfe1dd4c-8bfd-43d0-99bc-ca60861dc418",
    "outputId": "e6698a84-856e-4354-b8bf-18fbd1ead65f",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:10:39.131851500Z",
     "start_time": "2024-07-04T08:09:52.895662700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize, but since we only have access to Document 1, I don't have any information about Document 2 or its authors.\n",
      "\u001B[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: tool_Document_1\n",
      "Action Input: {'input': 'author', 'type': 'string'}\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: summary_tool\n",
      "Action Input: {'input': 'A Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with Machine Learning Techniques'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: As part of exploration and production, the oil and gas industry produces substantial amounts of data within different disciplines, with 80% being unstructured like reports, presentations, spreadsheets, etc. To assist geoscientists and engineers, Machine Learning (ML) and Artificial Intelligence (AI) technologies are applied to process the unstructured data from 440 wells from the Bonaparte Basin in Australia, making it possible to perform more accurate analysis and make faster decisions.\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: vector_tool\n",
      "Action Input: {'input': 'A Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with Machine Learning Techniques'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: A CASE STUDY OF UNDERSTANDING THE BONAPARTE BASIN USING UNSTRUCTURED DATA ANALYSIS WITH MACHINE LEARNING TECHNIQUES\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: summary_tool\n",
      "Action Input: {'input': 'A CASE STUDY OF UNDERSTANDING THE BONAPARTE BASIN USING UNSTRUCTURED DATA ANALYSIS WITH MACHINE LEARNING TECHNIQUES'}\n",
      "\u001B[0m\u001B[1;3;34mObservation: A Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with Machine Learning Techniques\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: The observation suggests that it is a case study on understanding the Bonaparte Basin, possibly discussing its geology or oil and gas exploration, applying machine learning techniques to unstructured data from 440 wells in Australia.\n",
      "\u001B[0m\u001B[1;3;34mObservation: The observation suggests that it is a case study on understanding the Bonaparte Basin, possibly discussing its geology or oil and gas exploration, applying machine learning techniques to unstructured data from 440 wells in Australia.\n",
      "\u001B[0m\u001B[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The authors of this document are not explicitly mentioned, as it appears to be a case study on understanding the Bonaparte Basin, possibly discussing its geology or oil and gas exploration, applying machine learning techniques to unstructured data from 440 wells in Australia.\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "print(response)# should use Boston agent -> vector tool\n",
    "response = top_agent.chat(\"Okay, can you tell me who if the author of that document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "843c79cb-fc7a-4eac-ba7f-6d25c7a8f08e",
   "metadata": {
    "id": "843c79cb-fc7a-4eac-ba7f-6d25c7a8f08e",
    "outputId": "a9b91624-943e-423b-c620-71d3a546f5eb",
    "ExecuteTime": {
     "end_time": "2024-07-04T08:29:02.657544Z",
     "start_time": "2024-07-04T08:28:46.836643600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new energy industry will continue to gather, integrate and analyze empirical data, whether it is on the reservoir, sub-surface or at hydrocarbon or future hydrogen production facilities.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "response = base_query_engine.query(\n",
    "    \"Tell me the author of the Bonaparte Basin Paper\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f99183-bdd9-4d49-8980-44919cf86c03",
   "metadata": {
    "id": "e6f99183-bdd9-4d49-8980-44919cf86c03"
   },
   "outputs": [],
   "source": [
    "# baseline: the response tells you nothing about Chicago...\n",
    "response.source_nodes[3].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7e0a4c-b7be-4797-9c82-1577693bd117",
   "metadata": {
    "id": "5b7e0a4c-b7be-4797-9c82-1577693bd117",
    "outputId": "68186957-1a77-4ca6-fd52-3c658a8ed4b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Calling Function ===\n",
      "Calling function: tool_Shanghai with args: {\n",
      "  \"input\": \"history\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"history\"\n",
      "}\n",
      "Got output: Shanghai has a rich history that dates back to ancient times. However, in the context provided, the history of Shanghai is mainly discussed in relation to its modern development. After the war, Shanghai's economy experienced significant growth, with increased agricultural and industrial output. The city's administrative divisions were rearranged, and it became a center for radical leftism during the 1950s and 1960s. The Cultural Revolution had a severe impact on Shanghai's society, but the city maintained economic production with a positive growth rate. Shanghai also played a significant role in China's Third Front campaign and has been a major contributor of tax revenue to the central government. Economic reforms were initiated in Shanghai in 1990, leading to the development of the Pudong district and its classification as an Alpha+ city.\n",
      "========================\n",
      "Got output: Shanghai's history is rich and complex, dating back to ancient times. However, its modern development is particularly noteworthy. After the war, Shanghai experienced significant economic growth, with a boost in both agricultural and industrial output. The city's administrative divisions were restructured, and it became a hub for radical leftism during the 1950s and 1960s.\n",
      "\n",
      "The Cultural Revolution had a profound impact on Shanghai's society, but despite this, the city managed to maintain economic production with a positive growth rate. Shanghai also played a significant role in China's Third Front campaign and has been a major contributor of tax revenue to the central government.\n",
      "\n",
      "In 1990, economic reforms were initiated in Shanghai, leading to the development of the Pudong district. This has helped Shanghai to be classified as an Alpha+ city, indicating its influence on the global economic stage.\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: tool_Beijing with args: {\n",
      "  \"input\": \"history\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"history\"\n",
      "}\n",
      "Got output: Beijing has a rich history that spans several dynasties. It was the capital of the Ming dynasty, during which the city took its current shape and many of its major attractions, such as the Forbidden City and the Temple of Heaven, were constructed. The Qing dynasty succeeded the Ming dynasty and made Beijing its sole capital. During this time, the Imperial residence and the general layout of the city remained largely unchanged. However, the city faced challenges during the Second Opium War and the Boxer Rebellion, resulting in the looting and destruction of important structures. In the early 20th century, Beijing saw the signing of a peace agreement between the Eight-Nation Alliance and the Chinese government, which led to the restoration of Qing dynasty rule. However, the dynasty eventually collapsed in 1911.\n",
      "========================\n",
      "Got output: Beijing has a rich and complex history that spans several dynasties. It served as the capital during the Ming dynasty, during which the city took its current shape and many of its major attractions, such as the Forbidden City and the Temple of Heaven, were constructed. The Qing dynasty succeeded the Ming dynasty and made Beijing its sole capital. During this time, the Imperial residence and the general layout of the city remained largely unchanged.\n",
      "\n",
      "However, the city faced significant challenges during the Second Opium War and the Boxer Rebellion, which resulted in the looting and destruction of important structures. In the early 20th century, Beijing saw the signing of a peace agreement between the Eight-Nation Alliance and the Chinese government, leading to the restoration of Qing dynasty rule. However, the dynasty eventually collapsed in 1911. Despite these tumultuous events, Beijing has managed to preserve its historical heritage while also evolving into a modern metropolis.\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: tool_Shanghai with args: {\n",
      "  \"input\": \"current economy\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"current economy\"\n",
      "}\n",
      "Got output: The current economy of Shanghai is strong and thriving. It is a global center for finance and innovation, and a national center for commerce, trade, and transportation. The city has a diverse economy, with its six largest industries comprising about half of its GDP. Shanghai has experienced rapid development and has been one of the fastest-developing cities in the world. It has recorded double-digit GDP growth in almost every year between 1992 and 2008. As of 2021, Shanghai had a GDP of CNÂ¥4.46 trillion ($1.106 trillion in PPP), making it one of the wealthiest cities in China. It is also the most expensive city in mainland China to live in. Shanghai is a major player in the global financial industry, ranking first in Asia and third globally in the Global Financial Centres Index. It is home to the Shanghai Stock Exchange, the largest stock exchange in China and the fourth-largest in the world. The city has attracted significant foreign investment and has been a hub for the technology industry and startups. Overall, the current economy of Shanghai is robust and continues to grow.\n",
      "========================\n",
      "Got output: The current economy of Shanghai is robust and thriving. It is a global center for finance and innovation, and a national center for commerce, trade, and transportation. The city has a diverse economy, with its six largest industries comprising about half of its GDP. \n",
      "\n",
      "Shanghai has experienced rapid development and has been one of the fastest-developing cities in the world. It has recorded double-digit GDP growth in almost every year between 1992 and 2008. As of 2021, Shanghai had a GDP of CNÂ¥4.46 trillion ($1.106 trillion in PPP), making it one of the wealthiest cities in China. \n",
      "\n",
      "Shanghai is also the most expensive city in mainland China to live in. It is a major player in the global financial industry, ranking first in Asia and third globally in the Global Financial Centres Index. The city is home to the Shanghai Stock Exchange, the largest stock exchange in China and the fourth-largest in the world. \n",
      "\n",
      "The city has attracted significant foreign investment and has been a hub for the technology industry and startups. Overall, the current economy of Shanghai is robust and continues to grow.\n",
      "========================\n",
      "=== Calling Function ===\n",
      "Calling function: tool_Beijing with args: {\n",
      "  \"input\": \"current economy\"\n",
      "}\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool with args: {\n",
      "  \"input\": \"current economy\"\n",
      "}\n",
      "Got output: The current economy of Beijing is dominated by the tertiary sector, which includes services such as professional services, wholesale and retail, information technology, commercial real estate, scientific research, and residential real estate. This sector generated 83.8% of the city's output in 2022. The secondary sector, which includes manufacturing and construction, accounted for 15.8% of output, while the primary sector, which includes agriculture and mining, contributed only 0.26%. The city has also identified six high-end economic output zones that are driving local economic growth, including Zhongguancun, Beijing Financial Street, Beijing Central Business District (CBD), Beijing Economic and Technological Development Area (Yizhuang), Beijing Airport Economic Zone, and Beijing Olympic Center Zone. These zones are home to various industries and sectors, such as technology companies, financial institutions, office buildings, industrial parks, and entertainment and sports centers.\n",
      "========================\n",
      "Got output: The current economy of Beijing is primarily driven by the tertiary sector, which includes services such as professional services, wholesale and retail, information technology, commercial real estate, scientific research, and residential real estate. This sector generated 83.8% of the city's output in 2022. The secondary sector, which includes manufacturing and construction, accounted for 15.8% of output, while the primary sector, which includes agriculture and mining, contributed only 0.26%.\n",
      "\n",
      "Beijing has also identified six high-end economic output zones that are driving local economic growth. These include Zhongguancun, Beijing Financial Street, Beijing Central Business District (CBD), Beijing Economic and Technological Development Area (Yizhuang), Beijing Airport Economic Zone, and Beijing Olympic Center Zone. These zones are home to various industries and sectors, such as technology companies, financial institutions, office buildings, industrial parks, and entertainment and sports centers.\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "response = top_agent.query(\n",
    "    \"Tell me the differences between Shanghai and Beijing in terms of history\"\n",
    "    \" and current economy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98540ec5-093d-41a0-888b-246ee7093cff",
   "metadata": {
    "id": "98540ec5-093d-41a0-888b-246ee7093cff",
    "outputId": "a8af0d51-e88f-4017-ca7e-c6889cd2833e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In terms of history, both Shanghai and Beijing have rich and complex pasts. Shanghai's history dates back to ancient times, but its modern development is particularly noteworthy. It experienced significant economic growth after the war and played a major role in China's economic reforms. Beijing, on the other hand, has a history that spans several dynasties and served as the capital during the Ming and Qing dynasties. It has preserved its historical heritage while evolving into a modern metropolis.\n",
      "\n",
      "In terms of current economy, Shanghai is a global center for finance and innovation. It has a diverse economy and has experienced rapid development, with a high GDP and significant foreign investment. It is a major player in the global financial industry and is home to the Shanghai Stock Exchange. Beijing's economy is primarily driven by the tertiary sector, with a focus on services such as professional services, information technology, and commercial real estate. It has identified high-end economic output zones that are driving local economic growth.\n",
      "\n",
      "Overall, both cities have thriving economies, but Shanghai has a stronger focus on finance and global influence, while Beijing has a diverse economy with a focus on services and high-end economic zones.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f49e28-e85e-4c93-8eed-68122ad9f537",
   "metadata": {
    "id": "a0f49e28-e85e-4c93-8eed-68122ad9f537",
    "outputId": "2c7df81f-2141-44ce-dbe0-3666ad2ca8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shanghai and Beijing have distinct differences in terms of history and current economy. Historically, Shanghai was the largest and most prosperous city in East Asia during the 1930s, while Beijing served as the capital of the Republic of China and later the People's Republic of China. Shanghai experienced significant growth and redevelopment in the 1990s, while Beijing expanded its urban area and underwent rapid development in the last two decades.\n",
      "\n",
      "In terms of the current economy, Shanghai is considered the \"showpiece\" of China's booming economy. It is a global center for finance and innovation, with a strong focus on industries such as retail, finance, IT, real estate, machine manufacturing, and automotive manufacturing. Shanghai is also home to the world's busiest container port, the Port of Shanghai. The city has a high GDP and is classified as an Alpha+ city by the Globalization and World Cities Research Network.\n",
      "\n",
      "On the other hand, Beijing is a global financial center and ranks third globally in the Global Financial Centres Index. It is also a hub for the Chinese and global technology industry, with a large startup ecosystem. Beijing has a strong presence in industries such as finance, technology, and pharmaceuticals. The city is home to the headquarters of large state banks and insurance companies, as well as the country's financial regulatory agencies.\n",
      "\n",
      "Overall, while both Shanghai and Beijing are important economic centers in China, Shanghai has a stronger focus on industries such as finance, retail, and manufacturing, while Beijing has a strong presence in finance, technology, and pharmaceuticals.\n"
     ]
    }
   ],
   "source": [
    "# baseline\n",
    "response = base_query_engine.query(\n",
    "    \"Tell me the differences between Shanghai and Beijing in terms of history\"\n",
    "    \" and current economy\"\n",
    ")\n",
    "print(str(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
