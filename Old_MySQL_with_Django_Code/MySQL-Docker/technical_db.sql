CREATE DATABASE  IF NOT EXISTS `technical` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci */ /*!80016 DEFAULT ENCRYPTION='N' */;
USE `technical`;
-- MySQL dump 10.13  Distrib 8.0.36, for Win64 (x86_64)
--
-- Host: localhost    Database: technical
-- ------------------------------------------------------
-- Server version	8.0.37

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!50503 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `chunks`
--

DROP TABLE IF EXISTS `chunks`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `chunks` (
  `chunk_id` int NOT NULL AUTO_INCREMENT,
  `paper_id` int DEFAULT NULL,
  `chunk_order` int NOT NULL,
  `chunk_text` varchar(1500) NOT NULL,
  PRIMARY KEY (`chunk_id`),
  KEY `paper_id` (`paper_id`),
  CONSTRAINT `chunks_ibfk_1` FOREIGN KEY (`paper_id`) REFERENCES `papers` (`paper_id`)
) ENGINE=InnoDB AUTO_INCREMENT=110 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `chunks`
--

LOCK TABLES `chunks` WRITE;
/*!40000 ALTER TABLE `chunks` DISABLE KEYS */;
INSERT INTO `chunks` VALUES (1,1,1,'\nA CASE STUDY OF UNDERSTANDING THE\nBONAPARTE BASIN USING UNSTRUCTURED DATA\nANALYSIS WITH MACHINE LEARNING TECHNIQUES\n\nAN. Sazali!, N.M. Hernandez’, F. Baillard\', K.G. Maver!\n\n\'Traya Energies\n\nSummary\n\nAs part of exploration and production the oil and gas industry produce substantial amounts of data\nwithin different disciplines of which 80% are unstructured like reports, presentations, spreadsheets etc.\nThe value of technical work is reduced due to the lack of time available for analysis and critical\nthinking and the under-utilization of the data. To assist geoscientist and engineers, Machine Learning\n(ML) and Artificial Intelligence (AI) technologies are applied to process the unstructured data from\n440 wells from the Bonaparte Basin in Australia making it possible to perform more accurate analysis\nand make faster decisions.\n\nBased on the play-based exploration pyramid concept, the time spent at the Basin Focus stage can be\nreduced, and more time are available to focus on the other project stages. The explorationist will be\nable to bring more value to the study.\n\nIt will be shown that potential issues encountered during exploration of the Bonaparte Basin can be\nidentified. Based on a quick look and gathering of all information it can be concluded that most of the\nproduction in the Bonaparte Basin is from Jurassic and Triassic with observed net pay of 18-60m\nthickness, porosity of 11-29% and saturation of 11-55% Sw.\n\n\n\nA Case Study of Understanding the Bonaparte Basin using Un'),(2,1,2,'ject stages. The explorationist will be\nable to bring more value to the study.\n\nIt will be shown that potential issues encountered during exploration of the Bonaparte Basin can be\nidentified. Based on a quick look and gathering of all information it can be concluded that most of the\nproduction in the Bonaparte Basin is from Jurassic and Triassic with observed net pay of 18-60m\nthickness, porosity of 11-29% and saturation of 11-55% Sw.\n\n\n\nA Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with\nMachine Learning Techniques\n\nIntroduction\n\nAs part of exploration and production the oil and gas industry produce substantial amounts of data\nwithin different disciplines of which 80% are unstructured like reports, presentations, spreadsheets etc\nand it is expected to grow exponentially. As a result, geoscientists and engineers spend 50 to 80% of\ntheir time searching and assembling data and only 1 to 5% of the data is fully utilized. The value of\ntechnical work is therefore reduced due to the lack of time available for analysis and critical thinking\nand the under-utilization of the data. To assist geoscientist and engineers, Machine Learning (ML)\nand Artificial Intelligence (AI) technologies are applied to process the unstructured data making it\npossible to perform more accurate analysis and make faster decisions.\n\nIn this case study the area of interest covers Bonaparte Basin, which is located north-west of\nthe Australian continental margin (Figure 1). It'),(3,1,3,'herefore reduced due to the lack of time available for analysis and critical thinking\nand the under-utilization of the data. To assist geoscientist and engineers, Machine Learning (ML)\nand Artificial Intelligence (AI) technologies are applied to process the unstructured data making it\npossible to perform more accurate analysis and make faster decisions.\n\nIn this case study the area of interest covers Bonaparte Basin, which is located north-west of\nthe Australian continental margin (Figure 1). It joins the Money Shoal basin in the north-east and\nthe Browse Basin in the south-west. Furthermore, the Timor Trough defines the northern boundary.\nThe areal extent of the basin is approximately 270,000 sq. km. The objective of this study is to\nunderstand and obtain meaningful insights into the Bonaparte Basin based on the substantial amount\nof information available in previous studies, reports and presentations. The unstructured data of the\nBonaparte Basin have been ingested in a Knowledge Container through consecutive ML and AI\npipelines and analysed using big data analytics tools.\n\nConsist of several structural elements :\n@ Ashmore Platform © Malita Graben\n© Vulcan Sub-Basin © Sahul Platform\n\n© Londonderry High @ Flamingo High\n© Petrel Sub-basin @ Flamingo Syncline\n© Darwin Shelf @® Sahul Syncline\n@ Calder Graben ® Nancar Trough\n@ Troubadour Terrace Laminaria High\nFigure 1 Location of the Bonaparte Basin within the Australian continental margin (left) and 14\nstructural elements obse'),(4,1,4,'ainer through consecutive ML and AI\npipelines and analysed using big data analytics tools.\n\nConsist of several structural elements :\n@ Ashmore Platform © Malita Graben\n© Vulcan Sub-Basin © Sahul Platform\n\n© Londonderry High @ Flamingo High\n© Petrel Sub-basin @ Flamingo Syncline\n© Darwin Shelf @® Sahul Syncline\n@ Calder Graben ® Nancar Trough\n@ Troubadour Terrace Laminaria High\nFigure 1 Location of the Bonaparte Basin within the Australian continental margin (left) and 14\nstructural elements observed within the Bonaparte Basin (right).\n\nMethodology\n\nAs of 2021, the Bonaparte Basin encompasses 440 wells representing 58 years of exploration history\nsummarized in over 270,000 pages of documents and in 250,000 images. It is estimated that billions of\ndollars have been invested over the years to acquire and interprete the data, making it a substantial\nsource of information for new exploration activities.\n\nThe Play Based Exploration (PBE) approach is often used as a traditional framework to refine the\ngeoscientists’s understanding from a broad basin level to a narrow prospect focus (Lottaroli et al., 2016).\nAs a start such an approach often involves capturing the current state of knowledge with massive\nbackground resources to understand and analyse the key features of the basin and the major risks\nassociated to it. Such information is primarily available in unstructured data, requiring geoscientist and\nengineers to process and ingest the information before focusing on a specific pla'),(5,1,5,'efine the\ngeoscientists’s understanding from a broad basin level to a narrow prospect focus (Lottaroli et al., 2016).\nAs a start such an approach often involves capturing the current state of knowledge with massive\nbackground resources to understand and analyse the key features of the basin and the major risks\nassociated to it. Such information is primarily available in unstructured data, requiring geoscientist and\nengineers to process and ingest the information before focusing on a specific play and prospect using\nstructured data. Therefore, we have modified the existing PBE pyramid to introduce an additional\n\n\n\ndimension associated with data science identifying the different types of data available at different\nstages, allowing us to better define the best suited ML/AI strategy for a given stage (Figure 2).\n\nSTRUCTURED\n\nTora nercare cercest\nUNSTRUCTURED ‘ 8 rence siento renown, se 30008)\n\nof being present and effective over 20,\n\n@ Assess the possibsty a play may\nexiets ina basin\n\nFigure 2 Customized Play Based Exploration (PBE) pyramid with ML technology (Modified from\nLottaroli et al., 2016).\n\nFocusing on the unstructured data associated with the Basin and Play Analysis, all the data from the\nBonaparte Basin have been processed through a succession of AI/ML automated pipeline such as\nNatural Language Processing or Deep Convolutional Neural Network (Hernandez et al., 2019), (Figure\n3). The sharable structured data is then further processed through deeper level of analytics '),(6,1,6,'ure 2 Customized Play Based Exploration (PBE) pyramid with ML technology (Modified from\nLottaroli et al., 2016).\n\nFocusing on the unstructured data associated with the Basin and Play Analysis, all the data from the\nBonaparte Basin have been processed through a succession of AI/ML automated pipeline such as\nNatural Language Processing or Deep Convolutional Neural Network (Hernandez et al., 2019), (Figure\n3). The sharable structured data is then further processed through deeper level of analytics to detect\ntrends and anomalies present within the data. Machine assistance is heavily used in repetitive tasks early\nin the process during the processing of the data and up to 95% of the tasks will be performed by the\nmachine. This provides additional time for the specialist to focus on critical thinking and cognitive skills\n\nto interpret the data.\n\nMACHINE LEARNING PIPELINE ANALYTICS GEOLOGICAL INSIGHTS\n\nEarly intespretation on\n\n* lithology Cloud Geological environment\n\n‘Text Search = —~ Data are geospatially\n+ Image Classification - + » distributed on maps for\nDeep Convolutional trends and anomalies\n\nUnstructured Neural Network Sharable observation\n\nData Structured » Image Search\nData\n+ |, Extracted | Metadata Extraction |, Document\n\nText and Tagging Tags Heat Map\n\n‘Natural Language Processing\n\nEarly trend can be observed\nacross the whole basin\n\nIdentify well analogues and\n\n* Knowledge Graph —- —* relationship between wells\n\n95% 1909 — 6) —\n\nMI/AI sequence automated with Analytics to'),(7,1,7,'re geospatially\n+ Image Classification - + » distributed on maps for\nDeep Convolutional trends and anomalies\n\nUnstructured Neural Network Sharable observation\n\nData Structured » Image Search\nData\n+ |, Extracted | Metadata Extraction |, Document\n\nText and Tagging Tags Heat Map\n\n‘Natural Language Processing\n\nEarly trend can be observed\nacross the whole basin\n\nIdentify well analogues and\n\n* Knowledge Graph —- —* relationship between wells\n\n95% 1909 — 6) —\n\nMI/AI sequence automated with Analytics tools for data display Hurnan high-level interpretation\nhuman in the loop for QC\n9,\n\ntue oN $$ 5% 40% mee |\n\nFigure 3 Unstructured Big Data pipeline\n\nIn this case study, interpretation using the Big Data workflow was used to understand the exploration\nhistory, how the basin developed, its petroleum system and the main issue of the dry wells occurrence\nto avoid repeating the mistakes of the past and improve future decision making.\n\n\n\nBy analysing the data, five potential issues are identified i.e. (i) Discrepancies in Formation Tops, (ii)\nLimited understanding of Lithology Distribution, (iii) Limited Mineral Composition Understanding,\n(iv) Fluid Distribution, and (v) Pressure/Temperature Patterns. Each potential issue is tackled by\nidentifying trends and anomalies across the basin using images, tables and plots extracted from the\nunstructured data corpus.\n\nResults\n\nAs an example, the analysis of the (ii) Limited Understanding of Lithology Distribution, shown in\nFigure 4, is performed usin'),(8,1,8,'(i) Discrepancies in Formation Tops, (ii)\nLimited understanding of Lithology Distribution, (iii) Limited Mineral Composition Understanding,\n(iv) Fluid Distribution, and (v) Pressure/Temperature Patterns. Each potential issue is tackled by\nidentifying trends and anomalies across the basin using images, tables and plots extracted from the\nunstructured data corpus.\n\nResults\n\nAs an example, the analysis of the (ii) Limited Understanding of Lithology Distribution, shown in\nFigure 4, is performed using heatmaps. The heatmaps show the distribution of clastic and carbonates\nacross the Bonaparte Basin and identify patterns and anomalies present in the area. The result can be\nsupported by the stratigraphic chart where the carbonate environment occurs in the younger formation\nfrom Cretaceous to Neogene period, whereas clastic environment is present in the older formations from\nTriassic to Cretaceous.\n\nBasal vanegrassive enadetone Limeston:\nand marine shelf sand mestone\nHE arine claystone and shale\n\nModitied from Frankowlez & McClay 2009\n\nFigure 4 Lithology distribution on heatmaps (left) and corresponding stratigraphic chart (right).\n\nThe analysis of the (iii) Limited Mineral Composition Understanding, shown in Figure 5, utilizes the\nthin section automatically extracted using ML classification over the full area and suggests that:\n\nQuartz overgrowth and kaolinite are quite common in Bonaparte Basin\n\nMica mineral can be observed at the north-eastern part of the basin\n\nHighly corroded, sk'),(9,1,9,'e\n\nModitied from Frankowlez & McClay 2009\n\nFigure 4 Lithology distribution on heatmaps (left) and corresponding stratigraphic chart (right).\n\nThe analysis of the (iii) Limited Mineral Composition Understanding, shown in Figure 5, utilizes the\nthin section automatically extracted using ML classification over the full area and suggests that:\n\nQuartz overgrowth and kaolinite are quite common in Bonaparte Basin\n\nMica mineral can be observed at the north-eastern part of the basin\n\nHighly corroded, skeletal feldspar has been extensively dissolved, which forms secondary\nporosity, and can be observed in the northern part of the basin\n\nSome patchy siderites are also observed in the southern part of the basin\n\n\n\nstam (cone\nFigure 5 Thin section images distributed on a map across the Bonaparte Basin.\n\nConclusion\n\nA regional understanding is critical and time consuming as it involves dealing with a very large data\n\nvolume. Within a project time frame, based on PBE pyramid, the time spent at the Basin Focus stage\n\ncan be reduced, and more time are available to focus on the other project stages. The explorationist will\nbe able to bring more value to the study.\n\nML applications have proven to be able to play a crucial part in order to organize large unstructured\ndata corpuses. This allows faster and accurate decision making within the fast-moving industry.\n\nIn this study, some potential issues encountered during exploration of the Bonaparte Basin can be\nidentified. Based on a quick look and'),(10,1,10,'sin Focus stage\n\ncan be reduced, and more time are available to focus on the other project stages. The explorationist will\nbe able to bring more value to the study.\n\nML applications have proven to be able to play a crucial part in order to organize large unstructured\ndata corpuses. This allows faster and accurate decision making within the fast-moving industry.\n\nIn this study, some potential issues encountered during exploration of the Bonaparte Basin can be\nidentified. Based on a quick look and gathering of all information it can be concluded that most of the\nproduction in the Bonaparte Basin is from Jurassic and Triassic with observed net pay ~18-60m\nthickness, porosity ~11-29% and saturation ~11-55% Sw.\n\nReferences\n\nHernandez N., Lucafias P., Graciosa J.C., Mamador C., and Panganiban L. C. I, 2019: Automated\ninformation retrieval from unstructured documents utilizing a sequence of smart machine learning\nmethods within a hybrid cloud container. EAGE Workshop on Big Data and Machine Learning for E&P\nEfficiency 25 - 27 February.\n\nLottaroli F., Craig J., Cozzi A., 2016: Evaluating a vintage play fairway exercise using subsequent\nexploration results: did it work? Petroleum Geoscience, Vol 24, no 2, p. 159 — 171.\n\nMaver K.G., Hernandez N., Lucafias P., Graciosa J.C., Mamador C., Panganiban L.C.L, Yu C., and\nMaver M.G., 2018: An automated information retrieval platform for unstructured well data smart\nmachine learning algorithms within a hybrid cloud container. EAGE/PESGB Worksho'),(11,1,11,'earning for E&P\nEfficiency 25 - 27 February.\n\nLottaroli F., Craig J., Cozzi A., 2016: Evaluating a vintage play fairway exercise using subsequent\nexploration results: did it work? Petroleum Geoscience, Vol 24, no 2, p. 159 — 171.\n\nMaver K.G., Hernandez N., Lucafias P., Graciosa J.C., Mamador C., Panganiban L.C.L, Yu C., and\nMaver M.G., 2018: An automated information retrieval platform for unstructured well data smart\nmachine learning algorithms within a hybrid cloud container. EAGE/PESGB Workshop on Machine\nLearning, 29 — 30 November.\n\n\n'),(12,2,1,'\nSupporting the UN 2050 Net Zero goals by reading\n\nthe earth better\n\nNina Marie Hernandez\", Kim Gunn Maver and Charmyne Mamador present the ED2\ninitiative, providing multivariate earth data to support UN climate change goals.\n\nIntroduction\n\nAgainst the backdrop of the current global pandemic, the UN\n2050 net zero goals call for global greenhouse emissions to be cut\nby half by 2030 and reach net zero no later than 2050 to achieve\nthe goal of limiting global warming to 1.5 Celsius above pre-in-\ndustrial levels. The EU has pledged to become the first carbon\nneutral continent by 2050, and more than 110 other countries\nhave pledged carbon neutrality by this time. Several energy com-\npanies have laid out their medium-to-long-term plans towards\nthis objective, which includes acquisition of renewables assets,\nand developing competitive technologies for carbon capture and\nstorage (CCS) and hydrogen production. Among the companies\nthat have set zero-emissions targets are BP, Shell, Total, Repsol,\nEquinor and Petronas. This energy pivot will require significant\ncapital spending to reach the goals.\n\nTo determine the technical and economic feasibility of these\nnew energy technologies and at the same time achieve sustainable\ndevelopment goals, multivariate earth data, both existing and\nnew, are required and are key to making the right management\nand investment decisions.\n\nEarthDoc pointing to the future\n\nTo this end, more than 39 years of conference proceedings and\npublications are already'),(13,2,2,',\nEquinor and Petronas. This energy pivot will require significant\ncapital spending to reach the goals.\n\nTo determine the technical and economic feasibility of these\nnew energy technologies and at the same time achieve sustainable\ndevelopment goals, multivariate earth data, both existing and\nnew, are required and are key to making the right management\nand investment decisions.\n\nEarthDoc pointing to the future\n\nTo this end, more than 39 years of conference proceedings and\npublications are already available from EAGE through Earth-\nDoc, which aggregates a wealth of subsurface information from\nresearch institutions, energy companies, service companies\nand dedicated professionals. The 70,000 scientific publications\nfocus on conventional topics within geoscience and engineering\nespecially in relation to oil and gas extraction. This subsurface\ninformation can be upcycled to provide highly valuable insights\nfor new energy technologies. The reuse of existing oil and\ngas data reduces data acquisition costs, which translate into a\nreduction in research and development costs.\n\nThrough a collaboration between Iraya Energies and EAGE, a\nnew database initiative has been launched with EarthDoc’s repos-\nitory of 70,000 scientific publications being processed using the\nlatest in machine learning and artificial intelligence techniques\nand initially available to institutional and corporate subscribers.\nThe whole data corpus is made instantly accessible and provides\nnew tools to search and retri'),(14,2,3,'sition costs, which translate into a\nreduction in research and development costs.\n\nThrough a collaboration between Iraya Energies and EAGE, a\nnew database initiative has been launched with EarthDoc’s repos-\nitory of 70,000 scientific publications being processed using the\nlatest in machine learning and artificial intelligence techniques\nand initially available to institutional and corporate subscribers.\nThe whole data corpus is made instantly accessible and provides\nnew tools to search and retrieve the diversity of information that\none is looking for across any technical discipline.\n\n‘raya Energies\n“Corresponding author, E-mail: nmh@irayaenergies.com\nDOI: 10.3997/1365-2397.fb2021045\n\nWith Big Data analytics applied to the entire data corpus of\n70,000 scientific publications, additional in-depth and advanced\nnavigation options are now available.\n\nTo extract information on a large scale, the ElasticDocs AI\npipeline is applied to the EarthDoc corpus. This pipeline consists\nof a set of algorithms which are used to identify blocks/segments\nwithin the document corpus. Optical Character Recognition\n(OCR) is applied to the text segments to convert them into\nprocessable text. A Deep Convolutional Neural Network (DCNN)\nalgorithm pipeline classifies the images into various generic and\ngeological image classes, including tables, seismic, map, well\nplots, stratigraphic charts, core, thin sections, image logs, and\nrose diagrams. Untagged images are generically categorized as\nfigures and re'),(15,2,4,'orithms which are used to identify blocks/segments\nwithin the document corpus. Optical Character Recognition\n(OCR) is applied to the text segments to convert them into\nprocessable text. A Deep Convolutional Neural Network (DCNN)\nalgorithm pipeline classifies the images into various generic and\ngeological image classes, including tables, seismic, map, well\nplots, stratigraphic charts, core, thin sections, image logs, and\nrose diagrams. Untagged images are generically categorized as\nfigures and remain accessible to the user.\n\nThe ingested documents are now available as structured\ninformation for analysis, which is possible in different ways:\n\n+ Metadata extraction of relevant information such as locations,\nnames etc.\nEfficient inter and intra-search of the global text corpus for\nquantitative textual analysis of document contents to create\nautomated and standardized geoscience or engineering content\nsummary.\nAutomated heatmaps to visualize density of information\nwithin a basin or country.\nImage extraction of similar image classes for efficient identifi-\ncation of analogues, duplicates and clusters.\n\nSome key functionality that has a significant impact on utilizing\nthe scientific publications is that not only are images classified\naccording to type, but it is possible to do a search on image\nembedded text making the search capabilities far more advanced\nthan just a normal search-and-find option. The search results of\nthis database are exportable as .csv files making statistical w'),(16,2,5,'\nImage extraction of similar image classes for efficient identifi-\ncation of analogues, duplicates and clusters.\n\nSome key functionality that has a significant impact on utilizing\nthe scientific publications is that not only are images classified\naccording to type, but it is possible to do a search on image\nembedded text making the search capabilities far more advanced\nthan just a normal search-and-find option. The search results of\nthis database are exportable as .csv files making statistical work\nand analysis instantly possible across data points.\n\ntary corporate data meets published\n\nic knowledge.\nRecognizing that energy companies hold valuable information in\ntheir repositories accumulated throughout many decades of oper-\nations, the data kept internally can now be easily integrated with\npublished geoscience and engineering data. The combination of\n\n\n\nFinal_well_Report_Bas\n\nKNOWLEDGE GRAPH\n\nFigure 1 Data source tray in the new database allows\naccess to multiple databases, including, but not\nlimited to, public data, proprietary data, and EarthDoc\ncorpus. Reference: Geoscience Australia, NOPIMS\n\nFigure 2 Earth readability tools are available in the\nnew database dashboard (Rahim et al., 2012).\n\n(© Fiber Optic Technology for Reservou Survediance,\n\n@ Operatonal Excelence in Qatar\'s Fest Successful\n\' ber Ope Sensing for mnproves Wellbare\nJo integrated Approach a Optenze Matera Selechon\n\nEngineanng Suceess mo Hartline Operations\n\n© few totalugent Completion Welt Design For.\n\n(@ M'),(17,2,6,' but not\nlimited to, public data, proprietary data, and EarthDoc\ncorpus. Reference: Geoscience Australia, NOPIMS\n\nFigure 2 Earth readability tools are available in the\nnew database dashboard (Rahim et al., 2012).\n\n(© Fiber Optic Technology for Reservou Survediance,\n\n@ Operatonal Excelence in Qatar\'s Fest Successful\n\' ber Ope Sensing for mnproves Wellbare\nJo integrated Approach a Optenze Matera Selechon\n\nEngineanng Suceess mo Hartline Operations\n\n© few totalugent Completion Welt Design For.\n\n(@ Masere Hyaraune Fracture Sumulabon on South\n\n© integrated Ape\n\n(© Seratgrapluc Architecture of the Latest Jurassic\n\n(© Natural Fracture interaction with Hydraule aEQ4§Rf Jurassic to Early Barrer\n\n‘© Breaking Ou Recovery Lent in Malaysian.\n\ndatabase results in a two-way enrichment process between public\nand private information. Scientific studies from peers in related\nfields offer additional information that either validates internal\ncompany studies, or offer alternative technical perspectives from\nindustry experts.\n\nIn the Figure 1 example above, we show how exploring\ntemperature information available in a Final Well Reports repos-\nitory that is commonly considered as proprietary corporate data,\nis combined with published EarthDoc contents. (Data source:\nNOPIMS)\n\n@ Pus.ications Figure 3 Visualization of knowledge graph illustrates\n\nrelated geological and engineering concepts.\n\nSeveral Earth readability tools are deployed in ED2K to\namplify EarthDoc capabilities. This includes access to '),(18,2,7,'m\nindustry experts.\n\nIn the Figure 1 example above, we show how exploring\ntemperature information available in a Final Well Reports repos-\nitory that is commonly considered as proprietary corporate data,\nis combined with published EarthDoc contents. (Data source:\nNOPIMS)\n\n@ Pus.ications Figure 3 Visualization of knowledge graph illustrates\n\nrelated geological and engineering concepts.\n\nSeveral Earth readability tools are deployed in ED2K to\namplify EarthDoc capabilities. This includes access to digital\ntextual content, the ability to quality control OCR results and\nread in multiple languages via machine-translate. Word clouds are\nsimple context clues which provide a quick summary of content\nof the articles.\n\nThe full ED2K corpus is geotagged to more than 800\ngeological basins around the world. The geotagging is one of the\nmost complex machine learning tasks in this implementation,\nbecause the scientific articles contain both location of the\n\n\n\ngeological area of interest, as well as the location of confer-\nences where the publications are presented. Often, these two\nreference locations are different and introduce ambiguity for the\nsystem.\n\nAnother feature that is implemented is the knowledge graph\nvisualization. It illustrates the connectivity of ‘related concepts’\nbased on publication references. In the example above, it shows\n‘operational excellence’, ‘optimization’ and ‘engineering suc-\ncess’ within the same network (Figure 3). Similarly, ‘hydraulic\nfracture stimulation’, '),(19,2,8,'e location of confer-\nences where the publications are presented. Often, these two\nreference locations are different and introduce ambiguity for the\nsystem.\n\nAnother feature that is implemented is the knowledge graph\nvisualization. It illustrates the connectivity of ‘related concepts’\nbased on publication references. In the example above, it shows\n‘operational excellence’, ‘optimization’ and ‘engineering suc-\ncess’ within the same network (Figure 3). Similarly, ‘hydraulic\nfracture stimulation’, ‘natural fracture interaction’, and ‘Jurassic\ncarbonates’ show connectivity. In theory, a knowledge graph can\nbe built in multiple ways by the user.\n\nUtilizing ED2K to reach the zero-emission goal\n\nThe advanced access in the new database makes possible a new\nuse of the highly valuable subsurface information and facilitates\ncross-discipline usage.\n\nMany of the new cross-function usages of the existing\nscientific publication repository are geothermal energy, hydrogen\nenergy, CCS and windmill foundation derisking.\n\nA query for ‘carbon capture and storage’ generates an\ninformation heat map captured in Figure 4. The map shows the\ngeographical distribution of the resulting publications either\nbased on country or basin.\n\nFigure 5 maps out the major carbon capture projects around\nthe world vis-a-vis needs requirement, Europe and US show high\nactivity in CCS initiatives, which are driven by government poli-\ncy. Comparing Figure 4 and Figure 5 (left), it may be incidental,\n\nalthough not entirely'),(20,2,9,'ation derisking.\n\nA query for ‘carbon capture and storage’ generates an\ninformation heat map captured in Figure 4. The map shows the\ngeographical distribution of the resulting publications either\nbased on country or basin.\n\nFigure 5 maps out the major carbon capture projects around\nthe world vis-a-vis needs requirement, Europe and US show high\nactivity in CCS initiatives, which are driven by government poli-\ncy. Comparing Figure 4 and Figure 5 (left), it may be incidental,\n\nalthough not entirely surprising that where there is a significant\namount of data to aid technical and management decision mak-\ning, CCS implementations are also active.\n\nMAP KNOWLEDGE GRAPH\n\nResults Density\n\nThe new database contains climate change and greenhouse\ngases industry discussion materials spanning over three decades.\nAlready between 1990 and 2000, the possibility of disposing\ncarbon dioxide (CO,) is discussed in papers such as J. Leeb. W,\n(1993), and Wildenborg, F.B., et. al., (1996), in conjunction with\nthe use of CO, for improved oil recovery methods (IOR).\n\nBetween 2001 and 2010, the discussions tackled issues in\nestablishing a geological storage hub (Espie, 2000) and various\npotential site feasibility studies (Gregersen et. al, 2000), costs\n(Wildenborg et. al, 2000), use of seismic monitoring (Benson,\n2003), (Gosselet et. al., 2006), pilot and numerical simulations\n(Domitrovic et. al., 2005), (Battistelli et.al, 2005), reservoir\nperformance (Broad et.al, 2007) and improving facilities perfor'),(21,2,10,'the use of CO, for improved oil recovery methods (IOR).\n\nBetween 2001 and 2010, the discussions tackled issues in\nestablishing a geological storage hub (Espie, 2000) and various\npotential site feasibility studies (Gregersen et. al, 2000), costs\n(Wildenborg et. al, 2000), use of seismic monitoring (Benson,\n2003), (Gosselet et. al., 2006), pilot and numerical simulations\n(Domitrovic et. al., 2005), (Battistelli et.al, 2005), reservoir\nperformance (Broad et.al, 2007) and improving facilities perfor-\nmance to reduce operating costs in CO, and H,S contaminated\nfields (Swatton et. al, 2009).\n\nFrom 2011 up to the present, with the advancements in\nseismic methods, reservoir modelling techniques and laboratory\nexperiments (Bolourinejad, 2013} many more complex analyses\non the subject of carbon storage were performed. Combined with\nenhanced oil recovery experiments (EOR), the amount of data\nmodelling CO, behavior underground has multiplied ten-fold.\n\nIn areas where it is not possible to implement subsurface\ncarbon capture, utilization strategies are discussed by Harsh, A.\net. al (2014) on the industrial usage of CO, including, but not\nlimited to, polymer processing and chemicals production.\n\nFor a deeper dive in the corpus, we draw an arbitrary areal\npolygon, indicated by the yellow box in Figure 5, around South\nEast Asia. Our new database reveals some of the strategies\nthat have been identified by an operator to manage greenhouse\nemissions in its operations in Malaysia (Mehta et.al.,2'),(22,2,11,'t subsurface\ncarbon capture, utilization strategies are discussed by Harsh, A.\net. al (2014) on the industrial usage of CO, including, but not\nlimited to, polymer processing and chemicals production.\n\nFor a deeper dive in the corpus, we draw an arbitrary areal\npolygon, indicated by the yellow box in Figure 5, around South\nEast Asia. Our new database reveals some of the strategies\nthat have been identified by an operator to manage greenhouse\nemissions in its operations in Malaysia (Mehta et.al.,2008).\nThis includes, among others, ending continuous gas flaring\n\nFigure 4 Indicative geolocation of global knowledge\nabout ‘carbon capture and storage’ between 2008\nto 2020.\n\nFigure 5 Location of major carbon capture projects\naround the world (leff) and the requirement index\nbased on fossil fuel production and consumption\n(tight). Reference: Global CCS Institute.\n\n\n\nand minimizing gas venting, improving energy efficiency in\nthe design of assets and production operations, accounting for\nthe cost of emitting greenhouse gases in investment decisions,\nsupporting development of CCS infrastructures, and policy\nadvocacy.\n\nThe rich diversity of data available in the new database\nare illustrated in Figures 6 and 7. These include graphical\ninformation of PVT analyses, miscibility, flow rates, and time-\nlapse pressure profiles, which are useful for reservoir simulation\nstudies focused on the interaction of reservoir rocks with CO,\nduring injection. Also available are petrography data that makes\n'),(23,2,12,' of emitting greenhouse gases in investment decisions,\nsupporting development of CCS infrastructures, and policy\nadvocacy.\n\nThe rich diversity of data available in the new database\nare illustrated in Figures 6 and 7. These include graphical\ninformation of PVT analyses, miscibility, flow rates, and time-\nlapse pressure profiles, which are useful for reservoir simulation\nstudies focused on the interaction of reservoir rocks with CO,\nduring injection. Also available are petrography data that makes\nit easier to interpret reservoir modelling results by being able\nto look at the structural fabric of the storage rocks down to\nmicroscopic levels.\n\nNew energy from old data\nWith this data-driven strategy it will be possible to facilitate the\npivot to new energy from valuable existing data. No part of the\ndata is left unprocessed. It may be that not all relevant informa-\ntion will exist within the 70,000 scientific publications, but this\ncan be confirmed instantly saving valuable time and resources\ndoing data exploration. On the other hand, if the information is\navailable, it will be immediately accessible, trackable and put\ninto context with other relevant information and geographically.\nFor example, the geothermal gradient is a key parameter of\ninterest in relation to geothermal energy. The temperature data in\n\nthe new database has been acquired by energy companies mostly\nfor the purpose of understanding the hydrocarbon generation\nwindow, petrophysical interpretation and reservoir mod'),(24,2,13,'es\ndoing data exploration. On the other hand, if the information is\navailable, it will be immediately accessible, trackable and put\ninto context with other relevant information and geographically.\nFor example, the geothermal gradient is a key parameter of\ninterest in relation to geothermal energy. The temperature data in\n\nthe new database has been acquired by energy companies mostly\nfor the purpose of understanding the hydrocarbon generation\nwindow, petrophysical interpretation and reservoir modelling\nanalyses. They can be relooked at for further exploratory geo-\nthermal applications. It currently excludes temperature data from\ngeothermal companies.\n\nWe are barely scratching the surface on the data and insights\navailable — multiple data stories are waiting to be reimagined,\nreconnected and retold in the context of the future of energy.\n\nAccelerating internal digitalization initiatives\n\nAll the elements of the new database are stored and structured\nin a digital data warehouse. They will be optionally available\nas an API link to be used for additional geological analysis,\ndata analytics, or machine learning experimentations. Already\nin structured format, they can be fed into additional natural lan-\nguage processing or image segmentation processing for in-house\nexperimentation.\n\nOpportunity for the energy geoscientists and\nengineers of the future\n\nWhile the energy industry has faced significant headwinds, it is\nnow moving faster than ever towards new, cleaner energy pro-\nduction'),(25,2,14,' be optionally available\nas an API link to be used for additional geological analysis,\ndata analytics, or machine learning experimentations. Already\nin structured format, they can be fed into additional natural lan-\nguage processing or image segmentation processing for in-house\nexperimentation.\n\nOpportunity for the energy geoscientists and\nengineers of the future\n\nWhile the energy industry has faced significant headwinds, it is\nnow moving faster than ever towards new, cleaner energy pro-\nduction. It is possible to see that multiple opportunities remain,\nas already pointed out by Raistrick (2008), and remain relevant in\n2021, for the geoscientists and engineers who are looking at the\n\nFigure 6 Experimental engineering data of CO,\nbehaviours in enhanced oil recovery operations of\nmature fields can be transferable to carbon storage\ndesign and monitoring.\n\n7 | | | | | | | | | Figure 7 Combination of carbonate petrography data\n\nand information extracted from well reports.\n\n\n\nResults Density\n\now EE High\n\nFigure 8 Locations of relatively ‘high geothermal gradient areas’ based on existing\ncorpus, data acquired by oil and gas companies. Data excludes information from\ngeothermal companies.\n\nFigure 9 Compiled graphical temperature information filtered by country, basin or\nan arbitrary polygon location. Reference: Geoscience Australia, NOPIMS.\n\nfuture of energy. Strong, flexible technical skills will be needed\nto explore for suitable carbon capture facilities, assess their\nstorage, conta'),(26,2,15,'\n\now EE High\n\nFigure 8 Locations of relatively ‘high geothermal gradient areas’ based on existing\ncorpus, data acquired by oil and gas companies. Data excludes information from\ngeothermal companies.\n\nFigure 9 Compiled graphical temperature information filtered by country, basin or\nan arbitrary polygon location. Reference: Geoscience Australia, NOPIMS.\n\nfuture of energy. Strong, flexible technical skills will be needed\nto explore for suitable carbon capture facilities, assess their\nstorage, containment and injectivity capacities. Meanwhile, the\nnew energy industry will continue to gather, integrate and analyse\nempirical data, whether it is on the reservoir, sub-surface or at\nhydrocarbon or future hydrogen production facilities.\n\nWe have already seen a lot of data. It is up to us to use the\nright tools to read the earth better, and get a head start towards\nnew energy.\n\nReferences\n\nGlobal CCS Institute (https://www.globalcesinstitute.com). CCS Facil-\nities Database (https://co2re.co/) Geoscience Australia, NOPIMS\n(http://www.ga.gov.aw/nopims).\n\nBolourinejad, P. and Herber, R. [2013]. Experimental and Modeling Study\nof Salt Precipitation during Injection of CO2 Contaminated with H2S\ninto Depleted Gas Fields in Northeast Netherlands - (SPE-164932),\n75th EAGE Conference & Exhibition incorporating SPE EUROPEC\n2013, London, UK.\n\nBattistelli, A., Giorgis, T. and Marzorati, D. [2005]. Modeling Halite Pre-\ncipitation around CO2 Injection Wells in Depleted Gas Reservoirs,\n67th EAGE Confe'),(27,2,16,'science Australia, NOPIMS\n(http://www.ga.gov.aw/nopims).\n\nBolourinejad, P. and Herber, R. [2013]. Experimental and Modeling Study\nof Salt Precipitation during Injection of CO2 Contaminated with H2S\ninto Depleted Gas Fields in Northeast Netherlands - (SPE-164932),\n75th EAGE Conference & Exhibition incorporating SPE EUROPEC\n2013, London, UK.\n\nBattistelli, A., Giorgis, T. and Marzorati, D. [2005]. Modeling Halite Pre-\ncipitation around CO2 Injection Wells in Depleted Gas Reservoirs,\n67th EAGE Conference & Exhibition, Madrid, Spain.\n\nBroad, J., Ab Majid, M.N., Ariffin, T., Hussain, A. and Basher, A.B.\n[2007]. Deposition of “Asphaltenes” during CO2 Injection and\nImplications for EOS Description and Reservoir Performance, IPT\n2007: International Petroleum Technology Conference, Dubai, Unit-\ned Arab Emirates.\n\nDomitrovic, D., Tuschl, M. and Sunjerga, S. [2005]. CO2 Pilot Injection\nat Ivanic Oil Field — Numerical Simulation, IOR 2005 - 13th Euro-\npean Symposium on Improved Oil Recovery, Budapest, Hungary.\n\nGosselet, A.C. and Singh, §. [2006]. Elastic Full Waveform Inversion\nfor CO2 Sequestration monitoring - ID Synthetic Data Investiga-\ntions,68th EAGE Conference and Exhibition incorporating SPE\nEUROPEC 2006\n\nGregersen, U.N., Johannessen, P., Kirby, G., Chadwick, A. and Holloway,\nS. [2000]. Regional Study of the Neogene Deposits in the Southem\nViking Graben Area - a Site for Potential CO2 Storage,62nd EAGE\nConference and Exhibition - Special Session on CO2, Glasgow, UK.\n\nHarsh, A.H. '),(28,2,17,'st, Hungary.\n\nGosselet, A.C. and Singh, §. [2006]. Elastic Full Waveform Inversion\nfor CO2 Sequestration monitoring - ID Synthetic Data Investiga-\ntions,68th EAGE Conference and Exhibition incorporating SPE\nEUROPEC 2006\n\nGregersen, U.N., Johannessen, P., Kirby, G., Chadwick, A. and Holloway,\nS. [2000]. Regional Study of the Neogene Deposits in the Southem\nViking Graben Area - a Site for Potential CO2 Storage,62nd EAGE\nConference and Exhibition - Special Session on CO2, Glasgow, UK.\n\nHarsh, A.H. and Anne, V.A. [2014]. Carbon Dioxide Capture, Utilization\nand Storage (CCUS),76th EAGE Conference and Exhibition 2014,\nAmsterdam, Netherlands.\n\nLeeb, W. [1993]. Case study of CO2 disposal in aquifers - A solution\nto reduce the greenhouse effect, 55th EAEG Meeting, Stavanger,\nNorway.\n\nMehta, A., Hj-Kip, S. and Foo, J. [2008]. Managing Greenhouse Gas\nEmissions in Upstream Operations in a Carbon-Constrained World,\nIPTC 2008: International Petroleum Technology Conference, Kuala\nLumpur, Malaysia\n\nRahim, M., Azran, A., Press, D., Lee, K-H., Phuat, C.T., Anis, L.,\nDarman, N. and Othman, M. [2012]. An Integrated Reservoir Sim-\nulation-Geomechanical Study on Feasibility of CO2 Storage in M4\nCarbonate Reservoir, Malaysia, IPTC 2012: Intemational Petroleum\nTechnology Conference, Bangkok, Thailand.\n\nRaistrick, M. [2008]. Carbon capture and storage projects to challenge\ngovernments, scientists, and engineers, First Break.\n\nWildenborg, A. G.H.and van der Meer L. [1996]. Potential\nof CO2-disposal in'),(29,2,18,'him, M., Azran, A., Press, D., Lee, K-H., Phuat, C.T., Anis, L.,\nDarman, N. and Othman, M. [2012]. An Integrated Reservoir Sim-\nulation-Geomechanical Study on Feasibility of CO2 Storage in M4\nCarbonate Reservoir, Malaysia, IPTC 2012: Intemational Petroleum\nTechnology Conference, Bangkok, Thailand.\n\nRaistrick, M. [2008]. Carbon capture and storage projects to challenge\ngovernments, scientists, and engineers, First Break.\n\nWildenborg, A. G.H.and van der Meer L. [1996]. Potential\nof CO2-disposal in deep reservoirs and aquifers of the Nether-\nJands,58th EAGE Conference and Exhibition, Netherlands.\n\nWildenborg, A., Floris, FD., van Wees, J. and Hendriks, C. [2000].\nCosts of CO2 Sequestration by Underground Storage,62nd EAGE\nConference and Exhibition - Special Session on CO2, Glasgow,\nUK.\n\nSwatton, M.J.R., van Soest-Vercammen, E. and Klein Nagelvoort, R.\n\n, Breune:\n\n[2009]. Innovation and Integration in LNG Technology Solutions,\nIPTC 2009: International Petroleum Technology Conference, Doha,\nQatar.\n\n\n'),(30,3,1,'\nSCALING AND OPTIMIZING PERFORMANCE AND COST OF MACHINE\n\nLEARNING INGESTION ON UNSTRUCTURED DATA FOR\nSUBSURFACE APPLICATIONS\n\nL.C.L. Panganiban’, F. Baillard\', N.M. Hernandez!\n\n\' Traya Energies\n\nSummary\n\nIn recent years, the energy industry has shifted their attention into extracting additional values from\ntheir in-house legacy datasets for shorter project turnaround and better decision making. Internal digital\ntransformation initiatives and access to new technology such as cloud computing, machine learning and\nmicroservices made it possible to shift towards a scalable ingestion platform.\n\nA modern scalable ingestion platform often includes 1) automated machine learning (ML) components\narticulated around pipelines to parse and go through the data and extract the needed information 2)\nstorage component such Database, Datawarehouse and Datalake defining what data to be stored and\nhow.\n\nIn this paper, we will provide a description of these different components, their modern implementation\ninto a cloud environment using microservices and some performance benchmark based on real world\ndata examples.\n\n\n\nMADRID | SPAIN\n\nScaling and optimizing performance and cost of machine learning ingestion on unstructured\ndata for subsurface applications\n\nIntroduction\n\nIn recent years, the energy industry has shifted their attention into extracting additional values from\ntheir in-house legacy datasets for shorter project turnaround and better decision making. Internal digital\ntransformation initi'),(31,3,2,'\ninto a cloud environment using microservices and some performance benchmark based on real world\ndata examples.\n\n\n\nMADRID | SPAIN\n\nScaling and optimizing performance and cost of machine learning ingestion on unstructured\ndata for subsurface applications\n\nIntroduction\n\nIn recent years, the energy industry has shifted their attention into extracting additional values from\ntheir in-house legacy datasets for shorter project turnaround and better decision making. Internal digital\ntransformation initiatives and access to new technology such as cloud computing, machine learning and\nmicroservices made it possible to shift towards a scalable ingestion platform.\n\nA modern scalable ingestion platform often includes 1) automated machine learning (ML) components\narticulated around pipelines to parse and go through the data and extract the needed information 2)\nstorage component such Database, Datawarehouse and Datalake defining what data to be stored and\nhow.\n\nIn this paper, we will provide a description of these different components, their modern implementation\ninto a cloud environment using microservices and some performance benchmark based on real world\ndata examples.\n\nScalable ingestion platform architecture\n\nThe vast majority of experimentation and testing of new ML application are performed on notebooks\non local machines leveraging on local hardware with the data and the code being stored locally-. Such\na setup has the advantage of being lightweight allowing a fast turnaround betwee'),(32,3,3,'iption of these different components, their modern implementation\ninto a cloud environment using microservices and some performance benchmark based on real world\ndata examples.\n\nScalable ingestion platform architecture\n\nThe vast majority of experimentation and testing of new ML application are performed on notebooks\non local machines leveraging on local hardware with the data and the code being stored locally-. Such\na setup has the advantage of being lightweight allowing a fast turnaround between two ML iterations.\nHowever the lack of traceability, compatibility and hardware limitation makes it challenging to scale\nsuch application, hence the requirement of a more scalable solution.\n\nIn comparison, a scalable ingestion platform is a type of system native able to handle current and future\nworkloads regardless of the amount of data ingested or users connecting to it, considering both scaling\nin (shrinking resources) and scaling out (expanding resources). An example of such system can be seen\non Figure 1. The architecture is made of of four components namely pipeline, storage, compute and\ninterfaces.\n\nINTERFACES,\n\nf Data Atelier\n\nImages\n\nCOMPUTE\nProduction\nRecords\n\nReparts,\nPresentations\n\nAPI Interaction\n\nMaintenance\nRecords\n\nFigure 1: Ingestion Platform Architecture\n\nPipeline or data pipeline is the main driver in extracting and transforming data into a unified format.\nThe typical ingestion workflow is based on an Extract-Transform-Load process with machine learning\nworkflows a'),(33,3,4,'seen\non Figure 1. The architecture is made of of four components namely pipeline, storage, compute and\ninterfaces.\n\nINTERFACES,\n\nf Data Atelier\n\nImages\n\nCOMPUTE\nProduction\nRecords\n\nReparts,\nPresentations\n\nAPI Interaction\n\nMaintenance\nRecords\n\nFigure 1: Ingestion Platform Architecture\n\nPipeline or data pipeline is the main driver in extracting and transforming data into a unified format.\nThe typical ingestion workflow is based on an Extract-Transform-Load process with machine learning\nworkflows able to accommodate the variety in sources and forms present in unstructured data\n(Hernandez et al., 2019).\n\n\n\nMADRID | SPAIN\n\nThe storage component is where the data resides. Storage is often the least thought about in development\nand architecture strategies, but it is one of the core contributors in terms of cost to the organization. The\ncompute node component schedules and orchestrates the data pipelines, logic flows, and algorithms.\nThe interface or the dashboard component provides the monitoring and observability capabilities. This\npresents the events, statuses, logs, and other operational insights that can be used for decision making.\nThese components have different ways to be scaled depending of 1) the environment of installation: on-\npremise vs. cloud 2) the volume and type of data being processed 3) the early development decision: all\nout of the box cloud providers solution, opensource self maintained solution or hybrid.\n\nComponent’s optimization\n\nThe first metrics to consider '),(34,3,5,'ring and observability capabilities. This\npresents the events, statuses, logs, and other operational insights that can be used for decision making.\nThese components have different ways to be scaled depending of 1) the environment of installation: on-\npremise vs. cloud 2) the volume and type of data being processed 3) the early development decision: all\nout of the box cloud providers solution, opensource self maintained solution or hybrid.\n\nComponent’s optimization\n\nThe first metrics to consider for optimization is processing time and should be independent from the\nvolume of data to be processed by the pipeline. This optimization is achieved through parallelization,\ndistributed computing and orchestration by scaling up or down the usage based on demand. Applications\nand data pipelines are packaged into containers and then deployed into a Kubernetes cluster in which it\nhandles the scheduling, distribution, and allocation across different machines. Workflow managers (\nWM) provide the platform to execute and orchestrate the jobs and tasks that are contained in a Pod/Pool\n(Figure 2). Workflow managers also implement orchestration and scheduling though it handles the tasks\nin the application layer where this determines if the data execution is successful\n\nFigure 2: Unstructured data pipeline with orchestration in place\n\nThe second metric to consider is the extensibility metric. A single service or component can be used\nbeyond what is originally intended. Thanks to the microservices'),(35,3,6,'form to execute and orchestrate the jobs and tasks that are contained in a Pod/Pool\n(Figure 2). Workflow managers also implement orchestration and scheduling though it handles the tasks\nin the application layer where this determines if the data execution is successful\n\nFigure 2: Unstructured data pipeline with orchestration in place\n\nThe second metric to consider is the extensibility metric. A single service or component can be used\nbeyond what is originally intended. Thanks to the microservices, exposing your data via application\nprogramming interfaces or APIs is a good practise (Figure 3). It provides a common language across\nteams where implementation is unified across different data sources. An additional advantage of the use\nof APIs is the additional level of abstraction for the storage of the data. Data are now accessible from\nvarious mounting locations through a single call, allowing democratization and versioning of the data.\n\nAPI LINK\n\nAPI LINK\n\n\n\nMADRID | SPAIN\n\nFigure 3: Connecting applications using API links\n\nAs an example, a full-text search endpoint originally used to do searches, can also be used to create\naggregations models like heatmap and knowledge graphs (Baillard et Al., 2021) as seen on Figure 4.\n\n@ wees\n\nFy ‘eure 4 4; Heatmap (left) and Knowledge Graph (right)\n\nThe third metric to consider is the UI/UX responsivness and define the optimal data representation for\nthe user or the operator to QC the data and for the service to have still an acceptable res'),(36,3,7,'necting applications using API links\n\nAs an example, a full-text search endpoint originally used to do searches, can also be used to create\naggregations models like heatmap and knowledge graphs (Baillard et Al., 2021) as seen on Figure 4.\n\n@ wees\n\nFy ‘eure 4 4; Heatmap (left) and Knowledge Graph (right)\n\nThe third metric to consider is the UI/UX responsivness and define the optimal data representation for\nthe user or the operator to QC the data and for the service to have still an acceptable response time. As\nseen on Figure 4, the heat map and the knowledge graph are built on-top of 100,000 data points\n(Mamador et al., 2021). Interface and visualization scaling requires removing the dependency on the\nvolume of data points. The amount of time visualizing 100 points should be the same as visualizing\n100,000 points — in which development of aggregation and interpolation workflows must be performed\nin the backend and frontend to overcome this volume challenge.\n\nBenchmarks and testing\n\nTo see the performance of this architecture we can consider the following data. We have 3 buckets of\ndata for this assessment. It is composed of geoscience documents that ranges from final well reports,\ngeological report, regional studies, interpretations coming from various file formats, countries, and\nlanguages. The data also includes images like thin sections, core, well logs, seismic. All these data are\naudited and stored into an object storage.\n\nThe following are the system that was used for th'),(37,3,8,'ee the performance of this architecture we can consider the following data. We have 3 buckets of\ndata for this assessment. It is composed of geoscience documents that ranges from final well reports,\ngeological report, regional studies, interpretations coming from various file formats, countries, and\nlanguages. The data also includes images like thin sections, core, well logs, seismic. All these data are\naudited and stored into an object storage.\n\nThe following are the system that was used for this assessment:\nBase case: 1-node with 4 cores and 16 GB of RAM from cloud provider without WM\nized case: 5-nodes with 4 cores and 16 GB of RAM per node from cloud provider with WM\n\nBucket | Number | Duration Failure Duration | Failure | Language Region\nof Pages (hours) Rate % (hours) Rate %\nSingle node (Single Cluster (Cluster)\nNode)\n\nee\n~Woas/ af 38505 Tals | Osean\n\nne 531 Mixed South\nAmerica\n\nTable 1: Extraction Performance with 4-cores and 16 GB of RAM per node\n\nTable 1 shows the extraction performance. The extraction workload encompasses the data loading,\nprocessing, and uploading. It was tested on a variety of datasets from different regions and languages.\nFor a single node execution, the average number of pages per hour is around 244 pages and for a 5-node\ncluster, the average number of pages per hour is 1000. Since workflow manager provide an auto-retry\n\n\n\nMADRID | SPAIN\n\nfunction, this reduces the failure rate by a significant amount. This is due to tasks that are failing due t'),(38,3,9,'erformance. The extraction workload encompasses the data loading,\nprocessing, and uploading. It was tested on a variety of datasets from different regions and languages.\nFor a single node execution, the average number of pages per hour is around 244 pages and for a 5-node\ncluster, the average number of pages per hour is 1000. Since workflow manager provide an auto-retry\n\n\n\nMADRID | SPAIN\n\nfunction, this reduces the failure rate by a significant amount. This is due to tasks that are failing due to\nnetwork or infrastructure issues to be retried or re-executed. The failure rate is now mostly on the data\nsince we haveve removed the infrastructure constraint.\n\nAs one of the metrics that we have set earlier, we need to consider the cost and infrastructure usage.\nUsing the same workloads, we can assess the cost and infrastructure optimization in Table 2.\n\na % Hardware % Hardware\nPages usage Avg. usage - Avg.\nSingle node Cluster\n\nTable 2: Extraction Infrastructure Utilization with 4-cores and 16GB of RAM\n\nTable 2 shows that we have a higher utilization in the cluster hence we are fully utilizing the server. In\nthe case of single node, only 41% of the resources is utilized vs 83% in the cluster approach. Both the\nreduced failure rates and the hardware utilization illustratres the gain of efficency in deploying a scalable\ningestion platform\n\nConclusions\n\nIn this paper, we have seen how to scale and optimize the ML ingestion pipeline for subsurface\napplications. We have shown that scali'),(39,3,10,'16GB of RAM\n\nTable 2 shows that we have a higher utilization in the cluster hence we are fully utilizing the server. In\nthe case of single node, only 41% of the resources is utilized vs 83% in the cluster approach. Both the\nreduced failure rates and the hardware utilization illustratres the gain of efficency in deploying a scalable\ningestion platform\n\nConclusions\n\nIn this paper, we have seen how to scale and optimize the ML ingestion pipeline for subsurface\napplications. We have shown that scaling and optimizing ML ingestion pipelines can lead to\nimprovements in terms of time and cost. We have also demonstrated that the ingestion platform is\nscalable to handle data from a variety of regions and languages. Finally, we’ve seen the power of scaling\nvisualizations and exposing the data via API links in which we can get more insight and enhance the\nability of the team to extract knowledge.\n\nReferences\n\nBaillard F. and Hernandez N.: A Case Study of Understanding Bonaparte Basin using Unstructured\nData Analysis with Machine Learning Techniques. 82nd EAGE Conference & Exhibition, 18-21\nOctober 2021, Amsterdam.\n\nMamador C., Hernandez N., Baillard F.: Production-scale processing of EAGE’s EarthDoc data\nto stimulate new insights in CO2 and new energy management. 82nd EAGE Conference &\nExhibition worskhop on ML solutions at scale, 22 October 2021, Amsterdam.\n\nHernandez N., Lucajias P., Graciosa J.C., Mamador C., and Panganiban L. C. I., 2019: Automated\n\ninformation retrieval from unstruc'),(40,3,11,'tured\nData Analysis with Machine Learning Techniques. 82nd EAGE Conference & Exhibition, 18-21\nOctober 2021, Amsterdam.\n\nMamador C., Hernandez N., Baillard F.: Production-scale processing of EAGE’s EarthDoc data\nto stimulate new insights in CO2 and new energy management. 82nd EAGE Conference &\nExhibition worskhop on ML solutions at scale, 22 October 2021, Amsterdam.\n\nHernandez N., Lucajias P., Graciosa J.C., Mamador C., and Panganiban L. C. I., 2019: Automated\n\ninformation retrieval from unstructured documents utilizing a sequence of smart machine learning\n\nmethods within a hybrid cloud container. EAGE Workshop on Big Data and Machine Learning for\nE&P Efficiency 25 - 27 February.\n\n\n'),(41,4,1,'\nBalas Premie? Gensrience Event\n\nT. Looi\', N.E. Arif\', N.M. Hernandez?, F. Baillard?\n\' Traya Energies\n\nSummary\n\nIn the upstream oil and gas sector, the processes of data mining, which involves searching, extracting, and\nvalidating information that sits within the technical documents, reports, presentations, and studies to understand\nexploration history and geological parameters are often challenging and requires vast resources to be completed.\nYet, many geological information that have already been mined, are stored in spreadsheets or niche databases,\nthat limits their abilities to be recycled for multiple uses within the organization. Basin information such as\nformation pressure, formation temperature, fracture pressure, drilling rate of penetration, total organic carbon\n(TOC) and lithologies are some of the typical parameters that are crucial to understand the basin scale geology,\nreservoir properties and identify opportunities within the area of interest and often needed to perform in depth\nworkflows, such as seismic reservoir characterization, basin modelling and geomechanical studies, which cover\nmultiple cycles of exploration, development and site development of future carbon waste disposals. The research\ndemonstrates the application of AI/ML technologies coupled with interactive data visualization and API\nconnectivity, that can significantly accelerate the extraction of knowledge that are sitting inside the reports and\nensure sustainability in data mining activities.\n\n'),(42,4,2,'in depth\nworkflows, such as seismic reservoir characterization, basin modelling and geomechanical studies, which cover\nmultiple cycles of exploration, development and site development of future carbon waste disposals. The research\ndemonstrates the application of AI/ML technologies coupled with interactive data visualization and API\nconnectivity, that can significantly accelerate the extraction of knowledge that are sitting inside the reports and\nensure sustainability in data mining activities.\n\nAPGCE 2022\nKuala Lumpur, Malaysia | 28 — 29 November 2022\n\n\n\nIntroduction\n\nIn the upstream oil and gas sector, the processes of data mining, which involves searching, extracting,\nand validating information that sits within the technical documents, reports, presentations, and studies\nto understand exploration history and geological parameters are often challenging and requires vast\nresources to be completed. Yet, many geological information that have already been mined, are stored\nin spreadsheets or niche databases, that limits their abilities to be recycled for multiple uses within the\norganization. Basin information such as formation pressure, formation temperature, fracture pressure,\ndrilling rate of penetration, total organic carbon (TOC) and lithologies are some of the typical\nparameters that are crucial to understand the basin scale geology, reservoir properties and identify\nopportunities within the area of interest and often needed to perform in depth workflows, such as\nseismic r'),(43,4,3,' or niche databases, that limits their abilities to be recycled for multiple uses within the\norganization. Basin information such as formation pressure, formation temperature, fracture pressure,\ndrilling rate of penetration, total organic carbon (TOC) and lithologies are some of the typical\nparameters that are crucial to understand the basin scale geology, reservoir properties and identify\nopportunities within the area of interest and often needed to perform in depth workflows, such as\nseismic reservoir characterization, basin modelling and geomechanical studies, which cover multiple\ncycles of exploration, development and site development of future carbon waste disposals.\n\nIn this paper, we are going to demonstrate on how we can improve on the traditional data mining\nworkflows and enhance data sustainability by focusing process improvement on three areas: a.)\naccelerating the speed of geoscience parameter extraction by Artificial Intelligence/ Machine learning\n(AI/ML) techniques, b.) increase data readability and integrity by effective visualization of both\nsource and output data, and c.) promote data reusability by enabling API access to extracted data.\n\nMethodology\n\nThe advancement in ML and AI with the support of advancement in cloud computing has made a\nsignificant impact to the traditional data mining workflows. However, AI/ML workflows are typically\nfocused only on the data extraction phase as a one-time effort, whereas an effective data mining\nstrategy should also ensu'),(44,4,4,'ncrease data readability and integrity by effective visualization of both\nsource and output data, and c.) promote data reusability by enabling API access to extracted data.\n\nMethodology\n\nThe advancement in ML and AI with the support of advancement in cloud computing has made a\nsignificant impact to the traditional data mining workflows. However, AI/ML workflows are typically\nfocused only on the data extraction phase as a one-time effort, whereas an effective data mining\nstrategy should also ensure data sustainability, which means the ability to visualize the data to identify\ntrends and patterns easily, and pass that high-integrity extracted information intact to multiple type of\nusers within the organization.\n\nA sustainable data mining strategy is proposed to accelerate extraction with an automated pipeline\nusing Machine Learning techniques such as Natural Language Processing (NLP) or Deep\nConvolutional Neural Network (DCNN) ingests all the unstructured data (Hernandez et al., 2019) in\nsteps 1 to 3, followed by human-in-the loop quality control, visualization and data trackability and\nconnectivity in steps 4 and 5. (Figure 1):\n\nA vast amount of unstructured data such as final well report, technical reports, working files\nthat varies from .pdf, .docx., .xlsx, .csv jpg, .png and .tif are used as the main source of\ninformation and feed into the production ready ML pipelines for audit, duplicates, and version\ndetections.\n\nThe unstructured data ingestion starts with the digitaliza'),(45,4,5,'\nsteps 1 to 3, followed by human-in-the loop quality control, visualization and data trackability and\nconnectivity in steps 4 and 5. (Figure 1):\n\nA vast amount of unstructured data such as final well report, technical reports, working files\nthat varies from .pdf, .docx., .xlsx, .csv jpg, .png and .tif are used as the main source of\ninformation and feed into the production ready ML pipelines for audit, duplicates, and version\ndetections.\n\nThe unstructured data ingestion starts with the digitalization of data using Optical Character\nRecognition (OCR). Next, Deep Convolutional Neural Network (DCNN) classifies the\nextracted images into their respective geological categories such as map, seismic,\nstratigraphic chart, SEM, thin section, core and well logs. Simultaneously, Natural Language\nProcessing (NLP) pipeline performs automated extraction and tagging of metadata.\n\nFurther analysis and knowledge extraction are available once the unstructured data is\ningested. Data Science analytical tool such as deep search with heat map density allows\nadditional insights where the user can monitor the trend of a parameter regionally. In addition,\ntables extracted from the reports are post-processed to retain the structure and extract the\nvalues.\n\nMissing depth interpolation, units’ standardization and plotting of values on the gradient\nisolines are part of the data validation procedures during the Human in the Loop quality\ncontrol.\n\nAPGCE 2022\nKuala Lumpur, Malaysia | 28 — 29 November 2022\n\n\n\n'),(46,4,6,' such as deep search with heat map density allows\nadditional insights where the user can monitor the trend of a parameter regionally. In addition,\ntables extracted from the reports are post-processed to retain the structure and extract the\nvalues.\n\nMissing depth interpolation, units’ standardization and plotting of values on the gradient\nisolines are part of the data validation procedures during the Human in the Loop quality\ncontrol.\n\nAPGCE 2022\nKuala Lumpur, Malaysia | 28 — 29 November 2022\n\n\n\nThe last step is the discovery stage that allows shareable structured data among the team\nmembers for further interpretation and insights. The final shareable structured data allows\ndata trackability within unstructured data, data export functionality, spatial filter to confine\nthe search to the area of interest, depth filter and outliers’ identification. The typical outputs\nof data mining exercise are the standard excel or csv as application agnostic format. In this\nstage, we also enabled API access to allow for easier connectivity to other geoscience\nplatforms or internally developed digital infrastructure systems.\n\nUnstructured Data ML/AI Pipeline and Human-in-the-Loop Quality Control\n\n1. Input 2. Unstructured Data Ingestion 3. Knowledge Extraction\n\nUnstructured Data Deep Convalutianal Neural Network (DCNN)\n\nvil\n\nExtracted\nimages\n\nImages Images\nclassification aes\n\nNatural Language Pracessing (NLP) Convertable\n\n¥\n\n5. Discovery 4, Human-in-the-Loop Quality Control\n\nShareable Depth int'),(47,4,7,'ccess to allow for easier connectivity to other geoscience\nplatforms or internally developed digital infrastructure systems.\n\nUnstructured Data ML/AI Pipeline and Human-in-the-Loop Quality Control\n\n1. Input 2. Unstructured Data Ingestion 3. Knowledge Extraction\n\nUnstructured Data Deep Convalutianal Neural Network (DCNN)\n\nvil\n\nExtracted\nimages\n\nImages Images\nclassification aes\n\nNatural Language Pracessing (NLP) Convertable\n\n¥\n\n5. Discovery 4, Human-in-the-Loop Quality Control\n\nShareable Depth interpolation Units Standardization values Quatity Control\nStructured Oata\n\n> Data trackatility\nta1010 ca iterpolabon c1 ® Pressure kgem? to psi\nGuoie ly | > Export data Functionality teterpolation of TOSS te tae Oe Potting extracted values\n\n> Spatial fuer and plat re-computation <—_— values using - radient ola\nSrectonat sata : > Depth from TVD to TVDSS gradient wales\n\n> Depth Filter and plot re-computation\n\n> Depth from ft tom\n> utters identification Depth from ftto\n\nFigure 1 Full implemented workflow of ingestion and analysis of unstructured data using ML/AI\n\nWe highlight the human-in-the-loop process during the data extraction process in step 4 above, which\nmeans there is a component of human interpretation during the geoscience parameter extraction\nprocess. Since there could be conflicting understanding on what is considered right or relevant data at\nthe time the extraction was done, it is important to keep the sources of information intact, so these can\nbe reviewed and updated when n'),(48,4,8,'flow of ingestion and analysis of unstructured data using ML/AI\n\nWe highlight the human-in-the-loop process during the data extraction process in step 4 above, which\nmeans there is a component of human interpretation during the geoscience parameter extraction\nprocess. Since there could be conflicting understanding on what is considered right or relevant data at\nthe time the extraction was done, it is important to keep the sources of information intact, so these can\nbe reviewed and updated when new additional data is available and the geological understanding of\nthe basin of interest has evolved. The two additional steps introduced in the data mining process\nincreases data integrity.\n\nResults\n\nIn this study, unstructured data from over 500 oil and gas wells are processed on a regional scale this\nincludes a total of 300,000 pages and 140,000 images. Six geological parameters, formation pressure,\nformation temperature, fracture pressure, drilling rate of penetration, total organic carbon (TOC) and\nlithologies were efficiently extracted, aggregated, validated, and finally visualized on scatter plots and\npie charts. The output from the case study shows notable knowledge analysis (Figure 2) that provides\ninsights on the regional consistency and information distribution of the area of interest and can be\nused as input into petroleum system modelling, reservoir characterization, idle wells review,\ngeomechanical studies or potential carbon storage studies.\n\nAPGCE 2022\nKuala Lumpur, Ma'),(49,4,9,'(TOC) and\nlithologies were efficiently extracted, aggregated, validated, and finally visualized on scatter plots and\npie charts. The output from the case study shows notable knowledge analysis (Figure 2) that provides\ninsights on the regional consistency and information distribution of the area of interest and can be\nused as input into petroleum system modelling, reservoir characterization, idle wells review,\ngeomechanical studies or potential carbon storage studies.\n\nAPGCE 2022\nKuala Lumpur, Malaysia | 28 — 29 November 2022\n\n\n\nFigure 2 Lithological pie chart distribution (left) and scatter plots of total organic carbon (TOC),\nrate of penetration (ROP), formation temperature, fracture pressure and formation pressure (right)\n\nConclusions\n\nThe research demonstrates the application of AI/ML technologies coupled with interactive data\nvisualization and API connectivity, that can significantly accelerate the extraction of knowledge that\nare sitting inside the reports and ensure sustainability in data mining activities. In the case study, we\nhave presented a workflow on how six (6) geological parameters from over 500 wells were\nsuccessfully extracted from unstructured data using ML and AI pipelines that can be used by multi-\nfaceted subsurface teams for more in-depth analysis within the area, resulting to the upcycling of\ngeological data across the full life cycle of a basin.\n\nReferences\n\nHernandez, N. M., Lucafias, P. J., Mamador, C., & Panganiban, L. [2019]. Automated Information\n'),(50,4,10,'y in data mining activities. In the case study, we\nhave presented a workflow on how six (6) geological parameters from over 500 wells were\nsuccessfully extracted from unstructured data using ML and AI pipelines that can be used by multi-\nfaceted subsurface teams for more in-depth analysis within the area, resulting to the upcycling of\ngeological data across the full life cycle of a basin.\n\nReferences\n\nHernandez, N. M., Lucafias, P. J., Mamador, C., & Panganiban, L. [2019]. Automated Information\nRetrieval from Unstructured Documents Utilizing a Sequence of Smart Machine Learning Methods\nwithin a Hybrid Cloud Container. EAGE Workshop on Big Data and Machine Learning for E&P\nEfficiency 25-27 February.\n\nMamador, C., Aranda, J. O., Arif, N. E., Hernandez, N. M., & Baillard, F. [2020]. A Geological\nRegional Case Study for Pressure, Temperature, and Salinity for the GoM using Machine Learning\nTechnology on Unstructured Data. AAPG Digital Subsurface for Asia Pacific Conference. Kuala\nLumpur, Malaysia.\n\nAPGCE 2022\nKuala Lumpur, Malaysia | 28 — 29 November 2022\n\n\n'),(51,5,1,'\nUtilizing Machine Learning to Gain Geological Insights through Unstructured Data for\nSustainable Exploration Activities — Case Study Pre-Salt Brazil\n\nIntroduction\n\nUnderstanding the basin regional trends and identifying the anomalies is a crucial background research\nduring basin exploration activities. One way to gain a sound knowledge about the geology and the\nexploration history is to analyse the vast amount of data accumulated over the years in an unstructured\nmanner. A sustainable data driven strategy leveraging on the latest advancement of Machine Learning\n(ML) and Analytics is applied on vast amount of unstructured data. By highlighting the data driven\nstrategy, the paper demonstrates such a strategy applied to pre-salt carbonates prospects located in the\nCampos and Santos Basins, offshore Brazil. The interpretation framework consists of first step, to\nidentify the regional first order trends and second step, to recognize the second order anomalies over\nthe full area providing a holistic picture of the area of interest.\n\nMethodology\n\nThe unstructured data comprise of more than 48,000 documents, primarily in Portuguese language, for\na total of 330,000 pages related to 50 years of exploration, covering the Campos and Santos Basins. The\nmethodologies are divided into two parts. In the part one methodology, the unstructured data is\nprocessed using machine learning techniques such as Natural Language Processing (NLP) for name\nentity recognition and language translation, and'),(52,5,2,' holistic picture of the area of interest.\n\nMethodology\n\nThe unstructured data comprise of more than 48,000 documents, primarily in Portuguese language, for\na total of 330,000 pages related to 50 years of exploration, covering the Campos and Santos Basins. The\nmethodologies are divided into two parts. In the part one methodology, the unstructured data is\nprocessed using machine learning techniques such as Natural Language Processing (NLP) for name\nentity recognition and language translation, and Deep Convolutional Neural Network (DCNN) for auto-\nimage recognition (Hernandez et al., 2019). In part two methodology, ML Analytics leverages the\nvisualization for data relationships to give a holistic view of the whole corpus (Baillard et al., 2021).\nThree steps study is used here, as explained below with illustration at Figure 1.\n© Step 1: Deep Search through text is the ability to use keywords search within the whole corpus\nand filter the wanted results. Filtered information is easily accessible for investigation. Deep\nSearch is also applied for images. DCNN image recognition and classification techniques classify\nthe images into eight categories: thin section, core, well plot, seismic, stratigraphic structural\nelements, map, table, and figure. A deep search allows user to search information tagged and\nidentified in these images. This step is to identify the geological and exploration challenges\nrelated to the area of interest.\nStep 2: Heat Map highlights the “hot zones” on map ba'),(53,5,3,'sible for investigation. Deep\nSearch is also applied for images. DCNN image recognition and classification techniques classify\nthe images into eight categories: thin section, core, well plot, seismic, stratigraphic structural\nelements, map, table, and figure. A deep search allows user to search information tagged and\nidentified in these images. This step is to identify the geological and exploration challenges\nrelated to the area of interest.\nStep 2: Heat Map highlights the “hot zones” on map based on the frequency of the keywords\nfound. Drawing a polygon on the map allows user to confine the search to focus only in the zone\nof their interest. On the other hand, instant text search through the whole corpus of English and\nPortuguese documents is possible with the automated translation. Users able to search through\nPortuguese documents using English keywords.\nStep 3: Contextual Knowledge Graph illustrates the connectivity of ‘related corpuses’ based on\nwell name referencing. This is useful to obtain related information from different wells\n(Hernandez et al., 2019). Besides that, Intuition provides clustered images view, a great approach\nto discover analogues for alike images such as thin section, core, SEM, and biomarker.\n\n« Deep Search within documents and * Information correlation using\nexport CSV Knowledge Graph\n\n—\n/ Deep Search through images and\nretrieve * Autometed translation of Portuguese * Analogs ieentfication using\n: to English, to be able ta combine Image Clustering'),(54,5,4,'ul to obtain related information from different wells\n(Hernandez et al., 2019). Besides that, Intuition provides clustered images view, a great approach\nto discover analogues for alike images such as thin section, core, SEM, and biomarker.\n\n« Deep Search within documents and * Information correlation using\nexport CSV Knowledge Graph\n\n—\n/ Deep Search through images and\nretrieve * Autometed translation of Portuguese * Analogs ieentfication using\n: to English, to be able ta combine Image Clustering\n\nm\n— information irom to languages\n\n2 py\n\nFigure I The data driven search strategy implementing the ML Analytics for visualization of data\nrelationships for a holistic view of whole corpus\n\nGE Conference on D: 1 Innovat\n\nfor a Sustainable Future\n\n\nCase study: Understanding the Pre-Salt Carbonate Regional Trends and Anomalies in Campos\nand Santos Basins\n\nThe Campos and Santos Basins, located to the east offshore of Brazil, are some of the most prolific oil\nand gas basins in the world with significant discoveries such as Tupi, Jupiter and Libra Fields. The\ninvestigation of the 50 years of pre-salt exploration history contained in the 48,000 documents processed\nrevealed that most of the challenges during exploration are caused by 1. Fluid distribution 2. The\nreservoir quality 3. CO2 and H2S presence 4. Overpressure patterns,\n\n1. Campos and Santos Basins are showing a variable fluid distribution. The type and quality of\nthe fluid trends are keys to define the best target for a sustainable'),(55,5,5,'icant discoveries such as Tupi, Jupiter and Libra Fields. The\ninvestigation of the 50 years of pre-salt exploration history contained in the 48,000 documents processed\nrevealed that most of the challenges during exploration are caused by 1. Fluid distribution 2. The\nreservoir quality 3. CO2 and H2S presence 4. Overpressure patterns,\n\n1. Campos and Santos Basins are showing a variable fluid distribution. The type and quality of\nthe fluid trends are keys to define the best target for a sustainable development.\n\n2. The diagenesis affects the reservoir quality. A good visualization of the lateral distribution of\nsuch process allows a better estimation of the porosity and the volume in place.\nThe regional gas issues, such as understanding the presence of CO2 and H2S is essential to\ngauge the reservoir diagenesis properties as well as to plan for production facilities or to avoid\nregions of sour and hazardous gaseous.\nThe irregularity in the thickness of the salt layer causes variability in the pressure regime with\noverpressure over the zone of interest and affecting the drilling campaigns.\n\nThese challenges are relevant to understand the geology and production issues encountered in distinct\nparts of the basins to improve sustainable in the exploration risk and reducing CO? footprint.\n\nFor the pre-salt oil trendings, Campos pre-salt generally has lighter oil compared to Santos pre-salt. As\nfor the anomalies, light oil or condensate discovery at Pau De Acucar, Seat and Gavea Fields '),(56,5,6,'the pressure regime with\noverpressure over the zone of interest and affecting the drilling campaigns.\n\nThese challenges are relevant to understand the geology and production issues encountered in distinct\nparts of the basins to improve sustainable in the exploration risk and reducing CO? footprint.\n\nFor the pre-salt oil trendings, Campos pre-salt generally has lighter oil compared to Santos pre-salt. As\nfor the anomalies, light oil or condensate discovery at Pau De Acucar, Seat and Gavea Fields in Campos\npre-salt, while heavy mostly found in the post-salt in the Southern Campos. In Santos, heavy oil reported\nin Jupiter Field. Note that heavy oil also discovered in Atlanta and Oliva Fields at post-salt. Illustration\nin Figure 2.\n\n«Guarulhos\n\nMediuni dil\nAP average\n0\n\nFigure 2 Heat map highlights the wells with “heavy oil” information. The oil API delineation is based\non Deep Search extraction from test reports for Santos (left map) and Campos (right map). Knowledge\nGraph was analyzed to find a group of fields that share the light oil information — Gavea, Seat and Pao\nde Acucar Fields.\n\nNext, we study the facies distribution at pre-salt carbonate. A collection of core and thin section images\nare retrieved from the Deep Search through images. In general, it is observed that the reservoir\nformation is in the shrub facies, microbiolites and coquina. For the trendingss, the area with COQo,\nleaching activities enhance the porosity, example in the Libra, Buzios Fields, going south to'),(57,5,7,' find a group of fields that share the light oil information — Gavea, Seat and Pao\nde Acucar Fields.\n\nNext, we study the facies distribution at pre-salt carbonate. A collection of core and thin section images\nare retrieved from the Deep Search through images. In general, it is observed that the reservoir\nformation is in the shrub facies, microbiolites and coquina. For the trendingss, the area with COQo,\nleaching activities enhance the porosity, example in the Libra, Buzios Fields, going south to Tupi and\nCarcara Fields in Santos. Fields proximal to the shore has poorer reservoir quality. One of the anomalies\nobserved is the hydrothermal activity can destroy the enhanced porosity by the leaching. The\nhydrothermal activity is anticipated in Field Albacora, Carcara, based on the observation on the core.\nIllustration in Figure 3.\n\nGE Conference on D: for a Sustainable Future\n\n\nMERO/LIBRA.\n\nMicrobial carbonates in\nsag sequence and coquina\nrift sequence\n\nITARPU\nShrub facies’ss.\n-cuaruitos\nMEXILHAO-\nPoor\nreservoir\nquality\n\nBUZIGS, ATAPU,SEPIA\nbial carbonates in\n\nsag sequence and\n\ncoquina rift sequence\n\n) frame\nFigure 3 Pre-salt carbonate facies distribution based on core and thin sections collection from Deep\nSearch through images. Analysing the diagenesis trend has an impact on reservoir quality\ninterpretation. (core and thin section images are taken from BDEP-ANP reports and Core Lab study)\n\nThe following study is to investigate the regional gas issues. The trend is showing that S'),(58,5,8,'XILHAO-\nPoor\nreservoir\nquality\n\nBUZIGS, ATAPU,SEPIA\nbial carbonates in\n\nsag sequence and\n\ncoquina rift sequence\n\n) frame\nFigure 3 Pre-salt carbonate facies distribution based on core and thin sections collection from Deep\nSearch through images. Analysing the diagenesis trend has an impact on reservoir quality\ninterpretation. (core and thin section images are taken from BDEP-ANP reports and Core Lab study)\n\nThe following study is to investigate the regional gas issues. The trend is showing that Santos is\nsuffering higher CO2 contamination compared to Campos, especially in the center deep-water Santos\narea. Jupiter Field suffers from very high concentration of CO2 gas, 79% recorded. Center deeper-water\nSantos is suffering from CO2 contamination potentially near to the mantel magmatic activities\nintercepted by deep-seated fault (Luca et al., 2017). Hazardous H2S gas presence in high amount\n(exceeding OSHA safe limit 50 ppm) in the Buzios, Iara Fields of Santos and Xerelete, Albacora Fields\nof Campos. Illustration in Figure 4.\n\neGuaruihos\nSéo Paulo”\n\nSantg\n\n, ‘BACALFAU\nBEM TE VI\nABERE\nOESTE\nCO, 15%\nFigure 4 Heat Map of “high CO2” information with delineation of CO2 concentration based on Deep\n\nSearch extraction from test reports in Santos (left map) and Campos (right map)\n\nLastly, is the study of formation pressure pattern. The formation data points are quickly extracted from\nunstructured data over 80 wells using Deep Search. The points are then plotted on gradient psi/ft ranging'),(59,5,9,'s\nof Campos. Illustration in Figure 4.\n\neGuaruihos\nSéo Paulo”\n\nSantg\n\n, ‘BACALFAU\nBEM TE VI\nABERE\nOESTE\nCO, 15%\nFigure 4 Heat Map of “high CO2” information with delineation of CO2 concentration based on Deep\n\nSearch extraction from test reports in Santos (left map) and Campos (right map)\n\nLastly, is the study of formation pressure pattern. The formation data points are quickly extracted from\nunstructured data over 80 wells using Deep Search. The points are then plotted on gradient psi/ft ranging\nfrom 0.35 to 0.7 psi/ft. Based on the pressure gradient validation, wells in Carcara, Sagitario and\nCorcovado Fields have exceptionally high formation pressure. An interpretation of fullstack PSDM\ncross-section shows thick salt area in Carcara and Sagitario, possible with faulting causes saline water\nintrusion which contribute to high pressure zone. Irregularity in thickness of salt layer causes variability\nin pressure regime, such as overpressure area to take note as this will have an impact on seismic\ninterpretation and drilling campaign. Illustration in Figure 5.\n\nEAGE Conference on Digital Innovation for a Sustainable Future\n\n\nFormation Pressure, psi\n10000 15000\n\nFigure 5 Formation pressure extraction for over 80 wells (all data points taken from BDEP-ANP\nreports). Exceptionally high pressure recorded in Guaratiba Group in Carcara, Sagitario and\nCorcovado, possible due to high salt thickness and faulting causes saline water. Fullstack PSDM cross\nsection shows thick salt in area of'),(60,5,10,'n seismic\ninterpretation and drilling campaign. Illustration in Figure 5.\n\nEAGE Conference on Digital Innovation for a Sustainable Future\n\n\nFormation Pressure, psi\n10000 15000\n\nFigure 5 Formation pressure extraction for over 80 wells (all data points taken from BDEP-ANP\nreports). Exceptionally high pressure recorded in Guaratiba Group in Carcara, Sagitario and\nCorcovado, possible due to high salt thickness and faulting causes saline water. Fullstack PSDM cross\nsection shows thick salt in area of Carcara and Sagitario (courtesy of Kattah et al., 2014).\n\nConclusion\n\nThe research shows the effectiveness of using ML/AI technologies and Analytics to mine through the\nvast amount of unstructured data and gain insights related to the regional trends and anomalies of\nimportant geological, reservoir and production parameters to minimize the risk of exploration and\nreduce carbon footprint. New tools in ML/AI and Analytics provides a new exploration frontier for\nsubsurface experts allowing them to interrogate and visualize a vast amount of data previously scattered\nin different format and location holistically.\n\nAcknowledgement\n\nWe would like to thank PETRONAS Petroleo Brasil LTDA for the collaboration with Iraya Energies\nto makes this paper possible. Our gratitude appreciation also goes to Banco de Dadoes Exploracao e\nProducao (BDEP) of National Agency of Petroleum (ANP) for making the data source available for\nthis study.\n\nReferences\n\nBaillard, F., & Hernandez, N. (2021). A Case Study '),(61,5,11,' interrogate and visualize a vast amount of data previously scattered\nin different format and location holistically.\n\nAcknowledgement\n\nWe would like to thank PETRONAS Petroleo Brasil LTDA for the collaboration with Iraya Energies\nto makes this paper possible. Our gratitude appreciation also goes to Banco de Dadoes Exploracao e\nProducao (BDEP) of National Agency of Petroleum (ANP) for making the data source available for\nthis study.\n\nReferences\n\nBaillard, F., & Hernandez, N. (2021). A Case Study of Understand Bonarparte Basin using Unstructured\nData Analysis with Machine Learning Techniques. EAGE Annual.\n\nHernandez, M., & Baillard, F. (2019). An effective G&G exploration strategy inspired by a wolfpack.\nForce workshop.\n\nHernandez, N., Lucafias, P., Graciosa, J, Mamador, C., & Panganiban, I. (2019). Automated\nInformation Retrieval from Unstructured Documents Utilizing a Sequence of Smart Machine Learning.\nEAGE Workshop on Big Data and Machine Learning for E&P Efficiency 25 - 27 February.\n\nHydrogen Sulfide. (2022). Retrieved from Occupational Safety and Health Administration:\nhttps://www.osha.gov/hydrogen-sulfide/hazards\n\nCore Lab [2022]. Pre-Salt Reservoirs of the Santos and Campos Basins, Brazil\n\nLuca, Pedro & Matias, Hugo & Carballo, Jose & Sineva, Diana & Pimentel, Gustavo & Tritlla, Jordi &\nCerda, Mateu & Loma, Rubén & Jiménez, Ricardo & Pontet, Matthieu & Martinez, Pedro & Vega,\nVictor. [2017]. Breaking Barriers and Paradigms in Presalt\n\nKattah, S., Balabekov, Y., [2014]. '),(62,5,12,'\n\nHydrogen Sulfide. (2022). Retrieved from Occupational Safety and Health Administration:\nhttps://www.osha.gov/hydrogen-sulfide/hazards\n\nCore Lab [2022]. Pre-Salt Reservoirs of the Santos and Campos Basins, Brazil\n\nLuca, Pedro & Matias, Hugo & Carballo, Jose & Sineva, Diana & Pimentel, Gustavo & Tritlla, Jordi &\nCerda, Mateu & Loma, Rubén & Jiménez, Ricardo & Pontet, Matthieu & Martinez, Pedro & Vega,\nVictor. [2017]. Breaking Barriers and Paradigms in Presalt\n\nKattah, S., Balabekov, Y., [2014]. New opportunities evident in the Santos Basin. Hartenergy. 2\nSeptember 2022. https://www.hartenergy.com/ep/exclusives/new-opportunities-evident-santos-basin-\n174870\n\nEAGE Conference on Digital Innovation for a Sustainable Future\n\n'),(63,6,1,'\nVIENNA | AUSTRIA\n\nCO, emissions the elephant in the room: a pathway of reduction using digitalization and\nunstructured data\n\nIntroduction\n\nIn this paper, we are exploring the challenges associated to climate change in the energy industry with\nthe paradigm of extracting oil and gas in a low CO, environment to limit the effect of climate change\nand provide the world with affordable source of energy for mobility and heat generation.\n\nWe will be discussing how carbon accounting allows to track direct and indirect source of emissions,\nits origins and the challenges associated to them.\n\nFinally, we will investigate how modern technology such as data mining can help mitigate direct and\nindirect emissions by increasing operations efficiency, identifying operation flaws, and implementing\nscalable Carbon Capture and Storage (CCS) implementation.\n\nCO) emission and world energy consumption\n\nRecently, the Intergovernmental Panel on Climate Change (IPCC) issued the sixth Assessment Report\n(AR6) related to Climate Change 2022: Impacts, Adaptation and Vulnerability which highlights\nthe urgency of limiting the increase of temperature to 1.5°C to reduce the impact of climate change:\n“Global warming, reaching 1.5°C in the near-term, would cause unavoidable increases in multiple\nclimate hazards and present multiple risks to ecosystems and humans (very high confidence). [...] Near-\nterm actions that limit global warming to close to 1.5°C would substantially reduce projected losses\nand damages re'),(64,6,2,'ated to Climate Change 2022: Impacts, Adaptation and Vulnerability which highlights\nthe urgency of limiting the increase of temperature to 1.5°C to reduce the impact of climate change:\n“Global warming, reaching 1.5°C in the near-term, would cause unavoidable increases in multiple\nclimate hazards and present multiple risks to ecosystems and humans (very high confidence). [...] Near-\nterm actions that limit global warming to close to 1.5°C would substantially reduce projected losses\nand damages related to climate change in human systems and ecosystems, compared to higher warming\nlevels, but cannot eliminate them all (very high confidence).” (IPCC, 2022).\n\nIn addition, the AR6 report linked to Climate Change 2022: Mitigation of Climate Change suggests\nthat “All global modelled pathways that limit warming to 1.5°C [...] involve rapid and deep and in\nmost cases immediate GHG emission reductions in all sectors. Modelled mitigation strategies to\nachieve these reductions include transitioning from fossil fuels without CCS to very low- or zero-carbon\nenergy sources, such as renewables or fossil fuels with CCS, demand side measures and improving\nefficiency, reducing non-CQ emissions” (IPCC, 2022).\n\nGlobal primary energy consumption by source\nPrimary energy is calculated based on the ‘substitution method’ which takes account of the inefficiencies in fossil\n\nfuel production by converting non-fossil energy into the energy inputs required if they had the same conversion\n\nlosses as fossil f'),(65,6,3,'from fossil fuels without CCS to very low- or zero-carbon\nenergy sources, such as renewables or fossil fuels with CCS, demand side measures and improving\nefficiency, reducing non-CQ emissions” (IPCC, 2022).\n\nGlobal primary energy consumption by source\nPrimary energy is calculated based on the ‘substitution method’ which takes account of the inefficiencies in fossil\n\nfuel production by converting non-fossil energy into the energy inputs required if they had the same conversion\n\nlosses as fossil fuels.\n\nOther\n\nenewables\n160,000 TWh fodern biofuels\n\n140,000 TWh 7 Hydropower\n— Nuclear\n7 Gas\n120,000 TWh\n\n100,000 TWh\n80,000 TWh\n60,000 TWh\n40,000 TWh\n\n20,000 TWh\n\n__ Traditional\n\nOTWh biomass\n1800 1850 1900 2019\n\n‘Source: Vaclav Smil (2017) & BP Statistical Review of World Energy OurWorldinData.org/energy «CC BY\n\n\n\nVIENNA | AUSTRIA\n\nFigure 1: Global primary energy consumption by source. Fossil energies account for 80% respectively\nCoal (25%), Oil (32%) and Gas (23%) (Source: Our World in Data - Energy)\n\nWhile limiting the reduction of CO2 to reduce the impact of climate change, the world is still highly\ndependent on fossil fuels, with fossil energies accounting in 2019 for more than 80% in total and Oil/Gas\nfor 60% alone (Figure 1). Combined with the need of powering the world and the challenges of climate\nchange, the energy sector has a central play to significantly monitor and reduce its CO2 footprint.\n\nCarbon accounting\n\nCarbon accounting is the process which allows organization t'),(66,6,4,'- Energy)\n\nWhile limiting the reduction of CO2 to reduce the impact of climate change, the world is still highly\ndependent on fossil fuels, with fossil energies accounting in 2019 for more than 80% in total and Oil/Gas\nfor 60% alone (Figure 1). Combined with the need of powering the world and the challenges of climate\nchange, the energy sector has a central play to significantly monitor and reduce its CO2 footprint.\n\nCarbon accounting\n\nCarbon accounting is the process which allows organization to quantify and monitor Greenhouse Gas\n(GHG) emissions, By construction, carbon accounting counts the direct and indirect emissions linked\nto the activity of the organization through the full value chain.\n\nDirect emission are the emissions directly related to the own activity of the company such as oil/gas\nextraction and production and are often referred to scope 1 emissions. Indirect emission are the\nemissions emitted considering the full value chain. In the case of the energy industry, this would\nconsider all the necessary services contracted to perform the extraction of oil and gas from the\nsubsurface and all the emissions related to the usage of the oil and gas as a molecule for the human\nusage in mobility, industry usage and heat generation. These emissions are falling into the scope 2\nassociated to the supply of energy (input) and the scope 3 linked to the oil and gas products sold (output).\n\nFigure 2 illustrates the breakdowns and evolution of four (4) representative Oil jor oper'),(67,6,5,'ld\nconsider all the necessary services contracted to perform the extraction of oil and gas from the\nsubsurface and all the emissions related to the usage of the oil and gas as a molecule for the human\nusage in mobility, industry usage and heat generation. These emissions are falling into the scope 2\nassociated to the supply of energy (input) and the scope 3 linked to the oil and gas products sold (output).\n\nFigure 2 illustrates the breakdowns and evolution of four (4) representative Oil jor operators’\nemissions until 2018. In average, 10% of the emissions are associated to direct emission and 90% of\nthem are linked to indirect emissions connected to hydrocarbon products sold. Considering oil and gas\ncommitment in reducing carbon emission, this means that both direct and indirect emissions would need\nto be tackled simultaneously.\n\nOil Majors\' Carbon emissions\n\nWats ee ce peeaes se rsa betas\nOperator A Operator B\nScope |\n\nSeopel\n\nScope\n\nOperator C Operator D\n\nFigure 2: Evolution of direct and indirect emissions for four (4) Oil Majors. scope 1; Own operations\n(direct emissions), scope 2: Power supply (indirect emissions), scope 3: Indirect from oil and gas\nproducts sold (indirect emissions) (Source: Modified from Reuters 2019)\n\nData mining technology\n\n\n\nVIENNA | AUSTRIA\n\nDecades of oil and gas operations associated to the exploration, development and production of oil\nfields have generated vast amount of data which provides a deep insight of constant optimization of\ncosts and r'),(68,6,6,'t and indirect emissions for four (4) Oil Majors. scope 1; Own operations\n(direct emissions), scope 2: Power supply (indirect emissions), scope 3: Indirect from oil and gas\nproducts sold (indirect emissions) (Source: Modified from Reuters 2019)\n\nData mining technology\n\n\n\nVIENNA | AUSTRIA\n\nDecades of oil and gas operations associated to the exploration, development and production of oil\nfields have generated vast amount of data which provides a deep insight of constant optimization of\ncosts and resources. The data are interpreted and compiled into unstructured data such as reports,\npresentations and studies providing a carbon copy of the history of the operations. The unstructured\ndata provides an immense potential of CO2 emissions reduction for both direct and indirect emissions.\nNew technology such as Data Mining and Machine Learning technology helps to process and retrieve\ninformation at scale contained in the unstructured data in a pipeline called ingestion of unstructured\ndata (Hernandez, 2019). The process involves automated pipeline of text identification, image\nclassification, Name Entity Recognition (NER), Knowledge Graph and Heat Map analysis.\n\nFor the direct emissions, the ingestion of unstructured data gives the G&G experts a discovery\nexperience allowing him to interrogate the full corpus of data instantly. Such ingestion platform reduces\nthe time of information retrieval and provides a holistic view of the lateral extent for the parameters of\ninterest. An example'),(69,6,7,'nandez, 2019). The process involves automated pipeline of text identification, image\nclassification, Name Entity Recognition (NER), Knowledge Graph and Heat Map analysis.\n\nFor the direct emissions, the ingestion of unstructured data gives the G&G experts a discovery\nexperience allowing him to interrogate the full corpus of data instantly. Such ingestion platform reduces\nthe time of information retrieval and provides a holistic view of the lateral extent for the parameters of\ninterest. An example of such application is seen in Figure 3, where reservoir intervals with high CO2\ncontent is highlighted for more than 500 wells associated to the ingestion of over 45,000 unstructured\ndocuments.\n\nResults Density\n\nLow mug High\n— Main deep faut mes fgnoous rocks (Ranta, 2020)\n> Taatane Gough fa (Matos, 2027) Crustal Domains (Zaién etal, 2018) MD Extrusions (Costa Comela, 2018}\n Post-sat wel Ml Hyper Extended Crust (Wb btusions (Costa Comeia, 2019)\n© Pre-tak wal 1 Btrached Tinned IB Iproous cock (Vier do Luc of a, 2017]\nDepocenter 7 Resistate\n\nFigure 3: Regional maps covering 500 wells showing the presence of high CO: content in the reservoir\ninterval (Source: https://doi.org/10.1016/) jsames.2022, 103760)\n\nReducing the time of information retrieval provides a unique opportunity of fast-tracking studies, hence\nreduce the CO2 emissions. An extended scope of the data ingestion workflow is the identifying and\ntracking the origin of operations flaws and best practices for potential improveme'),(70,6,8,'Vier do Luc of a, 2017]\nDepocenter 7 Resistate\n\nFigure 3: Regional maps covering 500 wells showing the presence of high CO: content in the reservoir\ninterval (Source: https://doi.org/10.1016/) jsames.2022, 103760)\n\nReducing the time of information retrieval provides a unique opportunity of fast-tracking studies, hence\nreduce the CO2 emissions. An extended scope of the data ingestion workflow is the identifying and\ntracking the origin of operations flaws and best practices for potential improvement and reduction of\nCO» emissions (Hernandez, 2021).\n\nThe reduction of indirect emissions involves large scale adoption and deployment of CCS capabilities\nworldwide. Such a pathway is possible by leveraging on current existing oil and gas assets which are\nthe mature depleting fields. Extensive work and studies over decades generated a vast amount of data\nsubsurface studies and production that are mined to rank the best opportunities for CCS based on the\nfield maturation, intervention history and geological settings. Once the target reservoir identified CO2\ninjection can then be monitored wit novel environmentally friendly methods such as using\nnondisruptive 4D seismic (Figure 4) to prove the potential. Such Data to Sink funnel provides the\ntechnical scalability and the cost effectiveness to assess CCS potential in a large area of interest.\n\n\n\nVIENNA | AUSTRIA\n\nData mining of historical\nunstructured data from 100% data sweep of\ndepleting mature oil and mature fields\n\ngas fields -\na\n\nDat'),(71,6,9,' geological settings. Once the target reservoir identified CO2\ninjection can then be monitored wit novel environmentally friendly methods such as using\nnondisruptive 4D seismic (Figure 4) to prove the potential. Such Data to Sink funnel provides the\ntechnical scalability and the cost effectiveness to assess CCS potential in a large area of interest.\n\n\n\nVIENNA | AUSTRIA\n\nData mining of historical\nunstructured data from 100% data sweep of\ndepleting mature oil and mature fields\n\ngas fields -\na\n\nData consolidation\nof existing G&G\nstructured data\n\n20% opportunities\nidentified\n\nCO2 injection\nand\nmonitoring\n\nCCS operation\n\nFigure 4: Data to Sink funnel workflow powered by data mining, machine learning and lean 4D seismic\nmonitoring.\n\nConclusions\n\nThe reduction of carbon emission at scale in the energy industry remains a challenge. In this paper we\nhave demonstrated how digitalization applied on unstructured data can help reduce both direct and\nindirect emissions.\n\nAcknowledgements\n\nWe would like to thank Iraya Energies for allowing us to publish this paper.\n\nReferences\n\nIPCC, 2022: Summary for Policymakers. In: Climate Change 2022: Mitigation of Climate Change.\nContribution of Working Group III to the Sixth Assessment Report of the Intergovernmental Panel on\nClimate Change [P.R. Shukla, J. Skea, R. Slade, A. Al Khourdajie, R. van Diemen, D. McCollum, M.\nPathak, S. Some, P. Vyas, R. Fradera, M. Belkacemi, A. Hasija, G. Lisboa, S. Luz, J. Malley, (eds.)].\nCambridge University Press, C'),(72,6,10,'ike to thank Iraya Energies for allowing us to publish this paper.\n\nReferences\n\nIPCC, 2022: Summary for Policymakers. In: Climate Change 2022: Mitigation of Climate Change.\nContribution of Working Group III to the Sixth Assessment Report of the Intergovernmental Panel on\nClimate Change [P.R. Shukla, J. Skea, R. Slade, A. Al Khourdajie, R. van Diemen, D. McCollum, M.\nPathak, S. Some, P. Vyas, R. Fradera, M. Belkacemi, A. Hasija, G. Lisboa, S. Luz, J. Malley, (eds.)].\nCambridge University Press, Cambridge, UK and New York, NY, USA. doi:\n10.1017/9781009157926.001\n\nIPCC, 2022: Summary for Policymakers [H.-O. Pértner, D.C. Roberts, E.S. Poloczanska, K.\nMintenbeck, M. Tignor, A. Alegria, M. Craig, 8. Langsdorf, S. Léschke, V. Méller, A. Okem (eds.)].\nIn: Climate Change 2022: Impacts, Adaptation, and Vulnerability. Contribution of Working Group II\nto the Sixth Assessment Report of the Intergovernmental Panel on Climate Change [H.-O. Pértner, D.C.\nRoberts, M. Tignor, E.S. Poloczanska, K. Mintenbeck, A. Alegria, M. Craig, S. Langsdorf, S. Léschke,\nV. Miller, A. Okem, B. Rama (eds.)]. Cambridge University Press. In Press.\n\nHernandez, N. M., Lucaiias, P. J., Mamador, C., & Panganiban, L. [2019]. Automated Information\nRetrieval from Unstructured Documents Utilizing a Sequence of Smart Machine Learning Methods\nwithin a Hybrid Cloud Container. EAGE Workshop on Big Data and Machine Learning for E&P\nEfficiency 25-27 February\n\nHernandez, N.M. and Maver, K.M. [2021]. ED2K Initiative launched t'),(73,6,11,'nbeck, A. Alegria, M. Craig, S. Langsdorf, S. Léschke,\nV. Miller, A. Okem, B. Rama (eds.)]. Cambridge University Press. In Press.\n\nHernandez, N. M., Lucaiias, P. J., Mamador, C., & Panganiban, L. [2019]. Automated Information\nRetrieval from Unstructured Documents Utilizing a Sequence of Smart Machine Learning Methods\nwithin a Hybrid Cloud Container. EAGE Workshop on Big Data and Machine Learning for E&P\nEfficiency 25-27 February\n\nHernandez, N.M. and Maver, K.M. [2021]. ED2K Initiative launched to support UN 2050 Net Zero\ngoals by reading the earth better, First Break, June 2021\n\n\n'),(74,7,1,'\nVIENNA AUSTRIA\n\nDouble funnel approach for screening of potential CO2 storage opportunities in the Norwegian\nContinental Shelf\n\nIntroduction\n\nCarbon capture and storage (CCS) is a key waste management strategy for reducing carbon dioxide (CO2)\nemissions and mitigating climate change. The Norwegian continental shelf has significant capacity for CCS,\nas it has several depleted oil and gas fields that can be used for storage of CO2. The field of CCS has seen\nsignificant growth in recent years, as the need to reduce carbon CO2 emissions becomes increasingly\nurgent. However, despite the increasing number of studies on CCS, there remains a lack of consensus on\nthe most effective methods for accelerating and scaling up CCS projects.\n\nIn this study, the integration of Machine Learning (ML) whereby the reports from the Norwegian Petroleum\nDirectorate (NPD) are ingested into one platform creates potential cost-effective solution by screening\nprevious knowledge gathered for depleting oil and gas fields and significantly reduces the time of the\nscreening, the evaluation and the ranking of CCS prospects. We investigate the feasibility of such a study\non the Norwegian Continental Shelf by analyzing the geology and capacity of existing oil and gas fields.\nThe analysis is conducted on historical data from final well reports for 361 wells (NPD, 2023) which are\nptiorly ingested using Machine Learning (ML) and Artificial Intelligence (AI) by indexing and tagging\nmetadata from the documents, ex'),(75,7,2,' fields and significantly reduces the time of the\nscreening, the evaluation and the ranking of CCS prospects. We investigate the feasibility of such a study\non the Norwegian Continental Shelf by analyzing the geology and capacity of existing oil and gas fields.\nThe analysis is conducted on historical data from final well reports for 361 wells (NPD, 2023) which are\nptiorly ingested using Machine Learning (ML) and Artificial Intelligence (AI) by indexing and tagging\nmetadata from the documents, extracting, and classifying images and generating geological interpretable\noutput such as heat maps or knowledge graphs. Our research includes a detailed characterization and\ninterpretation of the subsurface geology, including the identification of potential storage formations, the\nanalysis of reservoir properties such as porosity and permeability and the evaluation of seal characteristics.\nWe also conducted a comprehensive assessment of the capacity for CO2 storage, considering factors such\nas injection rate and pressure buildup.\n\nMethodology\n\nDepleting oil and gas fields in the Norwegian Continental Shelf with their massive amount of data being\ncollected over decades of development and production are often considered good candidates for CCS\nopportunities. Unfortunately the vast amount of knowledge come with the challenges associated to the lack\nof normalization of the data and the diversity of the different format and template utilized making it difficult\nto utilize the full potential '),(76,7,3,'jection rate and pressure buildup.\n\nMethodology\n\nDepleting oil and gas fields in the Norwegian Continental Shelf with their massive amount of data being\ncollected over decades of development and production are often considered good candidates for CCS\nopportunities. Unfortunately the vast amount of knowledge come with the challenges associated to the lack\nof normalization of the data and the diversity of the different format and template utilized making it difficult\nto utilize the full potential of such data without allocating significant manual work.\n\nIn our case study, Machine learning pipelines are used to classify, cluster, and extract insights from such\nan unstructured data. Priorly trained and G&G domain specific natural language processing (NLP)\ntransformers are executed on the text to perform indexing, metadata tagging and topic modeling, when\nDeep Convolutional Neural Network (DCNN) extract, classify and segment extracted images. Such an\napproach has the advantage of significantly lessening manual human intervention allowing G&G experts\nto focus on the interpretation of the data itself using a front end deployed interface (Baillard et al., 2019).\nAs seen in Figure 1 the data visualization and interpretation are performed through a suite of six analytical\ntools: (1) summarizes the important attributes of the well automatically extracted from the document, (2)\naids in portraying the well data on a map and visualizes the lateral distribution of search queries, (3)\nprovid'),(77,7,4,'tly lessening manual human intervention allowing G&G experts\nto focus on the interpretation of the data itself using a front end deployed interface (Baillard et al., 2019).\nAs seen in Figure 1 the data visualization and interpretation are performed through a suite of six analytical\ntools: (1) summarizes the important attributes of the well automatically extracted from the document, (2)\naids in portraying the well data on a map and visualizes the lateral distribution of search queries, (3)\nprovides an in-depth search within all the corpus for the text and any tagged associated metadata using\nNLP, (4) correlates wells between each other’s to understand and interpret the semantic structure of the\nbasin(5) searches the images extracted from DCNN into its respected geological categories, (6) quantifies\nthe frequency of different lithologies present from the different wells.\n\n\n\nVIENNA AUSTRIA\n\nImage search module 4\nDeep-search module Correlate findings with oe Lithotogy tab\n6 Oe, nevevnape tase i\nse\nz\n\nFigure 1 Analytical tools used for the case study research strategy for CO2 storage screening\n\nSuch a set of tools provides powerful means for understanding and interpreting large and complex sets of\ndata. It can help to identify patterns, trends, and relationships that might not be immediately apparent from\nraw data due to the segregation of information in separate files for each well. By narrowing down the scope\nof focus on selected wells, the exclusion of non-relevant well and tim'),(78,7,5,'ase i\nse\nz\n\nFigure 1 Analytical tools used for the case study research strategy for CO2 storage screening\n\nSuch a set of tools provides powerful means for understanding and interpreting large and complex sets of\ndata. It can help to identify patterns, trends, and relationships that might not be immediately apparent from\nraw data due to the segregation of information in separate files for each well. By narrowing down the scope\nof focus on selected wells, the exclusion of non-relevant well and time frame reduction of the process can\nbe accomplished.\n\nIn this paper, we propose a new CCS screening workflow called Double Funnel Approach (DFA), seen on\nFigure 2 which consists of a “data sweep” and a “data target”, The “data sweep” aims to reduce all findings\nfrom all ingested data to key learnings and key wells over the area of interest, allowing to review and rank\nthe most suitable field candidates for potential CCS opportunities. The “data target” follows the “data\nsweep” and focuses only on the field selected candidates and aims to refine and enhance the existing\nunstructured data with seismic, logs, interpretation and geomodel data, During this exercise, redundant and\nirrelevant data are removed through efficient automated version indexing and cross-correlation with the\nunstructured data. Finally, the data is now ready for screening for CO2 injection capacity and monitoring\nanalysis.\nUnstructured Data Interpretation\n\nKey learnings\nKey wells.\n\n“Data Sweep” Key risks Target field'),(79,7,6,'on the field selected candidates and aims to refine and enhance the existing\nunstructured data with seismic, logs, interpretation and geomodel data, During this exercise, redundant and\nirrelevant data are removed through efficient automated version indexing and cross-correlation with the\nunstructured data. Finally, the data is now ready for screening for CO2 injection capacity and monitoring\nanalysis.\nUnstructured Data Interpretation\n\nKey learnings\nKey wells.\n\n“Data Sweep” Key risks Target field for CO2\nKey drivers\n\nBasin Mapping | area storage\n\n“Data Target”\n\nPlay Evaluation\n\nReservoir Properties |\n\nMonitor Injection in\n\nSeal Properties Depleting fields review\n\nand ranking\n\nIdentify learning over Propagate learning\nsedimentary basins over key depleting\nfields\n\nFigure 2 Proposed Double Funnel Approach for CCS Screening Studies\nCCS “data sweep” use case offshore Norway\n\nThe ingestion of data for the case study comprises of 490,000 pages and 440,000 images, covering a total\nof 361 wells within 5 basins in Norway consolidating 50 years of exploration, development, and production.\nAll these data has been retrieved from the Norwegian Petroleum Directorate (NPD). The “data sweep” of\nthe data was completed in 21 days which evaluated various hypothesis and converge on the key learnings,\nkey wells, key risks, and key drivers.\n\n\n\nVIENNA AUSTRIA\n\nFigure 3 shows the generated knowledge graph associated to the zone of interest. Knowledge graph is a\nstructured way to represent and organize'),(80,7,7,'within 5 basins in Norway consolidating 50 years of exploration, development, and production.\nAll these data has been retrieved from the Norwegian Petroleum Directorate (NPD). The “data sweep” of\nthe data was completed in 21 days which evaluated various hypothesis and converge on the key learnings,\nkey wells, key risks, and key drivers.\n\n\n\nVIENNA AUSTRIA\n\nFigure 3 shows the generated knowledge graph associated to the zone of interest. Knowledge graph is a\nstructured way to represent and organize knowledge in a way that is easily queried and traversed across all\nthe corpus of documents ingested. This makes it useful for a holistic interpretation of the wells present in\nthe area of interest, interpreting and ranking them based on their location and importance in the graph\nrespectively as “alpha” or geological analogue, “scouts”, “pack or “lone-spirit” wells. As observed, the\nstructure of the knowledge graph does indicate a non-homogeneous distribution with 7 different clusters\nbeing identified. Each cluster is centered around key wells acting as key geological analogues (“alpha”\nwell) for the surrounded wells located within the cluster. “Scouts” wells define the unique critical paths\nbetween adjacent clusters, allowing geologists to deeper understand the geology and exploration history of\nthe area of interest (Hernandez et al., 2019).\n\nBEM ue\n\nFigure 3 Knowledge graph with clusters of wells from the Norwegian dataset\n\nBased on the recognized clusters, wells are further investig'),(81,7,8,'tified. Each cluster is centered around key wells acting as key geological analogues (“alpha”\nwell) for the surrounded wells located within the cluster. “Scouts” wells define the unique critical paths\nbetween adjacent clusters, allowing geologists to deeper understand the geology and exploration history of\nthe area of interest (Hernandez et al., 2019).\n\nBEM ue\n\nFigure 3 Knowledge graph with clusters of wells from the Norwegian dataset\n\nBased on the recognized clusters, wells are further investigated by cross-correlating their respective post\ndrill conclusion, formation penetration and keywords search associated to reservoir properties, seal\ncharacteristics or a specific search allowing a deeper dive in the corpus. An example of such full corpus\nsearch for ‘porosity’ detected from the well final well reports, within text, images and tables identifying the\nrelevant values of the porosity and their associated formations. Auto-classified images can enhance the\nanalysis by providing detailed information about the textures, layers, and structural characteristics of the\nrocks through different scales, from field scale with seismic stacks or isochrone map, to microscopic scale\nwith thin section images. Additionally, image analysis techniques such as pattern recognition can be used\nto automatically extract features and classify rock formations.\n\nIn this example, the “data sweep” suggests suitable areas for CCS in the Norwegian Sea corresponding to\nHeidrun and Marulk fields. The study '),(82,7,9,'d information about the textures, layers, and structural characteristics of the\nrocks through different scales, from field scale with seismic stacks or isochrone map, to microscopic scale\nwith thin section images. Additionally, image analysis techniques such as pattern recognition can be used\nto automatically extract features and classify rock formations.\n\nIn this example, the “data sweep” suggests suitable areas for CCS in the Norwegian Sea corresponding to\nHeidrun and Marulk fields. The study highlights the potential of Ile and Garn formation within the Fangst\nGroup under the Heidrun Field. These intervals show good average depths for COQ2 storage for supercritical\nstorage, and are characterized by good porosity and permeability, with a significant net sand thickness. Seal\nintegrity has been confirmed and validated. The interval above Ile and Garn are currently producing, and\ntherefore has seismic and velocity data which allows precise CO2 injection monitoring through\nmicroseismic. The upper Ile and Garn aquifers have good reservoirs in the southern part of the Froan Basin\nwhich may indicate additional potential CCS storage in this area.\n\n\n\nVIENNA AUSTRIA\n\n6507/ ean 7 a Tomm Moderate Claystone Sandstone\n(ile Fm)\n\nFigure 4 Suneonine CO2 storage candidates based on lithology, average porosity, average permeability,\nand seal characteristics.\n\nConclusion\n\nThe study showcases how a “Double Funnel Approach” through an ML data ingestion pipeline can be an\nefficient screening tool '),(83,7,10,'he upper Ile and Garn aquifers have good reservoirs in the southern part of the Froan Basin\nwhich may indicate additional potential CCS storage in this area.\n\n\n\nVIENNA AUSTRIA\n\n6507/ ean 7 a Tomm Moderate Claystone Sandstone\n(ile Fm)\n\nFigure 4 Suneonine CO2 storage candidates based on lithology, average porosity, average permeability,\nand seal characteristics.\n\nConclusion\n\nThe study showcases how a “Double Funnel Approach” through an ML data ingestion pipeline can be an\nefficient screening tool to analyze, review and rank CCS potential using readily available unstructured data.\nIn this case, 490,000 pages of documents have been analyzed in 21 days to identify potential CCS\nopportunities below Heidrun producing field, extended across the Froan basin. Additional analysis through\nthe “data target” may now be undertaken around Heidrun field on related wells, seismic and interpretation\ndata.\n\nTo conclude, such an analysis suggests the scalability and the cost effectiveness of the methodology for\nrapidly addressing the requirements of new CCS capabilities to mitigate the impact of the Climate Change.\n\nAcknowledgment\n\nThis paper utilizes the data from the Norwegian Petroleum Directorate (NPD) open dataset. Disclaimer of\nthose interpretations from the study are from investigation and analysis of the authors alone.\n\nReferences\n\nBaillard, F., & Hernandez, N. (2021). A Case Study of Understanding the Bonaparte Basin using\nUnstructured Data Analysis with Machine Learning Techniques. EAGE'),(84,7,11,'y for\nrapidly addressing the requirements of new CCS capabilities to mitigate the impact of the Climate Change.\n\nAcknowledgment\n\nThis paper utilizes the data from the Norwegian Petroleum Directorate (NPD) open dataset. Disclaimer of\nthose interpretations from the study are from investigation and analysis of the authors alone.\n\nReferences\n\nBaillard, F., & Hernandez, N. (2021). A Case Study of Understanding the Bonaparte Basin using\nUnstructured Data Analysis with Machine Learning Techniques. EAGE Annual.\n\nHernandez, M., & Baillard, F, (2019). An effective G&G exploration strategy inspired by a wolfpack.\nFORCE Workshop.\n\nNorwegian Petroleum Directorate. (n.d.). 5 - The Norwegian Sea. Retrieved at January 20, 2023 from\nhttps://www.npd.no/en/facts/publications/co2-atlases/co2-atlas-for-the-norwegian-continental-shelf/5-the-\n\nnorwegian-sea/\n\nNorwegian Petroleum Directorate. (n.d.). 4 - The Norwegian North Sea. Retrieved at January 20, 2023\nfrom https://www.npd.no/en/facts/publications/co2-atlases/co2-atlas-for-the-norwegian-continental-\nshelf/4-the-norwegian-north-sea/\n\n\n'),(85,8,1,'\nVIENNA | AUSTRIA\n\nSand Production and Control Benchmarking through Unstructured Data Analysis with Machine\nLearning in the North Sea\n\nIntroduction\n\nSand production has been serving as a bottleneck to the oil and gas industry, contributing to disruption\nof daily production operations, casing deformation, erosion of well tubing, pipelines, and surface\nequipment, expediting to significant non-productive time (NPT) costing millions of dollars in loss\nannually. Conventional areal studies for sand production will only be limited to few wells and heavily\ndependent on data availability, human-based interpretation, and time constraints. To administer a\nholistic basin study for sand production comprising of hundreds of wells with conventional manual\nmethod is considered complex and time consuming, hence sand mitigation best practices are typically\nderived from localized reservoir and production engineering data only, and knowledge is organically\nbuilt through accumulated expert experience in the area over multiple years of operatorship. Information\nand reports may be derived from millions of pages of legacy well reports, documentations, or files from\nover 40 years ranging from digitized medium to hand-written reports in countless formats.\n\nA sustainable strategy of data-driven basis shall be leveraged to address the knowledge and information\nmanagement issues utilizing the latest advancement of Machine Learning (ML) and data analytics to\nmaximize the potential of unstructured data. Ut'),(86,8,2,'e area over multiple years of operatorship. Information\nand reports may be derived from millions of pages of legacy well reports, documentations, or files from\nover 40 years ranging from digitized medium to hand-written reports in countless formats.\n\nA sustainable strategy of data-driven basis shall be leveraged to address the knowledge and information\nmanagement issues utilizing the latest advancement of Machine Learning (ML) and data analytics to\nmaximize the potential of unstructured data. Utilizing the intuitive data-driven approach, the paper will\nhighlight the areal causation of sand production based on geological characteristics and the best\npractices of sand control commenced by 8 operators in Norwegian Basins practically informative for\nfuture exploration wells to be developed nearby current wells. The study first creates a relationship\nbetween the causation of sand production versus the sand control practices implied and best practices\nare derived from the practices of multi-wells.\n\nMethodology\n\nUnstructured data in nature is significantly sophisticated to be manually interpreted and skimmed\nthrough by human-intervention. An intuitive approach embedded with Deep Convolutional Neural\nNetwork (DCNN) for autonomous image recognition and Natural Language Processing (NLP) for texts\nand entity processing and recognition has been a pioneering enterprise-scaled platform in managing\nunstructured data (Hernandez et al., 2019). It will be capable to ingest “big data” for the c'),(87,8,3,'wells.\n\nMethodology\n\nUnstructured data in nature is significantly sophisticated to be manually interpreted and skimmed\nthrough by human-intervention. An intuitive approach embedded with Deep Convolutional Neural\nNetwork (DCNN) for autonomous image recognition and Natural Language Processing (NLP) for texts\nand entity processing and recognition has been a pioneering enterprise-scaled platform in managing\nunstructured data (Hernandez et al., 2019). It will be capable to ingest “big data” for the case study\ncomprising of 70,000 files with 490,000 pages and 430,000 images inclusive of 361 wells over 5 basins\nin Norway. 40 years of unstructured data for sand production case study is consolidated approximately\nwithin 16 days of study period.\n\nWell Summary Tab Word Cloud Detaled search Correlate findings with\nKnowledge Graph\n\nplots /images/maps/stratigraphic\n\nOo Generic Search Heat Map @& Lithology tab Image search module\nplots\n\nFigure 1 Data-driven case study research strategy for sand production in Norwegian Basins.\n\n\n\nVIENNA | AUSTRIA\n\nStep (1) is a generic deep search of the sand production scope across the whole corpus. Step (2) leads\nto discovering all wells significant parameters based on the files search results within well summary\ntab. Step (3) portrays an early insight of the general idea of the document with word cloud. Step (4) heat\nmap resembles the wells distribution in a GIS map based on colour density of corpus search frequency\nmentioned in the documents. To uncover '),(88,8,4,'ction in Norwegian Basins.\n\n\n\nVIENNA | AUSTRIA\n\nStep (1) is a generic deep search of the sand production scope across the whole corpus. Step (2) leads\nto discovering all wells significant parameters based on the files search results within well summary\ntab. Step (3) portrays an early insight of the general idea of the document with word cloud. Step (4) heat\nmap resembles the wells distribution in a GIS map based on colour density of corpus search frequency\nmentioned in the documents. To uncover more questions along the research process and to obtain more\nin-depth information, step (5) is conducted in an iterative manner. Step (6) is to relate the lithology\ndistribution by lithology count within each well document to the previous detailed search parameters.\nStep (7) aims to find more representable images to support the case study through the automatically\nclassified images through DCNN in the reports. After all significant parameters are obtained, well-to-\nwell relationship is studied to get more details on further causation and best practices of sand production\nmanagement.\n\nThe features of this intuitive knowledge management platform transform voluminous unstructured data\ninto structured data that are ready to be consumed and utilised for production enhancement case study.\nFour main ML analytical tools embedded in the platform are as presented below (Baillard et al.,2021):\n\ne Expeditious and intelligent search module by keyword-basis searching through hundreds of\nthousands of'),(89,8,5,'o get more details on further causation and best practices of sand production\nmanagement.\n\nThe features of this intuitive knowledge management platform transform voluminous unstructured data\ninto structured data that are ready to be consumed and utilised for production enhancement case study.\nFour main ML analytical tools embedded in the platform are as presented below (Baillard et al.,2021):\n\ne Expeditious and intelligent search module by keyword-basis searching through hundreds of\nthousands of pages of texts and texts embedded inside images.\n\ne Autonomous extraction of images from documents and image segregation into respective image\nclasses of tables, figures, well plots and maps with DCNN image detection algorithm.\nKnowledge graph with contextual well name relationship portraying connectivity of ‘related\ncorpuses’ to understand well-to-well relationship as described in their respective document corpus.\nHeat Map illustrates the density of keywords by colour gradient on wells based on the search results\non a map. Polygonal or square filter feature enable selective wells to be screened out for users\nnarrowed search interest.\n\nCase Study: Preliminary Study on Sand Production and Developing Sand Control Benchmark of\nNorwegian Basin with Unstructured Data\n\nThe study was conducted extensively throughout approximately 361 wells consuming 16 days of study\nperiod covering the analysis and interpretation of sand production trends, causation and best practices\nin Norwegian Basins. A '),(90,8,6,'n the search results\non a map. Polygonal or square filter feature enable selective wells to be screened out for users\nnarrowed search interest.\n\nCase Study: Preliminary Study on Sand Production and Developing Sand Control Benchmark of\nNorwegian Basin with Unstructured Data\n\nThe study was conducted extensively throughout approximately 361 wells consuming 16 days of study\nperiod covering the analysis and interpretation of sand production trends, causation and best practices\nin Norwegian Basins. A total of 8 operators were participating in exploration phase of reservoirs\nespecially in Voring and Northern North Sea basins.\n\nVoring Basin Wells\n\nFigure 2 Reservoir Performance Tests (RFT) for Voring Basin Wells\n\nSand production was reported in 7 wells and most discoveries of sanding were reported during Drill-\nstem Testing (DST) and reported in completion reports and drilling program reports as in Figure 2. A\n\n\n\nVIENNA | AUSTRIA\n\nfew wells were reported to experience little to no sanding issues however pro\ninformation in the scope of best practices and recommendations.\n\nUpper part - clean sandstone, fina grained, wall sorted 6608/10-2\nand friable\n\nLower part —-medium to coarse grained, moderately\n\nSorted, friside to tose grains.\n\nModerately sorted, fine to very fine grain size, 6507/84\npredominantly friable except the tight zones with\nhard cementation.\n\nRaES ‘Sofft tertiasy clay stones wih sil and sand layers 640719-2\n‘Sandstones, friable to very friable, moderately\n\nssoited, subrou'),(91,8,7,'es however pro\ninformation in the scope of best practices and recommendations.\n\nUpper part - clean sandstone, fina grained, wall sorted 6608/10-2\nand friable\n\nLower part —-medium to coarse grained, moderately\n\nSorted, friside to tose grains.\n\nModerately sorted, fine to very fine grain size, 6507/84\npredominantly friable except the tight zones with\nhard cementation.\n\nRaES ‘Sofft tertiasy clay stones wih sil and sand layers 640719-2\n‘Sandstones, friable to very friable, moderately\n\nssoited, subrounded to rounded\n\n“Traces of sands (+-1% at 5000 STB/tay rate) | meee |\n\nGeologic Time Scale\n\nFigure 3 Chronostratigraphic evaluation of Sand Prone Wells\n\nReferring to Figure 3, chronostratigraphic reports denote sufficient reasoning of sand production\noccurrence throughout Norwegian basins as most sanding issues are prone in younger formations or\ntertiary aged formations from Upper Jurassic, Late Paleocene, Danian, Early Toracian, Sinemurian and\nUpper Toracian. Cuttings and core samples obtained from different stratigraphy depths of sanding prone\nformations; Tofte, Aldra, Froya and Top Heimdal were mostly described with friable, loosely grained,\ntraces of sands, soft tertiary claystone, little to no cementation and fine-grained characteristics\ndominantly originating from sandstone, carbonate and claystone-sandstone mix lithology. Marginal\nmarine and deltaic depositional environments leads to less cementation and loosely grained deposited\n\nsand characteristics.\n\nQuality of MOT wireline '),(92,8,8,' from different stratigraphy depths of sanding prone\nformations; Tofte, Aldra, Froya and Top Heimdal were mostly described with friable, loosely grained,\ntraces of sands, soft tertiary claystone, little to no cementation and fine-grained characteristics\ndominantly originating from sandstone, carbonate and claystone-sandstone mix lithology. Marginal\nmarine and deltaic depositional environments leads to less cementation and loosely grained deposited\n\nsand characteristics.\n\nQuality of MOT wireline samples wes\nquestioned due to plugging of the tool By\n‘sand peoduction\n\n= Sand detection monitor did not indieate\n‘any sand was being produced with the\n{uid stream and verified trough grind\n‘outs which indicates only tracer of BSW.\n\n+ Unconsolidated sands stringers in Utsira\nformation caused an increase of ROP rates\nupto 270 meters per hour without ary\n§W08 as the zone is primarily lose,\nunconsolidated.\n\n+ Prepacked screens were installed across\nfeservoir interval to prevent sand\nproduction fram unconsolidated formation\n\n‘+ Formation stability test - increase rate flow\nSequence after the main buikdup, Sand\nproduction and erosion is monitored by\nSANDEC system - record impact of sand\ngrains on probe\n\n+ Formation stability testis constraint by low\nermeabllty reservoir ~ not wanting to\nflow the well approaching absolute open\nflow\n\nNorthern North Sea Basin Wells\nSand failure test (high-rate test- gradual\ncrease to high rate for 6 hours)\nNo sand production observed\n\n= Wel was flowed for 5000'),(93,8,9,' unconsolidated formation\n\n‘+ Formation stability test - increase rate flow\nSequence after the main buikdup, Sand\nproduction and erosion is monitored by\nSANDEC system - record impact of sand\ngrains on probe\n\n+ Formation stability testis constraint by low\nermeabllty reservoir ~ not wanting to\nflow the well approaching absolute open\nflow\n\nNorthern North Sea Basin Wells\nSand failure test (high-rate test- gradual\ncrease to high rate for 6 hours)\nNo sand production observed\n\n= Wel was flowed for 5000 stb/d for 24\nihours and choked back to 3960 stb/d when\nsand production is above 1% for several\nminutes (4/196 for S000 stb/d) - 141 psi\ndrawdown\n\n+ Monitored with non-lntrusive sand\ndetection sensor as well as manual\n\nsamoling\n+ Ifsandis not produced in signi\n‘quantities, the flow rate will be reduced\n\nprimary sand contro! with downhole wire\nrapped screen run in tec string.\n\nSand production monitored with non-\nintrusive sand detection sensor as well as\nmmanwal sampling\n\nRate should be reduced i sand production\n>2.5% continues more than 15-20 minutes\nBSEW samples should be taken’\n\nat early phase of rate build-up and before\nand ater rate cronges\n\nFigure 4 Heat Map for sand production best practices — Voring Basin\n\nReservoir parameters analy:\n\nis was conducted to observe the sanding occurrence trends with respect to\n\nporosity, permeability, skin and perforation shots for each wells experiencing sand production issue.\nStimulated wells with negative skin value, significant high permeabilit'),(94,8,10,' sand production\n>2.5% continues more than 15-20 minutes\nBSEW samples should be taken’\n\nat early phase of rate build-up and before\nand ater rate cronges\n\nFigure 4 Heat Map for sand production best practices — Voring Basin\n\nReservoir parameters analy:\n\nis was conducted to observe the sanding occurrence trends with respect to\n\nporosity, permeability, skin and perforation shots for each wells experiencing sand production issue.\nStimulated wells with negative skin value, significant high permeability and porosity values are\n\n\n\nVIENNA | AUSTRIA\n\narbitrarily associated to sanding issues. However, a few wells do highlight these characteristics but no\nor little sanding occurred, and assumptions were made that inter-grain cementation are intact or sanding\nprobably will occur soon in the later phases of the reservoir as sand production onset is distinct in each\nwell. Perforation design does not lead primarily towards sand production as comparison has been made\nfor wells with the same perforation shots with significantly different reservoir parameters for\ncomparative analysis. Pore pressure abnormalities attained from Knowledge Graph module possibly\ncauses sand production problems specifically reported in well 25/5-5 and 25/6-3 within Heimdal\nformation interval, leading to depleted pressure gradient trends in both wells. Analysing the causation\nof sanding creates an understanding in relation to the sand control practices conducted for each of the\nwells. Wells describing sand production '),(95,8,11,' shots with significantly different reservoir parameters for\ncomparative analysis. Pore pressure abnormalities attained from Knowledge Graph module possibly\ncauses sand production problems specifically reported in well 25/5-5 and 25/6-3 within Heimdal\nformation interval, leading to depleted pressure gradient trends in both wells. Analysing the causation\nof sanding creates an understanding in relation to the sand control practices conducted for each of the\nwells. Wells describing sand production issues, sand control mitigation methods and other relatable\ndescriptions of sand production were intensively analysed as Figure 4 above.\n\nConclusion\n\nManaging unstructured data into an intuitive structured data with embedded end-to-end ML\nadvanced technology made it possible to interpret, analyze and make decisions with regards to\nhandling “big data” and derive sand production causation and best practices across 490,000\npages of public documents inclusive of 361 wells and 2 Norwegian basins in total. The novel\napproach serves as a holistic study of sand management focused on unstructured “big data”\nwhich combines multiple digitalization techniques currently applied in the petroleum industry.\nMaximizing the potential of underutilized unstructured data leads to opening of vast\nopportunities for enhancement of production in existing oil and gas wells, and reduces\ninvestment in the drilling of newer, more expensive wells, in alignment with a re-use, reduce,\nup-cycle mentality, towards sust'),(96,8,12,'otal. The novel\napproach serves as a holistic study of sand management focused on unstructured “big data”\nwhich combines multiple digitalization techniques currently applied in the petroleum industry.\nMaximizing the potential of underutilized unstructured data leads to opening of vast\nopportunities for enhancement of production in existing oil and gas wells, and reduces\ninvestment in the drilling of newer, more expensive wells, in alignment with a re-use, reduce,\nup-cycle mentality, towards sustainable energy transition for the industry.\n\nAcknowledgment\n\nThis paper utilizes the data from the Norwegian Petroleum Directorate (NPD) open dataset. Disclaimer\nof those interpretations from the study are from investigation and analysis of the authors alone.\n\nReferences\n\nAcock, A. & ORourke, T. & Shirmboh, D. & Alexander, J. & Andersen, G. & Kaneko, T. &\nVenkitaraman, A. & Lopez de Cardenas, Jorge & Nishi, M. & Numasawa, M. & Yoshioka, K. & Roy,\nA. & Wilson, A. & Twynam, Allan. (2004). Practical approaches to sand management. Oilfield\nReview. 16. 10-27.\n\nBaillard, F., & Hernandez, N. (2021). A Case Study of Understand Bonaparte Basin using\nUnstructured Data Analysis with Machine Learning Techniques. EAGE Annual.\n\nHernandez, M., & Baillard, F, (2019). An effective G&G exploration strategy inspired by a wolfpack.\nForce workshop.\n\nHernandez, N., Lucafias, P., Graciosa, J., Mamador, C., & Panganiban, I. (2019). Automated\nInformation Retrieval from Unstructured Documents Utilizing a Sequen'),(97,8,13,'l approaches to sand management. Oilfield\nReview. 16. 10-27.\n\nBaillard, F., & Hernandez, N. (2021). A Case Study of Understand Bonaparte Basin using\nUnstructured Data Analysis with Machine Learning Techniques. EAGE Annual.\n\nHernandez, M., & Baillard, F, (2019). An effective G&G exploration strategy inspired by a wolfpack.\nForce workshop.\n\nHernandez, N., Lucafias, P., Graciosa, J., Mamador, C., & Panganiban, I. (2019). Automated\nInformation Retrieval from Unstructured Documents Utilizing a Sequence of Smart Machine\nLearning. EAGE Workshop on Big Data and Machine Learning for E&P Efficiency 25 - 27 February.\n\nTiab, D. D., Erle C. (2012). Petrophysics - Theory and Practice of Measuring Reservoir Rock and\nFluid Transport Properties (3rd Edition) - 9.36 Porosity as Strength Indicator to Evaluate Sand\nProduction. Elsevier.\n\nKim, 8.H., Sharma, M. M., and Harvey J. F. (2011). A Predictive Model for Sand Production in\nPoorly Consolidated Sands. International Petroleum Technology Conference. Doi:\nhttps://doi.org/10.2523/IPTC-15087-MS.\n\n\n'),(98,9,1,'\nVIENNA | AUSTRIA\n\nUsing Machine Learning-Based Data Factory to Unlock Mining in Australia for Environmental,\nSocial and Corporate Governance (ESG)\n\nIntroduction\n\nThe road to net zero requires a lot of raw materials from the mining industry. Renewable energy systems\nfor solar, hydro, and wind need to be built to support the transition. Among the many metals critical to\ntechnology and infrastructure necessary for new energy, copper is highly sought after thanks to its\nconductive efficiency making it an irreplaceable element of any electrical equipment. Therefore, it is\nprojected that by 2050, the demand for copper will reach more than 53 million metric tons. This is\n“more than all the copper consumed in the world between 1900 and 2021”. Given the above, copper\nprice spikes, and copper supply challenges are to be expected (Bonakdarpour & Bailey, 2022). Hence,\nit is crucial to optimize the way copper is mined in order to meet future demands, accelerate the energy\ntransition and execute the plans of stakeholders to achieve Environmental, Social and Corporate\nGovernance (ESG) targets.\n\nOptimization of copper mining exploration and operations starts with the capability to easily make\ndecisions and gain insights using the organizations’ data. However, this data is often unstructured,\nscattered, and unsearchable. To extract, manage and sustainably utilize all these unstructured data, a\ndigital data factory composed of an orchestration of Machine Learning (ML) pipelines, data tracking'),(99,9,2,'e plans of stakeholders to achieve Environmental, Social and Corporate\nGovernance (ESG) targets.\n\nOptimization of copper mining exploration and operations starts with the capability to easily make\ndecisions and gain insights using the organizations’ data. However, this data is often unstructured,\nscattered, and unsearchable. To extract, manage and sustainably utilize all these unstructured data, a\ndigital data factory composed of an orchestration of Machine Learning (ML) pipelines, data tracking,\nand monitoring services, has been implemented on a subset of data from the Geological Survey of\nQueensland (GSQ) in Australia. Utilizing the ML-based Data factory approach, this paper highlights\nhow mining information from the GSQ can be analyzed, unlocked, and used in optimizing the various\nstages of the copper mining operation such as exploration, mining operation, copper ore processing,\nreclamation and safety, health, and environmental control.\n\nMethodology\n\nUnstructured data from the GSQ contains scientific reports, borehole completion reports, publications,\njournals, mining datasets, map collections, and mining records. The documents are highly technical and\nspread over decades of mining operations making manual human interpretation and data extraction\nchallenging. The dataset covers 62 years of mining operation in Queensland and has been ingested in\nthe data factory at a rate of 3,000 pages and 4,000 images per day through its scalable automated ML\npipeline and big data capabil'),(100,9,3,'ontains scientific reports, borehole completion reports, publications,\njournals, mining datasets, map collections, and mining records. The documents are highly technical and\nspread over decades of mining operations making manual human interpretation and data extraction\nchallenging. The dataset covers 62 years of mining operation in Queensland and has been ingested in\nthe data factory at a rate of 3,000 pages and 4,000 images per day through its scalable automated ML\npipeline and big data capabilities. The steps of the processing include uploading the data to the cloud,\naudit of the data, text/image extraction, image classification, and table export capabilities as seen in\nFigure 1. The features of this ML-based data factory transform voluminous unstructured data into\nstructured data that are readily accessible through a cloud-based application for text, image, and\nknowledge search. The data factory’s features have applications that can be extended to all copper\nmining stages.\n\neb Province Deposit\nHON Qnsene\nittle Evas\nsit\n\nSearch Table OCR Image classification Word Cloud\n\nFigure 1 Transversal corpus search features of the digital data factory.\nSeamless Search Tool\n\nTo have a firm grasp of the copper resource information covering the definitions, inferences,\nindications, and compiled measurements, geologists and mining professionals involved in the\n\n\n\nVIENNA | AUSTRIA\n\nexploration stage of the copper mining projects would need to be able to gain new insights and search\nthrough'),(101,9,4,'nce Deposit\nHON Qnsene\nittle Evas\nsit\n\nSearch Table OCR Image classification Word Cloud\n\nFigure 1 Transversal corpus search features of the digital data factory.\nSeamless Search Tool\n\nTo have a firm grasp of the copper resource information covering the definitions, inferences,\nindications, and compiled measurements, geologists and mining professionals involved in the\n\n\n\nVIENNA | AUSTRIA\n\nexploration stage of the copper mining projects would need to be able to gain new insights and search\nthrough their unstructured data (OceanaGold Corporation, 2022). The ingestion and digestion process\nmakes it possible to obtain new knowledge and insights, which is very difficult to achieve from the\noriginal unstructured data. The ingested data is run through a Machine Learning-based pipeline that\ntransforms the unstructured data into structured data with its elements made searchable (Mamador et\nal., 2020). With the seamless search, geological, mineral and deposit information can be found\nefficiently and fresh insights into the site mineralogy can be gained with relative ease (Maver et al.,\n2021).\n\nCopper mineral deposit models can be correlated to their appropriate locations on geological maps and\nsupported by visual evidence such as mineralogy descriptions in drill hole cores and geological maps\nas displayed in Figure 2. Solid geological inferences can be made regarding the characteristics of the\ncopper ore deposit, which can then lead to feasible drilling and productive mining plans suppo'),(102,9,5,'and fresh insights into the site mineralogy can be gained with relative ease (Maver et al.,\n2021).\n\nCopper mineral deposit models can be correlated to their appropriate locations on geological maps and\nsupported by visual evidence such as mineralogy descriptions in drill hole cores and geological maps\nas displayed in Figure 2. Solid geological inferences can be made regarding the characteristics of the\ncopper ore deposit, which can then lead to feasible drilling and productive mining plans supported by\nowned data.\n\nee iy pty\n\nFigure 2 Findings from deep corpus search of copper resource models and new geological insights\nacross Queensland.\n\nFile, Domain and Image Tagging\n\nFile, domain, and image tagging are performed through the data factory. This allows for the\nconsolidation of unstructured data, breaking data silos across documents and disciplines, and making\nall the relevant data during the copper mining operation accessible across organizations and contractors,\nhence streamlining mining workflows and supporting cross-company collaboration (Maver et al., 2021).\n\nAn ongoing copper mining operation would continuously produce various figurative and imagery data\nsuch as resources and machinery management, schedules, rainfall, land survey information, engineering\nsolutions, geology, and more. Some, if not all of these will be integrated or considered in the mining\nplan or model (OceanaGold Corporation, 2022).\n\nThe continuous aggregation by domain experts and ingestion of unstruc'),(103,9,6,'ning workflows and supporting cross-company collaboration (Maver et al., 2021).\n\nAn ongoing copper mining operation would continuously produce various figurative and imagery data\nsuch as resources and machinery management, schedules, rainfall, land survey information, engineering\nsolutions, geology, and more. Some, if not all of these will be integrated or considered in the mining\nplan or model (OceanaGold Corporation, 2022).\n\nThe continuous aggregation by domain experts and ingestion of unstructured data that happens during\nthis stage of operation are improved via machine learning processes and scaled suitably. This is\nparticularly useful to track, reassess, visualize, and evaluate the mining plans or mining models at\nvarious copper mining stages, such as tracking the progress of specific sub-blocks at a particular time.\nInformation from various sources would be integrated or correlated with these mining plans or models\nto support decision-making associated with the site as shown in Figure 3. This process facilitates not\nonly efficient and easy problem-solving conditions but supports planning processes and governance\nstructures to be data-based and able to respond to ESG opportunities, risks and challenges (Maver et\nal., 2021).\n\n\n\nVIENNA | AUSTRIA\n\nTICK HILL\nMINE\n\nFigure 3 Important documents and varying information concerning the mining model at different\nscales are accessible by various roles (mining engineers, operators, surveyors, geologists, etc.) across\nthe organizatio'),(104,9,7,'igure 3. This process facilitates not\nonly efficient and easy problem-solving conditions but supports planning processes and governance\nstructures to be data-based and able to respond to ESG opportunities, risks and challenges (Maver et\nal., 2021).\n\n\n\nVIENNA | AUSTRIA\n\nTICK HILL\nMINE\n\nFigure 3 Important documents and varying information concerning the mining model at different\nscales are accessible by various roles (mining engineers, operators, surveyors, geologists, etc.) across\nthe organization involved.\n\nTable extraction\n\nEngineering, geoscience, and even metallurgic processes at the copper processing plant produce a vast\nwealth of tables and numerical data (OceanaGold Corporation, 2022). Classifying tables to a particular\nimage group is also tracked by the data factory. Optical character recognition (OCR) is used to identify\nand locate individual and specific information which can be used in further analyses. By automatically\nconverting each table image to a .csv file, valuable information becomes easily available, searchable,\nand aggregated across various mining operation stages or copper processing plant processes as shown\non Figure 4, Manual translation of the table to a .csv file can therefore be avoided, and information\ntracked to the original location in the report.\n\npara\nINDICATED os oa | 96 96\n\nwweenwed | oooe | 27 | a\nTom [ose [as fos fo |\n\nFigure 4 Table image with numerical values such as tonnage and ore grades identified by the data\nfactory’s OCR and extracted'),(105,9,8,'ormation becomes easily available, searchable,\nand aggregated across various mining operation stages or copper processing plant processes as shown\non Figure 4, Manual translation of the table to a .csv file can therefore be avoided, and information\ntracked to the original location in the report.\n\npara\nINDICATED os oa | 96 96\n\nwweenwed | oooe | 27 | a\nTom [ose [as fos fo |\n\nFigure 4 Table image with numerical values such as tonnage and ore grades identified by the data\nfactory’s OCR and extracted for external documents.\n\nBeing able to extract numerical values from tables is valuable to help track mining information such as\nore grades, tonnage, production values, coordinates, rainfall, work hours, processing plant or laboratory\nparameters.\n\nMapping of Similar Files and Word Cloud\n\nA data factory contains multiple tools that facilitate rapid comprehension and understanding of the\ncontext of the data. For example, the searchability of the elements in each document allows transverse\ndocuments within the vast database with similar keywords to be grouped together through the search\nresults. The word cloud associated with each document also allows rapid comprehension of the whole\ndocument briefly. Finding analogues, similarities, and historical issues is a problem that now has a\nsolution. In the case of safety, health, and environmental control, incidents, lost time injury (LTD, and\nreports of investigation (RI) can be traced to specific documents or even specific pages in the origin'),(106,9,9,'ts within the vast database with similar keywords to be grouped together through the search\nresults. The word cloud associated with each document also allows rapid comprehension of the whole\ndocument briefly. Finding analogues, similarities, and historical issues is a problem that now has a\nsolution. In the case of safety, health, and environmental control, incidents, lost time injury (LTD, and\nreports of investigation (RI) can be traced to specific documents or even specific pages in the original\nreports,\n\n\n\nVIENNA | AUSTRIA\n\nEnvironmental, Social and Corporate Governance Targets in Mining\n\nThe data factory certainly allows the risks and opportunities related to sustainability to be recognized,\nevaluated and managed under a holistic framework pertaining to environmental, social and governance\naspects. The data factory approach is an incentive that can add value and align the mining operation\nwith broader Environmental, Social and Corporate Governance (ESG) goals to limit environment\nimpact. The data-supported holistic ecosystem from this approach is positioned to enhance cost-benefit\nassessment of the ESG throughout the mining cycle.\n\nConclusion\n\nManaging unstructured data into structured data embedded with end-to-end ML/AI advanced\ntechnology made it possible to explore, analyze and make fast decisions using big data. With this,\norganizations involved in copper mining projects and more generally in the mining industry are able to\nprocess and present new mining information a'),(107,9,10,'mpact. The data-supported holistic ecosystem from this approach is positioned to enhance cost-benefit\nassessment of the ESG throughout the mining cycle.\n\nConclusion\n\nManaging unstructured data into structured data embedded with end-to-end ML/AI advanced\ntechnology made it possible to explore, analyze and make fast decisions using big data. With this,\norganizations involved in copper mining projects and more generally in the mining industry are able to\nprocess and present new mining information and knowledge from the dataset. Tools within the data\nfactory such as deep search module, word cloud, table extraction, and image identification contribute\nsignificantly to providing a comprehensive understanding across the full value chain for the copper\ndeposit models, mining plans, copper treatment processes, rehabilitation plans, and safety, health and\nenvironmental trends. Hence, maximizing the capability of unstructured data has proved impactful in\nterms of significantly reducing the consumption of research time and costs (Maver et al., 2021) and\nalign mining operations with global ESG limiting the impact on the environment.\n\nAcknowledgment\n\nThis paper utilizes the data from the GSQ Open Data Portal database (geoscience.data.qld.gov.au). This\ndatabase is owned by the Queensland Government and is open for public access. However, the\ninterpretation and conclusion contained in this report are those of the authors alone.\n\nReferences\n\nBonakdarpour, M. & Bailey, T.M. (2022) The future o'),(108,9,11,'ption of research time and costs (Maver et al., 2021) and\nalign mining operations with global ESG limiting the impact on the environment.\n\nAcknowledgment\n\nThis paper utilizes the data from the GSQ Open Data Portal database (geoscience.data.qld.gov.au). This\ndatabase is owned by the Queensland Government and is open for public access. However, the\ninterpretation and conclusion contained in this report are those of the authors alone.\n\nReferences\n\nBonakdarpour, M. & Bailey, T.M. (2022) The future of copper — Will the looming supply gap short-\ncircuit the energy transition? S&P Global. JAS Markit.\n\nMamador, C., Aranda, J. O., Arif, N. E., Hernandez, N. M., & Baillard, F. (2020, September). A\ngeological regional case study for pressure, temperature, and salinity for the GoM using machine\nlearning technology on unstructured data. In EAGE/AAPG digital subsurface for Asia Pacific\nConference (Vol. 2020, No. 1, pp. 1-4). European Association of Geoscientists & Engineers.\n\nMaver, K. G., Baillard, F., & Hernandez, N. M. (2021, May). Accelerating E&P Decisions by\nApplying Artificial Intelligence and Big Data Analytics to Unstructured Data. In Digital Subsurface\nConference in Latin America (Vol. 2021, No. 1, pp. 1-5). European Association of Geoscientists &\nEngineers.\n\nMaver, K. G., Hernandez, N. M., Baillard, F., & Cooper, R. (2020). Processing of unstructured\ngeoscience and engineering information for instant access and extraction of new knowledge. First\nBreak, 38(6), 59-64.\n\nOceanaGold '),(109,9,12,' F., & Hernandez, N. M. (2021, May). Accelerating E&P Decisions by\nApplying Artificial Intelligence and Big Data Analytics to Unstructured Data. In Digital Subsurface\nConference in Latin America (Vol. 2021, No. 1, pp. 1-5). European Association of Geoscientists &\nEngineers.\n\nMaver, K. G., Hernandez, N. M., Baillard, F., & Cooper, R. (2020). Processing of unstructured\ngeoscience and engineering information for instant access and extraction of new knowledge. First\nBreak, 38(6), 59-64.\n\nOceanaGold Corporation (2022). The mining process. OceanaGold. Retrieved from\nhttps://oceanagold.com/operation/macraes/the-mining-process/\n\n\n');
/*!40000 ALTER TABLE `chunks` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `papers`
--

DROP TABLE IF EXISTS `papers`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!50503 SET character_set_client = utf8mb4 */;
CREATE TABLE `papers` (
  `paper_id` int NOT NULL AUTO_INCREMENT,
  `title` varchar(255) NOT NULL,
  PRIMARY KEY (`paper_id`)
) ENGINE=InnoDB AUTO_INCREMENT=10 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `papers`
--

LOCK TABLES `papers` WRITE;
/*!40000 ALTER TABLE `papers` DISABLE KEYS */;
INSERT INTO `papers` VALUES (1,'2021 - A Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with Machine Learning Techniques.pdf'),(2,'2021 - Supporting the UN 2050 Net Zero goals by reading the earth better.pdf'),(3,'2022 - Scaling and optimizing performance and cost of machine learning ingestion on unstructured data for subsurface applications.pdf'),(4,'2022 - Sustainable data mining AI_ML-based parameter extraction, data visualization and connectivity to upcycle big-data for basin analysis .pdf'),(5,'2022 - Utilizing Machine Learning to Gain Geological Insights through Unstructured Data for  Sustainable Exploration Activities.pdf'),(6,'2023 - CO2 emissions the elephant in the room – A pathway of reduction using digitalization and unstructured data.pdf'),(7,'2023 - Double funnel approach for screening of potential CO2 storage opportunities in the Norwegian Continental Shelf.pdf'),(8,'2023 - Sand Production and Control Benchmarking through Unstructured Data Analysis with Machine Learning in the North Sea.pdf'),(9,'2023 - Using Machine Learning-Based Data Factory to Unlock Mining in Australia for Environmental,  Social and Corporate Governance (ESG).pdf');
/*!40000 ALTER TABLE `papers` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2024-06-18 11:43:35
