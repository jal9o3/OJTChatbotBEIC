{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Database Notebook\n",
    "I made this notebook to explore the MySQL Python Connector. Markdown cells have\n",
    "been added for the team's future reference.\n",
    "### WARNING: Do not 'RUN ALL' cells! \n",
    "### This notebook contains cells that remove databases, papers, writes data, etc.\n",
    "### For exploration purposes only! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enter MySQL Password\n",
    "Password will be input this way to avoid being exposed in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "password = input(\"Enter your database password: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Connection\n",
    "This cell connects us to MySQL. Change the host and username as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_db = mysql.connector.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"root\",\n",
    "    password=password,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing the MySQL cursor\n",
    "This cursor allows us to perform MySQL operations using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = text_db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a database\n",
    "This cell creates a new database. Change the database name as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor.execute(\"CREATE DATABASE technical_database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop a database\n",
    "### (Be careful with this cell!)\n",
    "This cell removes an existing database. Change name as needed (the cell below outputs a list of existing databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor.execute(\"DROP DATABASE technical_database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List all existing databases\n",
    "This cell displays all MySQL databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('information_schema',)\n",
      "('mysql',)\n",
      "('performance_schema',)\n",
      "('sakila',)\n",
      "('sys',)\n",
      "('world',)\n"
     ]
    }
   ],
   "source": [
    "mycursor.execute(\"SHOW DATABASES\")\n",
    "for x in mycursor:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Database\n",
    "This connects us to a specific database. Change the database name as needed (you \n",
    "can choose from the list of databases output by the cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_db = mysql.connector.connect(\n",
    "    host=\"127.0.0.1\",\n",
    "    user=\"root\",\n",
    "    password=password,\n",
    "    database=\"technical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor = text_db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop a table from the database\n",
    "### (Be careful with this cell!)\n",
    "\n",
    "This removes a table from the database that we are connected to. Change the table name as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor.execute(\"DROP TABLE papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a sample table for papers\n",
    "This creates a database table called 'papers' with columns for primary key (auto incremented), title, author, and chunk (consisting of 255 chars at most)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycursor.execute(\"\"\"\n",
    "    CREATE TABLE papers (\n",
    "        id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        title VARCHAR(255),\n",
    "        author VARCHAR(255),\n",
    "        chunk TEXT(255)\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show all tables in database\n",
    "This cell lists all the tables in the database that we're connected to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('chunks',)\n",
      "('papers',)\n"
     ]
    }
   ],
   "source": [
    "mycursor.execute(\"SHOW TABLES\")\n",
    "for x in mycursor:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See current directory\n",
    "We're about to work with files (Tesseract output .txt files), so we need to check our current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\Documents\\OJTChatbotBEIC\\MySQL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload an extracted paper into the database\n",
    "The paper is divided into chunks (up to length 255) and stored into the database, \n",
    "along with metadata such as title and author name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the entire file content\n",
    "with open('../Tesseract/Extracted.txt', 'r', encoding=\"utf-8\", errors='ignore') as file:\n",
    "    file_content = file.read()\n",
    "\n",
    "# Split the content into smaller strings (up to 255 characters)\n",
    "max_length = 255\n",
    "split_content = [file_content[i:i + max_length] for i in range(0, len(file_content), max_length)]\n",
    "\n",
    "title = \"A CASE STUDY OF UNDERSTANDING THE BONAPARTE BASIN USING UNSTRUCTURED DATA ANALYSIS WITH MACHINE LEARNING TECHNIQUES\"\n",
    "authors = \"A.N.N. Sazali, N.M. Hernandez, F. Baillard, K.G. Maver\"\n",
    "\n",
    "# Insert each smaller string into the database\n",
    "query = \"INSERT INTO papers (title, author, chunk) VALUES (%s, %s, %s)\"\n",
    "for content in split_content:\n",
    "    mycursor.execute(query, (title, authors, content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commit the changes into the database\n",
    "This updates the database, this time for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print all rows of the table\n",
    "Useful for checking the committed changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 1, \"\\nA CASE STUDY OF UNDERSTANDING THE\\nBONAPARTE BASIN USING UNSTRUCTURED DATA\\nANALYSIS WITH MACHINE LEARNING TECHNIQUES\\n\\nAN. Sazali!, N.M. Hernandezâ€™, F. Baillard', K.G. Maver!\\n\\n'Traya Energies\\n\\nSummary\\n\\nAs part of exploration and production the oil and gas \")\n",
      "(2, 1, 2, 'art of exploration and production the oil and gas industry produce substantial amounts of data\\nwithin different disciplines of which 80% are unstructured like reports, presentations, spreadsheets etc.\\nThe value of technical work is reduced due to the lack')\n",
      "(3, 1, 3, 'value of technical work is reduced due to the lack of time available for analysis and critical\\nthinking and the under-utilization of the data. To assist geoscientist and engineers, Machine Learning\\n(ML) and Artificial Intelligence (AI) technologies are ap')\n",
      "(4, 1, 4, 'd Artificial Intelligence (AI) technologies are applied to process the unstructured data from\\n440 wells from the Bonaparte Basin in Australia making it possible to perform more accurate analysis\\nand make faster decisions.\\n\\nBased on the play-based explorat')\n",
      "(5, 1, 5, 'aster decisions.\\n\\nBased on the play-based exploration pyramid concept, the time spent at the Basin Focus stage can be\\nreduced, and more time are available to focus on the other project stages. The explorationist will be\\nable to bring more value to the stu')\n",
      "(6, 1, 6, 'ionist will be\\nable to bring more value to the study.\\n\\nIt will be shown that potential issues encountered during exploration of the Bonaparte Basin can be\\nidentified. Based on a quick look and gathering of all information it can be concluded that most of ')\n",
      "(7, 1, 7, ' all information it can be concluded that most of the\\nproduction in the Bonaparte Basin is from Jurassic and Triassic with observed net pay of 18-60m\\nthickness, porosity of 11-29% and saturation of 11-55% Sw.\\n\\n\\n\\nA Case Study of Understanding the Bonaparte')\n",
      "(8, 1, 8, 'Sw.\\n\\n\\n\\nA Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with\\nMachine Learning Techniques\\n\\nIntroduction\\n\\nAs part of exploration and production the oil and gas industry produce substantial amounts of data\\nwithin different d')\n",
      "(9, 1, 9, 'uce substantial amounts of data\\nwithin different disciplines of which 80% are unstructured like reports, presentations, spreadsheets etc\\nand it is expected to grow exponentially. As a result, geoscientists and engineers spend 50 to 80% of\\ntheir time searc')\n",
      "(10, 1, 10, ' and engineers spend 50 to 80% of\\ntheir time searching and assembling data and only 1 to 5% of the data is fully utilized. The value of\\ntechnical work is therefore reduced due to the lack of time available for analysis and critical thinking\\nand the under-')\n",
      "(11, 1, 11, ' for analysis and critical thinking\\nand the under-utilization of the data. To assist geoscientist and engineers, Machine Learning (ML)\\nand Artificial Intelligence (AI) technologies are applied to process the unstructured data making it\\npossible to perform')\n",
      "(12, 1, 12, 'he unstructured data making it\\npossible to perform more accurate analysis and make faster decisions.\\n\\nIn this case study the area of interest covers Bonaparte Basin, which is located north-west of\\nthe Australian continental margin (Figure 1). It joins the')\n",
      "(13, 1, 13, 'ralian continental margin (Figure 1). It joins the Money Shoal basin in the north-east and\\nthe Browse Basin in the south-west. Furthermore, the Timor Trough defines the northern boundary.\\nThe areal extent of the basin is approximately 270,000 sq. km. The ')\n",
      "(14, 1, 14, 'of the basin is approximately 270,000 sq. km. The objective of this study is to\\nunderstand and obtain meaningful insights into the Bonaparte Basin based on the substantial amount\\nof information available in previous studies, reports and presentations. The')\n",
      "(15, 1, 15, 'n previous studies, reports and presentations. The unstructured data of the\\nBonaparte Basin have been ingested in a Knowledge Container through consecutive ML and AI\\npipelines and analysed using big data analytics tools.\\n\\nConsist of several structural ele')\n",
      "(16, 1, 16, 'nalytics tools.\\n\\nConsist of several structural elements :\\n@ Ashmore Platform Â© Malita Graben\\nÂ© Vulcan Sub-Basin Â© Sahul Platform\\n\\nÂ© Londonderry High @ Flamingo High\\nÂ© Petrel Sub-basin @ Flamingo Syncline\\nÂ© Darwin Shelf @Â® Sahul Syncline\\n@ Calder Graben Â® ')\n",
      "(17, 1, 17, ' Darwin Shelf @Â® Sahul Syncline\\n@ Calder Graben Â® Nancar Trough\\n@ Troubadour Terrace Laminaria High\\nFigure 1 Location of the Bonaparte Basin within the Australian continental margin (left) and 14\\nstructural elements observed within the Bonaparte Basin (ri')\n",
      "(18, 1, 18, 'l elements observed within the Bonaparte Basin (right).\\n\\nMethodology\\n\\nAs of 2021, the Bonaparte Basin encompasses 440 wells representing 58 years of exploration history\\nsummarized in over 270,000 pages of documents and in 250,000 images. It is estimated t')\n",
      "(19, 1, 19, 'documents and in 250,000 images. It is estimated that billions of\\ndollars have been invested over the years to acquire and interprete the data, making it a substantial\\nsource of information for new exploration activities.\\n\\nThe Play Based Exploration (PBE)')\n",
      "(20, 1, 20, 'tion activities.\\n\\nThe Play Based Exploration (PBE) approach is often used as a traditional framework to refine the\\ngeoscientistsâ€™s understanding from a broad basin level to a narrow prospect focus (Lottaroli et al., 2016).\\nAs a start such an approach ofte')\n",
      "(21, 1, 21, 'li et al., 2016).\\nAs a start such an approach often involves capturing the current state of knowledge with massive\\nbackground resources to understand and analyse the key features of the basin and the major risks\\nassociated to it. Such information is prima')\n",
      "(22, 1, 22, ' risks\\nassociated to it. Such information is primarily available in unstructured data, requiring geoscientist and\\nengineers to process and ingest the information before focusing on a specific play and prospect using\\nstructured data. Therefore, we have mod')\n",
      "(23, 1, 23, 'pect using\\nstructured data. Therefore, we have modified the existing PBE pyramid to introduce an additional\\n\\n\\n\\ndimension associated with data science identifying the different types of data available at different\\nstages, allowing us to better define the b')\n",
      "(24, 1, 24, 'fferent\\nstages, allowing us to better define the best suited ML/AI strategy for a given stage (Figure 2).\\n\\nSTRUCTURED\\n\\nTora nercare cercest\\nUNSTRUCTURED â€˜ 8 rence siento renown, se 30008)\\n\\nof being present and effective over 20,\\n\\n@ Assess the possibsty a ')\n",
      "(25, 1, 25, ' and effective over 20,\\n\\n@ Assess the possibsty a play may\\nexiets ina basin\\n\\nFigure 2 Customized Play Based Exploration (PBE) pyramid with ML technology (Modified from\\nLottaroli et al., 2016).\\n\\nFocusing on the unstructured data associated with the Basin a')\n",
      "(26, 1, 26, ' the unstructured data associated with the Basin and Play Analysis, all the data from the\\nBonaparte Basin have been processed through a succession of AI/ML automated pipeline such as\\nNatural Language Processing or Deep Convolutional Neural Network (Hernan')\n",
      "(27, 1, 27, 'ssing or Deep Convolutional Neural Network (Hernandez et al., 2019), (Figure\\n3). The sharable structured data is then further processed through deeper level of analytics to detect\\ntrends and anomalies present within the data. Machine assistance is heavily')\n",
      "(28, 1, 28, 'ent within the data. Machine assistance is heavily used in repetitive tasks early\\nin the process during the processing of the data and up to 95% of the tasks will be performed by the\\nmachine. This provides additional time for the specialist to focus on cr')\n",
      "(29, 1, 29, ' additional time for the specialist to focus on critical thinking and cognitive skills\\n\\nto interpret the data.\\n\\nMACHINE LEARNING PIPELINE ANALYTICS GEOLOGICAL INSIGHTS\\n\\nEarly intespretation on\\n\\n* lithology Cloud Geological environment\\n\\nâ€˜Text Search = â€”~ D')\n",
      "(30, 1, 30, ' Cloud Geological environment\\n\\nâ€˜Text Search = â€”~ Data are geospatially\\n+ Image Classification - + Â» distributed on maps for\\nDeep Convolutional trends and anomalies\\n\\nUnstructured Neural Network Sharable observation\\n\\nData Structured Â» Image Search\\nData\\n+ |,')\n",
      "(31, 1, 31, 'ervation\\n\\nData Structured Â» Image Search\\nData\\n+ |, Extracted | Metadata Extraction |, Document\\n\\nText and Tagging Tags Heat Map\\n\\nâ€˜Natural Language Processing\\n\\nEarly trend can be observed\\nacross the whole basin\\n\\nIdentify well analogues and\\n\\n* Knowledge Grap')\n",
      "(32, 1, 32, 'sin\\n\\nIdentify well analogues and\\n\\n* Knowledge Graph â€”- â€”* relationship between wells\\n\\n95% 1909 â€” 6) â€”\\n\\nMI/AI sequence automated with Analytics tools for data display Hurnan high-level interpretation\\nhuman in the loop for QC\\n9,\\n\\ntue oN $$ 5% 40% mee |\\n\\nFig')\n",
      "(33, 1, 33, 'in the loop for QC\\n9,\\n\\ntue oN $$ 5% 40% mee |\\n\\nFigure 3 Unstructured Big Data pipeline\\n\\nIn this case study, interpretation using the Big Data workflow was used to understand the exploration\\nhistory, how the basin developed, its petroleum system and the ma')\n",
      "(34, 1, 34, 'e basin developed, its petroleum system and the main issue of the dry wells occurrence\\nto avoid repeating the mistakes of the past and improve future decision making.\\n\\n\\n\\nBy analysing the data, five potential issues are identified i.e. (i) Discrepancies in')\n",
      "(35, 1, 35, 'al issues are identified i.e. (i) Discrepancies in Formation Tops, (ii)\\nLimited understanding of Lithology Distribution, (iii) Limited Mineral Composition Understanding,\\n(iv) Fluid Distribution, and (v) Pressure/Temperature Patterns. Each potential issue ')\n",
      "(36, 1, 36, 'essure/Temperature Patterns. Each potential issue is tackled by\\nidentifying trends and anomalies across the basin using images, tables and plots extracted from the\\nunstructured data corpus.\\n\\nResults\\n\\nAs an example, the analysis of the (ii) Limited Underst')\n",
      "(37, 1, 37, ' example, the analysis of the (ii) Limited Understanding of Lithology Distribution, shown in\\nFigure 4, is performed using heatmaps. The heatmaps show the distribution of clastic and carbonates\\nacross the Bonaparte Basin and identify patterns and anomalies')\n",
      "(38, 1, 38, 'onaparte Basin and identify patterns and anomalies present in the area. The result can be\\nsupported by the stratigraphic chart where the carbonate environment occurs in the younger formation\\nfrom Cretaceous to Neogene period, whereas clastic environment i')\n",
      "(39, 1, 39, 's to Neogene period, whereas clastic environment is present in the older formations from\\nTriassic to Cretaceous.\\n\\nBasal vanegrassive enadetone Limeston:\\nand marine shelf sand mestone\\nHE arine claystone and shale\\n\\nModitied from Frankowlez & McClay 2009\\n\\nFi')\n",
      "(40, 1, 40, ' shale\\n\\nModitied from Frankowlez & McClay 2009\\n\\nFigure 4 Lithology distribution on heatmaps (left) and corresponding stratigraphic chart (right).\\n\\nThe analysis of the (iii) Limited Mineral Composition Understanding, shown in Figure 5, utilizes the\\nthin se')\n",
      "(41, 1, 41, 'rstanding, shown in Figure 5, utilizes the\\nthin section automatically extracted using ML classification over the full area and suggests that:\\n\\nQuartz overgrowth and kaolinite are quite common in Bonaparte Basin\\n\\nMica mineral can be observed at the north-e')\n",
      "(42, 1, 42, 'Basin\\n\\nMica mineral can be observed at the north-eastern part of the basin\\n\\nHighly corroded, skeletal feldspar has been extensively dissolved, which forms secondary\\nporosity, and can be observed in the northern part of the basin\\n\\nSome patchy siderites are')\n",
      "(43, 1, 43, 'thern part of the basin\\n\\nSome patchy siderites are also observed in the southern part of the basin\\n\\n\\n\\nstam (cone\\nFigure 5 Thin section images distributed on a map across the Bonaparte Basin.\\n\\nConclusion\\n\\nA regional understanding is critical and time consu')\n",
      "(44, 1, 44, ' regional understanding is critical and time consuming as it involves dealing with a very large data\\n\\nvolume. Within a project time frame, based on PBE pyramid, the time spent at the Basin Focus stage\\n\\ncan be reduced, and more time are available to focus ')\n",
      "(45, 1, 45, ' be reduced, and more time are available to focus on the other project stages. The explorationist will\\nbe able to bring more value to the study.\\n\\nML applications have proven to be able to play a crucial part in order to organize large unstructured\\ndata co')\n",
      "(46, 1, 46, 'rt in order to organize large unstructured\\ndata corpuses. This allows faster and accurate decision making within the fast-moving industry.\\n\\nIn this study, some potential issues encountered during exploration of the Bonaparte Basin can be\\nidentified. Based')\n",
      "(47, 1, 47, 'on of the Bonaparte Basin can be\\nidentified. Based on a quick look and gathering of all information it can be concluded that most of the\\nproduction in the Bonaparte Basin is from Jurassic and Triassic with observed net pay ~18-60m\\nthickness, porosity ~11-')\n",
      "(48, 1, 48, ' observed net pay ~18-60m\\nthickness, porosity ~11-29% and saturation ~11-55% Sw.\\n\\nReferences\\n\\nHernandez N., Lucafias P., Graciosa J.C., Mamador C., and Panganiban L. C. I, 2019: Automated\\ninformation retrieval from unstructured documents utilizing a seque')\n",
      "(49, 1, 49, 'eval from unstructured documents utilizing a sequence of smart machine learning\\nmethods within a hybrid cloud container. EAGE Workshop on Big Data and Machine Learning for E&P\\nEfficiency 25 - 27 February.\\n\\nLottaroli F., Craig J., Cozzi A., 2016: Evaluatin')\n",
      "(50, 1, 50, '\\nLottaroli F., Craig J., Cozzi A., 2016: Evaluating a vintage play fairway exercise using subsequent\\nexploration results: did it work? Petroleum Geoscience, Vol 24, no 2, p. 159 â€” 171.\\n\\nMaver K.G., Hernandez N., Lucafias P., Graciosa J.C., Mamador C., Pan')\n",
      "(51, 1, 51, 'ez N., Lucafias P., Graciosa J.C., Mamador C., Panganiban L.C.L, Yu C., and\\nMaver M.G., 2018: An automated information retrieval platform for unstructured well data smart\\nmachine learning algorithms within a hybrid cloud container. EAGE/PESGB Workshop on ')\n",
      "(52, 1, 52, ' a hybrid cloud container. EAGE/PESGB Workshop on Machine\\nLearning, 29 â€” 30 November.\\n\\n\\n')\n",
      "(53, 2, 1, \"\\nSCALING AND OPTIMIZING PERFORMANCE AND COST OF MACHINE\\n\\nLEARNING INGESTION ON UNSTRUCTURED DATA FOR\\nSUBSURFACE APPLICATIONS\\n\\nL.C.L. Panganibanâ€™, F. Baillard', N.M. Hernandez!\\n\\n' Traya Energies\\n\\nSummary\\n\\nIn recent years, the energy industry has shifted th\")\n",
      "(54, 2, 2, 'n recent years, the energy industry has shifted their attention into extracting additional values from\\ntheir in-house legacy datasets for shorter project turnaround and better decision making. Internal digital\\ntransformation initiatives and access to new ')\n",
      "(55, 2, 3, 'ital\\ntransformation initiatives and access to new technology such as cloud computing, machine learning and\\nmicroservices made it possible to shift towards a scalable ingestion platform.\\n\\nA modern scalable ingestion platform often includes 1) automated mac')\n",
      "(56, 2, 4, 'ingestion platform often includes 1) automated machine learning (ML) components\\narticulated around pipelines to parse and go through the data and extract the needed information 2)\\nstorage component such Database, Datawarehouse and Datalake defining what d')\n",
      "(57, 2, 5, 'tabase, Datawarehouse and Datalake defining what data to be stored and\\nhow.\\n\\nIn this paper, we will provide a description of these different components, their modern implementation\\ninto a cloud environment using microservices and some performance benchmar')\n",
      "(58, 2, 6, ' using microservices and some performance benchmark based on real world\\ndata examples.\\n\\n\\n\\nMADRID | SPAIN\\n\\nScaling and optimizing performance and cost of machine learning ingestion on unstructured\\ndata for subsurface applications\\n\\nIntroduction\\n\\nIn recent y')\n",
      "(59, 2, 7, 'subsurface applications\\n\\nIntroduction\\n\\nIn recent years, the energy industry has shifted their attention into extracting additional values from\\ntheir in-house legacy datasets for shorter project turnaround and better decision making. Internal digital\\ntrans')\n",
      "(60, 2, 8, 'and better decision making. Internal digital\\ntransformation initiatives and access to new technology such as cloud computing, machine learning and\\nmicroservices made it possible to shift towards a scalable ingestion platform.\\n\\nA modern scalable ingestion ')\n",
      "(61, 2, 9, ' ingestion platform.\\n\\nA modern scalable ingestion platform often includes 1) automated machine learning (ML) components\\narticulated around pipelines to parse and go through the data and extract the needed information 2)\\nstorage component such Database, Da')\n",
      "(62, 2, 10, 'information 2)\\nstorage component such Database, Datawarehouse and Datalake defining what data to be stored and\\nhow.\\n\\nIn this paper, we will provide a description of these different components, their modern implementation\\ninto a cloud environment using mic')\n",
      "(63, 2, 11, ' implementation\\ninto a cloud environment using microservices and some performance benchmark based on real world\\ndata examples.\\n\\nScalable ingestion platform architecture\\n\\nThe vast majority of experimentation and testing of new ML application are performed ')\n",
      "(64, 2, 12, 'n and testing of new ML application are performed on notebooks\\non local machines leveraging on local hardware with the data and the code being stored locally-. Such\\na setup has the advantage of being lightweight allowing a fast turnaround between two ML i')\n",
      "(65, 2, 13, 'weight allowing a fast turnaround between two ML iterations.\\nHowever the lack of traceability, compatibility and hardware limitation makes it challenging to scale\\nsuch application, hence the requirement of a more scalable solution.\\n\\nIn comparison, a scala')\n",
      "(66, 2, 14, ' a more scalable solution.\\n\\nIn comparison, a scalable ingestion platform is a type of system native able to handle current and future\\nworkloads regardless of the amount of data ingested or users connecting to it, considering both scaling\\nin (shrinking res')\n",
      "(67, 2, 15, ' to it, considering both scaling\\nin (shrinking resources) and scaling out (expanding resources). An example of such system can be seen\\non Figure 1. The architecture is made of of four components namely pipeline, storage, compute and\\ninterfaces.\\n\\nINTERFACE')\n",
      "(68, 2, 16, 'eline, storage, compute and\\ninterfaces.\\n\\nINTERFACES,\\n\\nf Data Atelier\\n\\nImages\\n\\nCOMPUTE\\nProduction\\nRecords\\n\\nReparts,\\nPresentations\\n\\nAPI Interaction\\n\\nMaintenance\\nRecords\\n\\nFigure 1: Ingestion Platform Architecture\\n\\nPipeline or data pipeline is the main driver')\n",
      "(69, 2, 17, 'ture\\n\\nPipeline or data pipeline is the main driver in extracting and transforming data into a unified format.\\nThe typical ingestion workflow is based on an Extract-Transform-Load process with machine learning\\nworkflows able to accommodate the variety in s')\n",
      "(70, 2, 18, 'ing\\nworkflows able to accommodate the variety in sources and forms present in unstructured data\\n(Hernandez et al., 2019).\\n\\n\\n\\nMADRID | SPAIN\\n\\nThe storage component is where the data resides. Storage is often the least thought about in development\\nand archi')\n",
      "(71, 2, 19, 'n the least thought about in development\\nand architecture strategies, but it is one of the core contributors in terms of cost to the organization. The\\ncompute node component schedules and orchestrates the data pipelines, logic flows, and algorithms.\\nThe i')\n",
      "(72, 2, 20, 'data pipelines, logic flows, and algorithms.\\nThe interface or the dashboard component provides the monitoring and observability capabilities. This\\npresents the events, statuses, logs, and other operational insights that can be used for decision making.\\nTh')\n",
      "(73, 2, 21, ' insights that can be used for decision making.\\nThese components have different ways to be scaled depending of 1) the environment of installation: on-\\npremise vs. cloud 2) the volume and type of data being processed 3) the early development decision: all\\n')\n",
      "(74, 2, 22, ' processed 3) the early development decision: all\\nout of the box cloud providers solution, opensource self maintained solution or hybrid.\\n\\nComponentâ€™s optimization\\n\\nThe first metrics to consider for optimization is processing time and should be independen')\n",
      "(75, 2, 23, 'zation is processing time and should be independent from the\\nvolume of data to be processed by the pipeline. This optimization is achieved through parallelization,\\ndistributed computing and orchestration by scaling up or down the usage based on demand. Ap')\n",
      "(76, 2, 24, 'y scaling up or down the usage based on demand. Applications\\nand data pipelines are packaged into containers and then deployed into a Kubernetes cluster in which it\\nhandles the scheduling, distribution, and allocation across different machines. Workflow m')\n",
      "(77, 2, 25, 'd allocation across different machines. Workflow managers (\\nWM) provide the platform to execute and orchestrate the jobs and tasks that are contained in a Pod/Pool\\n(Figure 2). Workflow managers also implement orchestration and scheduling though it handles')\n",
      "(78, 2, 26, 'ent orchestration and scheduling though it handles the tasks\\nin the application layer where this determines if the data execution is successful\\n\\nFigure 2: Unstructured data pipeline with orchestration in place\\n\\nThe second metric to consider is the extensi')\n",
      "(79, 2, 27, 'lace\\n\\nThe second metric to consider is the extensibility metric. A single service or component can be used\\nbeyond what is originally intended. Thanks to the microservices, exposing your data via application\\nprogramming interfaces or APIs is a good practis')\n",
      "(80, 2, 28, 'n\\nprogramming interfaces or APIs is a good practise (Figure 3). It provides a common language across\\nteams where implementation is unified across different data sources. An additional advantage of the use\\nof APIs is the additional level of abstraction for')\n",
      "(81, 2, 29, 'of APIs is the additional level of abstraction for the storage of the data. Data are now accessible from\\nvarious mounting locations through a single call, allowing democratization and versioning of the data.\\n\\nAPI LINK\\n\\nAPI LINK\\n\\n\\n\\nMADRID | SPAIN\\n\\nFigure 3')\n",
      "(82, 2, 30, 'a.\\n\\nAPI LINK\\n\\nAPI LINK\\n\\n\\n\\nMADRID | SPAIN\\n\\nFigure 3: Connecting applications using API links\\n\\nAs an example, a full-text search endpoint originally used to do searches, can also be used to create\\naggregations models like heatmap and knowledge graphs (Baill')\n",
      "(83, 2, 31, 'ns models like heatmap and knowledge graphs (Baillard et Al., 2021) as seen on Figure 4.\\n\\n@ wees\\n\\nFy â€˜eure 4 4; Heatmap (left) and Knowledge Graph (right)\\n\\nThe third metric to consider is the UI/UX responsivness and define the optimal data representation ')\n",
      "(84, 2, 32, 'ivness and define the optimal data representation for\\nthe user or the operator to QC the data and for the service to have still an acceptable response time. As\\nseen on Figure 4, the heat map and the knowledge graph are built on-top of 100,000 data points\\n')\n",
      "(85, 2, 33, 'dge graph are built on-top of 100,000 data points\\n(Mamador et al., 2021). Interface and visualization scaling requires removing the dependency on the\\nvolume of data points. The amount of time visualizing 100 points should be the same as visualizing\\n100,00')\n",
      "(86, 2, 34, '00 points should be the same as visualizing\\n100,000 points â€” in which development of aggregation and interpolation workflows must be performed\\nin the backend and frontend to overcome this volume challenge.\\n\\nBenchmarks and testing\\n\\nTo see the performance o')\n",
      "(87, 2, 35, '\\n\\nBenchmarks and testing\\n\\nTo see the performance of this architecture we can consider the following data. We have 3 buckets of\\ndata for this assessment. It is composed of geoscience documents that ranges from final well reports,\\ngeological report, regiona')\n",
      "(88, 2, 36, 'rom final well reports,\\ngeological report, regional studies, interpretations coming from various file formats, countries, and\\nlanguages. The data also includes images like thin sections, core, well logs, seismic. All these data are\\naudited and stored into')\n",
      "(89, 2, 37, 'eismic. All these data are\\naudited and stored into an object storage.\\n\\nThe following are the system that was used for this assessment:\\nBase case: 1-node with 4 cores and 16 GB of RAM from cloud provider without WM\\nized case: 5-nodes with 4 cores and 16 GB')\n",
      "(90, 2, 38, 'thout WM\\nized case: 5-nodes with 4 cores and 16 GB of RAM per node from cloud provider with WM\\n\\nBucket | Number | Duration Failure Duration | Failure | Language Region\\nof Pages (hours) Rate % (hours) Rate %\\nSingle node (Single Cluster (Cluster)\\nNode)\\n\\nee\\n')\n",
      "(91, 2, 39, '%\\nSingle node (Single Cluster (Cluster)\\nNode)\\n\\nee\\n~Woas/ af 38505 Tals | Osean\\n\\nne 531 Mixed South\\nAmerica\\n\\nTable 1: Extraction Performance with 4-cores and 16 GB of RAM per node\\n\\nTable 1 shows the extraction performance. The extraction workload encompass')\n",
      "(92, 2, 40, 'ion performance. The extraction workload encompasses the data loading,\\nprocessing, and uploading. It was tested on a variety of datasets from different regions and languages.\\nFor a single node execution, the average number of pages per hour is around 244 ')\n",
      "(93, 2, 41, 'he average number of pages per hour is around 244 pages and for a 5-node\\ncluster, the average number of pages per hour is 1000. Since workflow manager provide an auto-retry\\n\\n\\n\\nMADRID | SPAIN\\n\\nfunction, this reduces the failure rate by a significant amount')\n",
      "(94, 2, 42, 's reduces the failure rate by a significant amount. This is due to tasks that are failing due to\\nnetwork or infrastructure issues to be retried or re-executed. The failure rate is now mostly on the data\\nsince we haveve removed the infrastructure constrain')\n",
      "(95, 2, 43, 'nce we haveve removed the infrastructure constraint.\\n\\nAs one of the metrics that we have set earlier, we need to consider the cost and infrastructure usage.\\nUsing the same workloads, we can assess the cost and infrastructure optimization in Table 2.\\n\\na % ')\n",
      "(96, 2, 44, ' and infrastructure optimization in Table 2.\\n\\na % Hardware % Hardware\\nPages usage Avg. usage - Avg.\\nSingle node Cluster\\n\\nTable 2: Extraction Infrastructure Utilization with 4-cores and 16GB of RAM\\n\\nTable 2 shows that we have a higher utilization in the cl')\n",
      "(97, 2, 45, ' shows that we have a higher utilization in the cluster hence we are fully utilizing the server. In\\nthe case of single node, only 41% of the resources is utilized vs 83% in the cluster approach. Both the\\nreduced failure rates and the hardware utilization ')\n",
      "(98, 2, 46, 'educed failure rates and the hardware utilization illustratres the gain of efficency in deploying a scalable\\ningestion platform\\n\\nConclusions\\n\\nIn this paper, we have seen how to scale and optimize the ML ingestion pipeline for subsurface\\napplications. We h')\n",
      "(99, 2, 47, 'gestion pipeline for subsurface\\napplications. We have shown that scaling and optimizing ML ingestion pipelines can lead to\\nimprovements in terms of time and cost. We have also demonstrated that the ingestion platform is\\nscalable to handle data from a vari')\n",
      "(100, 2, 48, 'on platform is\\nscalable to handle data from a variety of regions and languages. Finally, weâ€™ve seen the power of scaling\\nvisualizations and exposing the data via API links in which we can get more insight and enhance the\\nability of the team to extract kno')\n",
      "(101, 2, 49, 'and enhance the\\nability of the team to extract knowledge.\\n\\nReferences\\n\\nBaillard F. and Hernandez N.: A Case Study of Understanding Bonaparte Basin using Unstructured\\nData Analysis with Machine Learning Techniques. 82nd EAGE Conference & Exhibition, 18-21\\n')\n",
      "(102, 2, 50, 'hniques. 82nd EAGE Conference & Exhibition, 18-21\\nOctober 2021, Amsterdam.\\n\\nMamador C., Hernandez N., Baillard F.: Production-scale processing of EAGEâ€™s EarthDoc data\\nto stimulate new insights in CO2 and new energy management. 82nd EAGE Conference &\\nExhib')\n",
      "(103, 2, 51, 'ew energy management. 82nd EAGE Conference &\\nExhibition worskhop on ML solutions at scale, 22 October 2021, Amsterdam.\\n\\nHernandez N., Lucajias P., Graciosa J.C., Mamador C., and Panganiban L. C. I., 2019: Automated\\n\\ninformation retrieval from unstructured')\n",
      "(104, 2, 52, 'Automated\\n\\ninformation retrieval from unstructured documents utilizing a sequence of smart machine learning\\n\\nmethods within a hybrid cloud container. EAGE Workshop on Big Data and Machine Learning for\\nE&P Efficiency 25 - 27 February.\\n\\n\\n')\n"
     ]
    }
   ],
   "source": [
    "# Execute a SELECT query\n",
    "query = \"SELECT * FROM chunks\"\n",
    "mycursor.execute(query)\n",
    "\n",
    "# Fetch all rows\n",
    "rows = mycursor.fetchall()\n",
    "\n",
    "# Print the results\n",
    "for row in rows:\n",
    "    print(row)  # You can format this output as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a dictionary from the paper\n",
    "As of 09/06/2024, the chatbot model uses the dictionary data type to look up information,\n",
    "so here I try to query the database and turn the paper we saved into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute a query to select all the rows from the 'papers' table\n",
    "query = \"SELECT title, author, chunk FROM papers\"\n",
    "mycursor.execute(query)\n",
    "\n",
    "# Fetch all the rows\n",
    "rows = mycursor.fetchall()\n",
    "\n",
    "# Create a dictionary with the format you specified\n",
    "paper = {\n",
    "    \"title\": rows[0][0],\n",
    "    \"author\": rows[0][1],\n",
    "    \"content\": ''.join(row[2] for row in rows)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check our output\n",
    "We verify if the paper has indeed been loaded into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'A CASE STUDY OF UNDERSTANDING THE BONAPARTE BASIN USING UNSTRUCTURED DATA ANALYSIS WITH MACHINE LEARNING TECHNIQUES', 'author': 'A.N.N. Sazali, N.M. Hernandez, F. Baillard, K.G. Maver', 'content': \"\\nA CASE STUDY OF UNDERSTANDING THE\\n\\nBONAPARTE BASIN USING UNSTRUCTURED DATA\\nANALYSIS WITH MACHINE LEARNING TECHNIQUES\\n\\nANN. Sazali!, N.M. Hernandez', F. Baillard', K.G. Maver!\\n\\n' Traya Energies\\n\\nSummary\\n\\nAs part of exploration and production the oil and gas industry produce substantial amounts of data\\nwithin different disciplines of which 80% are unstructured like reports, presentations, spreadsheets etc.\\nThe value of technical work is reduced due to the lack of time available for analysis and critical\\nthinking and the under-utilization of the data. To assist geoscientist and engineers, Machine Learning\\n(ML) and Artificial Intelligence (AI) technologies are applied to process the unstructured data from\\n440 wells from the Bonaparte Basin in Australia making it possible to perform more accurate analysis\\nand make faster decisions.\\n\\nBased on the play-based exploration pyramid concept, the time spent at the Basin Focus stage can be\\nreduced, and more time are available to focus on the other project stages. The explorationist will be\\nable to bring more value to the study.\\n\\nIt will be shown that potential issues encountered during exploration of the Bonaparte Basin can be\\nidentified. Based on a quick look and gathering of all information it can be concluded that most of the\\nproduction in the Bonaparte Basin is from Jurassic and Triassic with observed net pay of 18-60m\\nthickness, porosity of 11-29% and saturation of 11-55% Sw.\\n\\n\\nA Case Study of Understanding the Bonaparte Basin using Unstructured Data Analysis with\\nMachine Learning Techniques\\n\\nIntroduction\\n\\nAs part of exploration and production the oil and gas industry produce substantial amounts of data\\nwithin different disciplines of which 80% are unstructured like reports, presentations, spreadsheets etc\\nand it is expected to grow exponentially. As a result, geoscientists and engineers spend 50 to 80% of\\ntheir time searching and assembling data and only 1 to 5% of the data is fully utilized. The value of\\ntechnical work is therefore reduced due to the lack of time available for analysis and critical thinking\\nand the under-utilization of the data. To assist geoscientist and engineers, Machine Learning (ML)\\nand Artificial Intelligence (AI) technologies are applied to process the unstructured data making it\\npossible to perform more accurate analysis and make faster decisions.\\n\\nIn this case study the area of interest covers Bonaparte Basin, which is located north-west of\\nthe Australian continental margin (Figure 1). It joins the Money Shoal basin in the north-east and\\nthe Browse Basin in the south-west. Furthermore, the Timor Trough defines the northern boundary.\\nThe areal extent of the basin is approximately 270,000 sq. km. The objective of this study is to\\nunderstand and obtain meaningful insights into the Bonaparte Basin based on the substantial amount\\nof information available in previous studies, reports and presentations. The unstructured data of the\\nBonaparte Basin have been ingested in a Knowledge Container through consecutive ML and AI\\npipelines and analysed using big data analytics tools.\\n\\nJava Sea\\n\\nConsist of several structural elements :\\n@ Ashmore Platform  Malita Graben\\n Vulcan Sub-Basin  Sahul Platform\\n Londonderry High @ Flamingo High\\n Petrel Sub-basin @ Flamingo Syncline\\n Darwin Shelf  Sahul Syncline\\n@ Calder Graben  Nancar Trough\\n@ Troubadour Terrace  @)_ Laminaria High\\n\\nFigure 1 Location of the Bonaparte Basin within the Australian continental margin (left) and 14\\nstructural elements observed within the Bonaparte Basin (right).\\n\\nMethodology\\n\\nAs of 2021, the Bonaparte Basin encompasses 440 wells representing 58 years of exploration history\\nsummarized in over 270,000 pages of documents and in 250,000 images. It is estimated that billions of\\ndollars have been invested over the years to acquire and interprete the data, making it a substantial\\nsource of information for new exploration activities.\\n\\nThe Play Based Exploration (PBE) approach is often used as a traditional framework to refine the\\ngeoscientistss understanding from a broad basin level to a narrow prospect focus (Lottaroli et al., 2016).\\nAs a start such an approach often involves capturing the current state of knowledge with massive\\nbackground resources to understand and analyse the key features of the basin and the major risks\\nassociated to it. Such information is primarily available in unstructured data, requiring geoscientist and\\nengineers to process and ingest the information before focusing on a specific play and prospect using\\nstructured data. Therefore, we have modified the existing PBE pyramid to introduce an additional\\n\\n  \\n\\n\\ndimension associated with data science identifying the different types of data available at different\\nstages, allowing us to better define the best suited ML/AI strategy for a given stage (Figure 2).\\n\\nProspect Specific chance\\n\\nProspect focus\\n= image, Map, Evaluate\\n\\nSTRUCTURED chance that a panicular prospect will\\nbecome\\na discovery in case play works.\\n\\nPlay Focus\\n\\n' Play Chance\\n1 Identity imits ofthe single play elements 1 the chance hat a parteuler play\\nunsracres: Al eee wean | gy mesnereamecercett\\n\\\\ dbcare te 1 elements (e.g. reservoir, seal, source)\\neffectiveness of each element | of being present and effective over an\\n\\n(Commmon Risk Segment Mapping)\\n\\n@ Analyse play statistics. Seon Gey O44 a\\n\\nDetermination and description of the\\nRegional context and Basin\\nFramework\\nUnderstanding of the Petroleum\\nsystem(s)\\n\\nBasin Focus\\n\\n= Assess the possibility a play may\\nexists in a basin\\n\\nFigure 2 Customized Play Based Exploration (PBE) pyramid with ML technology (Modified from\\nLottaroli et al., 2016).\\n\\nFocusing on the unstructured data associated with the Basin and Play Analysis, all the data from the\\nBonaparte Basin have been processed through a succession of AI/ML automated pipeline such as\\nNatural Language Processing or Deep Convolutional Neural Network (Hernandez et al., 2019), (Figure\\n3). The sharable structured data is then further processed through deeper level of analytics to detect\\ntrends and anomalies present within the data. Machine assistance is heavily used in repetitive tasks early\\nin the process during the processing of the data and up to 95% of the tasks will be performed by the\\nmachine. This provides additional time for the specialist to focus on critical thinking and cognitive skills\\nto interpret the data.\\n\\nMACHINE LEARNING PIPELINE ANALYTICS, GEOLOGICAL INSIGHTS\\n\\n-inniogy an Early interpretation on\\nGeological environment\\n\\n* Text Search Data are geospatially\\n\\nExtracted image ea\\n+ Imoges. ~~ Image Classification  > nae |, distributed on maps for\\nDeep Convolutional ee trends and anomalies\\nUnstructured pecker hom Giants\\n Structured * Image Search\\nData ee\\na\\n\\n|, Extracted | Metadata Extraction |, Document\\nText and Tagging Tags\\n\\nNotural tanguage Processing\\n\\n._ Early trend can be observed\\n\\n> Heat Mi in\\nea MEe across the whole basin\\n\\n._ Identify well analogues and\\n\\nHEE oe relationship between wells\\n\\nMACHINE 4 on =\\n\\nAf. 50.%5 . i 60% 0%\\n\\nML/Al sequence automated with Analytics tools for data display Human high-level interpretation\\nhuman in the loop for QC\\n\\nFigure 3 Unstructured Big Data pipeline\\n\\nIn this case study, interpretation using the Big Data workflow was used to understand the exploration\\nhistory, how the basin developed, its petroleum system and the main issue of the dry wells occurrence\\nto avoid repeating the mistakes of the past and improve future decision making.\\n\\n\\n\\nBy analysing the data, five potential issues are identified i.e. (i) Discrepancies in Formation Tops, (ii)\\nLimited understanding of Lithology Distribution, (iii) Limited Mineral Composition Understanding,\\n(iv) Fluid Distribution, and (v) Pressure/Temperature Patterns. Each potential issue is tackled by\\nidentifying trends and anomalies across the basin using images, tables and plots extracted from the\\nunstructured data corpus.\\n\\nResults\\n\\nAs an example, the analysis of the (ii) Limited Understanding of Lithology Distribution, shown in\\nFigure 4, is performed using heatmaps. The heatmaps show the distribution of clastic and carbonates\\nacross the Bonaparte Basin and identify patterns and anomalies present in the area. The result can be\\nsupported by the stratigraphic chart where the carbonate environment occurs in the younger formation\\nfrom Cretaceous to Neogene period, whereas clastic environment is present in the older formations from\\nTriassic to Cretaceous.\\n\\nNe\\n\\nPALEOGENE\\neae\\n\\nEa\\n\\nCRETACEOUS\\n\\n$3233 3 3\\nJURASSIC\\n\\ntoy | Mee\\n\\nTing 5\\nwo\\n\\nBasal transgressive snadstone EEEEEE Limestone\\n\\nand marine shelf sand\\nMarine claystone and shale ES van\\n\\nModitied trom Frankowicz & McClay 2009\\n\\nFigure 4 Lithology distribution on heatmaps (left) and corresponding stratigraphic chart (right).\\n\\nThe analysis of the (iii) Limited Mineral Composition Understanding, shown in Figure 5, utilizes the\\nthin section automatically extracted using ML classification over the full area and suggests that:\\n\\n Quartz overgrowth and kaolinite are quite common in Bonaparte Basin\\n\\ne Mica mineral can be observed at the north-eastern part of the basin\\n\\ne Highly corroded, skeletal feldspar has been extensively dissolved, which forms secondary\\nporosity, and can be observed in the northern part of the basin\\n\\ne Some patchy siderites are also observed in the southern part of the basin\\n\\n  \\n\\n\\nFigure 5 Thin section images distributed on a map across the Bonaparte Basin.\\n\\nConclusion\\n\\nA regional understanding is critical and time consuming as it involves dealing with a very large data\\nvolume. Within a project time frame, based on PBE pyramid, the time spent at the Basin Focus stage\\ncan be reduced, and more time are available to focus on the other project stages. The explorationist will\\nbe able to bring more value to the study.\\n\\nML applications have proven to be able to play a crucial part in order to organize large unstructured\\ndata corpuses. This allows faster and accurate decision making within the fast-moving industry.\\n\\nIn this study, some potential issues encountered during exploration of the Bonaparte Basin can be\\nidentified. Based on a quick look and gathering of all information it can be concluded that most of the\\nproduction in the Bonaparte Basin is from Jurassic and Triassic with observed net pay ~18-60m\\nthickness, porosity ~11-29% and saturation ~11-55% Sw.\\n\\nReferences\\n\\nHernandez N., Lucafias P., Graciosa J.C., Mamador C., and Panganiban L. C. L, 2019: Automated\\ninformation retrieval from unstructured documents utilizing a sequence of smart machine learning\\nmethods within a hybrid cloud container. EAGE Workshop on Big Data and Machine Learning for E&P\\nEfficiency 25 - 27 February.\\n\\nLottaroli F., Craig J., Cozzi A., 2016: Evaluating a vintage play fairway exercise using subsequent\\nexploration results: did it work? Petroleum Geoscience, Vol 24, no 2, p. 159-171.\\n\\nMaver K.G., Hernandez N., Lucafias P., Graciosa J.C., Mamador C., Panganiban L.C.I., Yu C., and\\nMaver M.G., 2018: An automated information retrieval platform for unstructured well data smart\\nmachine learning algorithms within a hybrid cloud container. EAGE/PESGB Workshop on Machine\\nLearning, 29  30 November.\\n\\n\\n\"}\n"
     ]
    }
   ],
   "source": [
    "# Print the paper\n",
    "print(paper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
